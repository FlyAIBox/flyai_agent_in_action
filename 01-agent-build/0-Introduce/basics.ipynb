{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0d1bad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfe28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098df42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1406a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194a11bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1868.62 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 788.88 GB, Free: 1123.48 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854fa9f5-f7e2-42f8-b208-ac339e9140be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   /opt/conda\n",
      "lf                     /opt/conda/envs/lf\n",
      "flyai_agent_in_action * /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 查看当前使用的Coda环境（带有*）\n",
    "%conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {
    "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
   },
   "source": [
    "\n",
    "## LangGraph教程：LangChain\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### **背景介绍**\n",
    "\n",
    "在 LangChain，我们的目标是让构建大型语言模型（LLM）应用变得简单。您可以构建的一种 LLM 应用就是**智能代理（Agent）**。构建智能代理非常令人兴奋，因为它们能够自动化以前不可能完成的各种任务。\n",
    "\n",
    "然而，在实践中，构建能够可靠执行这些任务的系统极其困难。在我们与用户合作将智能代理投入生产的过程中，我们发现通常需要更多的**控制**。例如，您可能需要智能代理始终优先调用某个特定的工具，或者根据其状态使用不同的提示词。\n",
    "\n",
    "为了解决这个问题，我们构建了 [**LangGraph**](https://langchain-ai.github.io/langgraph/) —— 一个用于构建智能代理和多智能体应用的框架。LangGraph 独立于 LangChain 包，其核心设计理念是帮助开发者为智能代理工作流添加更好的**精确性**和**控制力**，使其适合现实世界系统的复杂性。\n",
    "\n",
    "-----\n",
    "\n",
    "### **设置**\n",
    "\n",
    "在开始之前，请按照 `README` 文件中的说明创建环境并安装依赖项。\n",
    "\n",
    "-----\n",
    "\n",
    "### **聊天模型**\n",
    "\n",
    "在本课程中，我们将使用[**聊天模型（Chat Models）**](https://python.langchain.com/v0.2/docs/concepts/#chat-models)，它们的功能是接收一系列消息作为输入，并以聊天消息作为输出。LangChain 本身不托管任何聊天模型，而是依赖于第三方集成。 是 LangChain 中支持的第三方聊天模型集成列表！默认情况下，课程将使用[ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)  ，因为它既流行又性能出色。正如前面提到的，请确保您已设置好 `OPENAI_API_KEY`。\n",
    "\n",
    "我们将检查您的 `OPENAI_API_KEY` 是否已设置；如果未设置，系统会提示您输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bffd3e-2b2a-4168-a1d6-546a6e770b67",
   "metadata": {
    "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
   },
   "source": [
    "\n",
    "#### OpenAI国内代理\n",
    "\n",
    "为方便大家再国内使用OpenAI，Claude等国外模型\n",
    "\n",
    "和OpenAI国内代理API易社区争取到了如下权益：\n",
    "\n",
    "OpenAI国内代理地址\n",
    "https://api.apiyi.com/register/?aff_code=we80\n",
    "新用户注册送0.1美金，注册成功后在以下表格中填写你的账号，平台会再赠送2美金（5个工作日到账）\n",
    "\n",
    "---- 【腾讯文档】API易账号收集 https://docs.qq.com/form/page/DQm1qb1VBQU9wR2xq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9a52c8",
   "metadata": {
    "id": "0f9a52c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的依赖包\n",
    "# %pip install 是 Jupyter 中安装 Python 包的命令\n",
    "# --quiet 参数减少输出信息\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langchain_community==0.3.29 tavily-python==0.7.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a15227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2a15227",
    "outputId": "2bc8601c-50bc-4353-fa6a-3c5d5c7ef112"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的模块\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 要设置的环境变量名称\n",
    "\n",
    "    功能:\n",
    "        - 检查环境变量是否已存在\n",
    "        - 如果不存在，则提示用户输入并设置\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):  # 检查环境变量是否已设置\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")  # 安全地获取用户输入\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {
    "id": "a326f35b"
   },
   "source": [
    "[这里](https://python.langchain.com/v0.2/docs/how_to/#chat-models)是一个有用的指南，介绍了您可以使用聊天模型做的所有事情，但我们在下面会展示一些重点。如果您按照 README 中的说明运行了 `pip install -r requirements.txt`，那么您已经安装了 `langchain-openai` 包。有了这个包，我们可以实例化我们的 `ChatOpenAI` 模型对象。如果您是第一次注册 API，您应该会收到[免费积分](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517)，可以应用于任何模型。您可以[在这里](https://openai.com/api/pricing/)查看各种模型的定价。笔记本将默认使用 `gpt-4o`，因为它在质量、价格和速度之间取得了良好的平衡[更多信息请参见这里](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini)，但您也可以选择价格较低的 `gpt-3.5` 系列模型。\n",
    "\n",
    "聊天模型有几个[标准参数](https://python.langchain.com/v0.2/docs/concepts/#chat-models)可以设置。最常见的两个是：\n",
    "\n",
    "* `model`：模型名称\n",
    "* `temperature`：采样温度\n",
    "\n",
    "`Temperature` 控制模型输出的随机性或创造性，其中低温度（接近 0）产生更确定性和专注的输出。这适合需要准确性或事实性响应的任务。高温度（接近 1）适合创造性任务或生成多样化响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19a54d3",
   "metadata": {
    "id": "e19a54d3"
   },
   "outputs": [],
   "source": [
    "# 导入 ChatOpenAI 类\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 创建 GPT-4o 聊天模型实例\n",
    "# model=\"gpt-4o\": 使用 GPT-4o 模型，这是 OpenAI 的最新模型\n",
    "# temperature=0: 设置温度为 0，使输出更加确定性和一致\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 创建 GPT-3.5 Turbo 聊天模型实例\n",
    "# model=\"gpt-3.5-turbo-0125\": 使用 GPT-3.5 Turbo 模型\n",
    "# temperature=0: 同样设置为 0，确保输出的一致性\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {
    "id": "28450d1b"
   },
   "source": [
    "LangChain 中的聊天模型有许多[默认方法](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface)。在大多数情况下，我们将使用：\n",
    "\n",
    "* `stream`：流式返回响应的块\n",
    "* `invoke`：在输入上调用链\n",
    "\n",
    "如前所述，聊天模型接受[消息](https://python.langchain.com/v0.2/docs/concepts/#messages)作为输入。消息具有一个角色（描述谁在说消息）和一个内容属性。我们稍后会更多地讨论这个问题，但这里让我们先展示基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1280e1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1280e1b",
    "outputId": "08c10f33-0874-413f-870b-d2cf24c7dff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 17, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_5d7ee1b844', 'id': 'chatcmpl-CMRCwguhduNHehBqxpJWrY6sUklau', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c5c7842f-328c-47ce-aa90-8845c2cf9eff-0', usage_metadata={'input_tokens': 17, 'output_tokens': 10, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入 HumanMessage 类，用于创建人类用户的消息\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 创建一个人类消息\n",
    "# content: 消息内容\n",
    "# name: 发送者的名称（可选）\n",
    "msg = HumanMessage(content=\"你好呀\", name=\"萤火AI百宝箱\")\n",
    "\n",
    "# 创建消息列表\n",
    "# 聊天模型通常接受消息列表作为输入\n",
    "messages = [msg]\n",
    "\n",
    "# 使用消息列表调用模型\n",
    "# invoke() 方法会发送消息给模型并返回响应\n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {
    "id": "cac73e4c"
   },
   "source": [
    "我们得到一个 `AIMessage` 响应。另外，请注意我们可以直接用字符串调用聊天模型。当字符串作为输入传递时，它会被转换为 `HumanMessage`，然后传递给底层模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27c6c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f27c6c9a",
    "outputId": "ee85a471-3314-43d1-c430-3066ee0844e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangGraph 是一种用于处理和分析语言数据的技术框架或工具集。它通常结合了图形数据库和自然语言处理（NLP）技术，以便更有效地理解和处理复杂的语言结构和关系。以下是对 LangGraph 的通俗解释、应用场景、竞品对比分析以及企业落地的概述：\\n\\n### 通俗解释\\nLangGraph 可以被视为一种将语言数据转化为图形结构的技术。图形结构能够更好地表示语言中的实体及其关系，比如词语之间的关联、句子结构、语义网络等。通过这种方式，LangGraph 可以帮助计算机更好地理解和处理自然语言。\\n\\n### 应用场景\\n1. **知识图谱构建**：LangGraph 可以用于构建知识图谱，将文本数据转化为结构化的信息网络，帮助企业更好地管理和利用知识。\\n   \\n2. **信息检索与问答系统**：通过图形结构，LangGraph 可以提高信息检索的准确性和效率，支持更复杂的问答系统。\\n\\n3. **情感分析与舆情监测**：利用语言图谱分析社交媒体或新闻中的情感和舆情，帮助企业进行市场分析和品牌管理。\\n\\n4. **文本生成与翻译**：通过理解语言结构，LangGraph 可以提高机器翻译和文本生成的质量。\\n\\n### 竞品对比分析\\nLangGraph 的竞品主要包括其他语言处理和图形数据库技术，如：\\n\\n- **Neo4j**：一种流行的图形数据库，虽然不专注于语言处理，但可以与 NLP 工具结合使用。\\n- **GraphDB**：专注于语义图谱和知识管理，适合与语言处理结合。\\n- **SpaCy 和 NLTK**：这些是流行的 NLP 库，可以与图形数据库结合使用以实现类似的功能。\\n\\nLangGraph 的优势在于其专注于语言数据的图形化处理，可能在处理复杂语言关系时更具优势。\\n\\n### 企业落地\\n企业可以通过以下方式将 LangGraph 技术落地：\\n\\n1. **数据集成**：将企业现有的文本数据集成到 LangGraph 中，构建语言图谱以支持各种应用。\\n\\n2. **定制解决方案**：根据企业需求，定制化开发信息检索、问答系统或情感分析工具。\\n\\n3. **培训与支持**：提供员工培训和技术支持，帮助企业更好地利用 LangGraph 技术。\\n\\n4. **持续优化**：通过不断更新和优化语言图谱，确保系统能够适应不断变化的语言数据和业务需求。\\n\\n通过这些方式，企业可以充分利用 LangGraph 的技术优势，提高语言数据处理的效率和效果。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 602, 'prompt_tokens': 28, 'total_tokens': 630, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_5d7ee1b844', 'id': 'chatcmpl-CMRD3vXlr7RJNwWuxBsixs5VII8CY', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c3272bba-9dfe-4177-a5d9-0f7aee5b8394-0', usage_metadata={'input_tokens': 28, 'output_tokens': 602, 'total_tokens': 630, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接使用字符串调用模型\n",
    "# 当传入字符串时，LangChain 会自动将其转换为 HumanMessage\n",
    "gpt4o_chat.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc2f0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdc2f0ca",
    "outputId": "6adff1a8-56cb-4f18-fbbb-b8a95dae682e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangGraph是一种基于语言学的图数据库，用于存储和查询语言相关的数据。它可以帮助用户更有效地管理和分析大量的语言数据，如文本、语音、图像等。\\n\\n应用场景：\\n1. 语言学研究：研究人员可以利用LangGraph存储和分析大量的语言数据，从而更好地理解语言的结构和演化规律。\\n2. 自然语言处理：LangGraph可以用于构建自然语言处理模型，帮助机器理解和生成自然语言。\\n3. 语言教育：教育机构可以利用LangGraph存储和管理语言学习资源，为学生提供个性化的学习体验。\\n\\n竞品对比分析：\\n与传统的关系型数据库相比，LangGraph更适合存储和查询语言相关的数据，因为它能够更好地处理语言之间的复杂关系。与其他图数据库相比，LangGraph专注于语言学领域，提供了更多针对语言数据的特定功能和优化。\\n\\n企业落地：\\n企业可以利用LangGraph构建自己的语言数据平台，用于存储和分析大量的语言数据。例如，一家在线教育公司可以利用LangGraph存储学生的语言学习记录，并根据学生的学习情况提供个性化的学习建议。另外，一家语言学研究机构可以利用LangGraph存储和分析大量的语言数据，从而推动语言学研究的进展。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 447, 'prompt_tokens': 36, 'total_tokens': 483, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': None, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': None}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'id': 'chatcmpl-CMRDdmdH3cfTxQaLmi16VyuihO22r', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c1afa06e-9f61-4dd8-b299-651bbc7afd53-0', usage_metadata={'input_tokens': 36, 'output_tokens': 447, 'total_tokens': 483, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 GPT-3.5 Turbo 模型调用\n",
    "# 同样可以直接传入字符串\n",
    "gpt35_chat.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d65a1a2-9828-4388-9510-1afb5453f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph是一种基于语言学的图数据库，用于存储和查询语言相关的数据。它可以帮助用户更有效地管理和分析大量的语言数据，如文本、语音、图像等。\n",
      "\n",
      "应用场景：\n",
      "1. 自然语言处理：LangGraph可以用于构建自然语言处理系统，帮助用户处理文本数据、进行语义分析、实现机器翻译等任务。\n",
      "2. 语言学研究：研究人员可以利用LangGraph存储和分析语言学数据，探索语言结构、语言演化等问题。\n",
      "3. 智能客服：LangGraph可以用于构建智能客服系统，帮助企业提供更高效的客户服务。\n",
      "\n",
      "竞品对比分析：\n",
      "与传统的关系型数据库相比，LangGraph具有更好的存储和查询效率，特别适用于存储和处理大规模的语言数据。与其他图数据库相比，LangGraph更专注于语言相关的数据，具有更丰富的语言学功能和特性。\n",
      "\n",
      "企业落地：\n",
      "企业可以通过部署LangGraph来构建自己的语言处理系统，提高数据管理和分析的效率。LangGraph可以与现有的数据处理工具和系统集成，帮助企业更好地利用语言数据实现商业目标。企业可以根据自身需求定制LangGraph的功能和性能，实现个性化的解决方案。\n",
      "--- 流式输出结束 ---\n"
     ]
    }
   ],
   "source": [
    "# 调用 stream 方法并遍历返回的流\n",
    "stream_response = gpt35_chat.stream(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")\n",
    "\n",
    "# 打印每个分块的内容，并把它们连接起来形成完整回复\n",
    "full_response = \"\"\n",
    "for chunk in stream_response:\n",
    "    # 提取每个分块（chunk）中的内容（content）\n",
    "    content = chunk.content\n",
    "    print(content, end=\"\", flush=True) # 使用 print 实时输出内容\n",
    "    full_response += content\n",
    "\n",
    "print(\"\\n--- 流式输出结束 ---\")\n",
    "# print(f\"\\n完整回复:\\n{full_response}\") # 打印完整回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {
    "id": "582c0e5a"
   },
   "source": [
    "所有聊天模型的接口都是一致的，模型通常在每个笔记本启动时初始化一次。\n",
    "\n",
    "因此，如果您强烈偏好另一个提供商，您可以轻松地在模型之间切换，而无需更改下游代码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {
    "id": "3ad0069a"
   },
   "source": [
    "## 搜索工具\n",
    "\n",
    "您还会在 README 中看到 [Tavily](https://tavily.com/)，这是一个为 LLM 和 RAG 优化的搜索引擎，旨在提供高效、快速和持久的搜索结果。如前所述，注册很容易，并提供慷慨的免费层级。一些课程（在模块 4 中）将默认使用 Tavily，但当然，如果您想为自己修改代码，也可以使用其他搜索工具。\n",
    "\n",
    "**Tavily API 密钥 (API Key) 的注册和申请流程**通常非常直接，但具体步骤可能会有细微变化。以下是一般性的指导和您可能需要遵循的步骤：\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 访问 Tavily 官方网站\n",
    "\n",
    "首先，您需要访问 **Tavily 的官方网站**（您已经提供了链接：[https://tavily.com/](https://tavily.com/)）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 查找注册/登录或 API 页面\n",
    "\n",
    "在网站上，您需要找到以下选项之一：\n",
    "* **“Sign Up”（注册）** 或 **“Log In”（登录）** 按钮。\n",
    "* **“API”、“Developers”（开发者）** 或 **“Pricing”（定价）** 页面。\n",
    "\n",
    "通常，您需要先创建一个用户账户才能申请 API 密钥。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 注册新账户\n",
    "\n",
    "如果您是新用户：\n",
    "* 点击 **“Sign Up”** 或 **“注册”**。\n",
    "* 您可能需要提供一个 **电子邮件地址** 和设置一个 **密码**。\n",
    "* 有些服务也支持使用 **Google** 或其他第三方账户直接登录/注册。\n",
    "* 完成注册后，您可能需要**验证您的邮箱**（通过点击发送到您邮箱的链接）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 访问 API 仪表板/设置\n",
    "\n",
    "登录您的账户后，您需要进入到管理 API 密钥的区域，这通常被称为：\n",
    "* **“Dashboard”（仪表板）**\n",
    "* **“API Keys”（API 密钥）**\n",
    "* **“Settings”（设置）** 或 **“Profile”（个人资料）**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 生成 API 密钥\n",
    "\n",
    "在这个 API 密钥管理页面：\n",
    "* 查找类似于 **“Create New Key”（创建新密钥）**、**“Generate API Key”（生成 API 密钥）** 或 **“Get Started”（开始使用）** 的按钮。\n",
    "* 点击该按钮，系统会**立即生成**一串独特的长字符，这就是您的 **Tavily API Key**。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 保存您的 API 密钥\n",
    "\n",
    "**重要提示：**\n",
    "* 生成的 API 密钥通常只会在**生成时显示一次**。\n",
    "* **请务必立即复制并安全地存储**（例如，在密码管理器或安全文档中）您的密钥。\n",
    "* 如果丢失，您可能需要生成一个新的密钥，而旧的密钥可能会被吊销。\n",
    "\n",
    "完成这些步骤后，您就可以在您的应用程序或开发环境中使用这个 **TAVILY\\_API\\_KEY** 来调用 Tavily 的搜索 API 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091dff13",
   "metadata": {
    "id": "091dff13"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# 设置 Tavily API 密钥\n",
    "# 这是使用 Tavily 搜索功能所必需的\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d69da9",
   "metadata": {
    "id": "52d69da9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tmp/ipykernel_2477/4221156914.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "# 导入 Tavily 搜索结果工具\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 创建 Tavily 搜索实例\n",
    "# max_results=3: 设置最大返回结果数量为 3\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "# 执行搜索\n",
    "# 搜索 \"What is LangGraph?\" 并获取结果\n",
    "search_docs = tavily_search.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06f87e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d06f87e6",
    "outputId": "5f9fe70c-6e2d-4f56-8217-563309f0118f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '【系统学习02】工作流vs Multi-Agent：Dify、LangGraph - 知乎专栏',\n",
       "  'url': 'https://zhuanlan.zhihu.com/p/1950863538311627763',\n",
       "  'content': 'Image 2: 血戎 本文把自己在 **Dify**、**LangGraph**、**AgentScope** 这三套工具上的实战感受放在一起对比，顺便聊聊 Multi-Agent 会不会像微服务那样走向“工程化”的未来。 | Dify (v1.6+) | DAG + 可视化编辑器 | 支持 Prompt 模板的变量替换 {{var}}，内置知识库检索（支持多源数据），工具调用需配置 OpenAPI 文档。 注意：内置的”Agent 节点”实际是工作流嵌套，非真正的 Multi-Agent | 上手快、低代码、适合业务流程 | 灵活性一般，多Agent弱（仅支持嵌套，无原生通信机制） | Multi-Agent 成功的关键不在于“有多少 Agent”，而在于 **通信方式、共享状态与角色分工**。 三大平台的 Multi-Agent 能力对比 | 平台 | 通信机制 | 多Agent支持 | 记忆 | 协作方式 | | 维度 | 工作流 | Multi-Agent | *   **Multi-Agent**：适合需要多角度判断、动态任务分解或角色并发协作的任务（例如复杂的市场分析、多角色对话系统、模拟谈判）。优点是灵活、能并行探索；缺点是调试和治理成本高。  2.   **可观测性**：日志、链路追踪、可视化监控会成为 Multi-Agent 平台标配（如：LangSmith）。 *   **方案**：**工作流**（流程清晰，各环节固定，无需多 Agent 协作） *   **方案**：**Multi-Agent**（需多角色协作，动态规划） *   **Multi-Agent**是你请来的一群网红大厨，灵感爆棚，但有时容易吵起来。 **未来趋势**：工作流和 Multi-Agent 不是非此即彼，而是互补共存。真正的高手，往往会两手都要抓——在流程性任务中用工作流保证稳定性，在探索性任务中用 Multi-Agent激发创造力。 **最终建议**：不要过早站队。先用工作流快速验证业务逻辑，再逐步引入 Multi-Agent 解决复杂场景。毕竟，AI 平台的终极目标，不是选择哪一派，而是让智能应用真正服务于人。 AI-Agent multi-agent Image 7: 血戎 AI Agent工作流实用手册：5种常见模式的实现与应用，助力生产环境稳定性 ====================================== 很多人认为使用AI Agent就是直接扔个提示词过去，然后等结果。做实验这样是没问题的，但要是想在生产环境稳定输出高质量结果，这套玩法就不行了。 核心问题是这种随意的提示方式根本扩展不… deephubmulti-agent 多 Agent 代理协同的三种架构 ============================= 原文： LangGraph: Multi-Agent Workflows 很高兴地介绍 langgraph 强大的用例 - 多代理工作流。在本博客中，我们将涵盖：&#34;multi-agen 多代理&#34;是什么意思？ 为什么“multi-agent 多… 混沌福王大模型端侧部署(On-Device AI) ===================== 前言手机与PC厂商从23年底，叙事中加入了一个概念--端侧AI 人工智能可以直接在移动设备上进行处理，无需连接到服务器或云端。它带来了许多好处，包括低延迟、更高的安全性和灵活性。即使设… 天晴Image 8: 复杂Multi-Agent系统设计思考 复杂Multi-Agent系统设计思考 =================== 一方小民',\n",
       "  'score': 0.98513},\n",
       " {'title': '快时尚电商行业智能体设计思路与应用实践（二）借助LangChain ...',\n",
       "  'url': 'https://aws.amazon.com/cn/blogs/china/fast-fashion-e-commerce-agent-design-ideas-and-application-practice-part-two/',\n",
       "  'content': 'from abc import ABC, abstractmethod from typing import Dict, Optional from langchain_community.chat_models import BedrockChat from langchain.prompts import ChatPromptTemplate from langchain.schema import BaseMessage class BaseAgent(ABC): \"\"\"Base class for all customer service agents.\"\"\" def __init__(self, model_id: str = \"anthropic.claude-3-sonnet-20240229-v1:0\", region: str = \"us-west-2\"): \"\"\"Initialize the agent with a Bedrock model.\"\"\" self.llm = BedrockChat( model_id=model_id, model_kwargs={\"temperature\": 0.7, \"max_tokens\": 2048}, region_name=region ) self.conversation_history: Dict[str, list[BaseMessage]] = {} def _get_history(self, conversation_id: str) -> list[BaseMessage]: \"\"\"Get conversation history for a specific conversation.\"\"\" return self.conversation_history.get(c[...]e the resources provided by this MCP server.\"\"\" return { \"order_data\": { \"description\": \"Access to order data\", \"handler\": self.handle_order_data_access }, \"sop_data\": { \"description\": \"Access to Standard Operating Procedures\", \"handler\": self.handle_sop_data_access } } def handle_process_question(self, args: Dict[str, Any]) -> Dict[str, Any]: \"\"\"Handle the process_question tool.\"\"\" question = args[\"question\"] conversation_id = args.get(\"conversation_id\") response, new_conversation_id = self.system.process_question(question, conversation_id) return { \"response\": response, \"conversation_id\": new_conversation_id } def handle_order_data_access(self, uri: str) -> Dict[str, Any]: \"\"\"Handle access to order data.\"\"\" if uri == \"all\": return {\"orders\": self.system.order_service.get_order_data()} order_id = uri order_info = self.system.order_service.get_order_info(order_id) if order_info: return {\"order\": order_info} return {\"error\": f\"Order {order_id} not found\"} def handle_sop_data_access(self, uri: str) -> Dict[str, Any]: \"\"\"Handle access to SOP data.\"\"\" if uri == \"order\": return {\"decision_tree\": self.system.sop_service.order_decision_tree} elif uri == \"logistics\": return {\"decision_tree\": self.system.sop_service.logistics_decision_tree} return {\"error\": f\"Unknown SOP type: {uri}\"} # MCP server configuration config = { \"name\": \"customer-service\", \"version\": \"1.0.0\", \"description\": \"Customer service system for e-commerce platform\", \"server\": CustomerServiceMCP() } ',\n",
       "  'score': 0.98355},\n",
       " {'title': '晓|2025年AI系统架构洞察：Agent三大框架全景调研 - 知乎专栏',\n",
       "  'url': 'https://zhuanlan.zhihu.com/p/1945267976937926834',\n",
       "  'content': 'Image 1) Image 2: 晓|2025年AI系统架构洞察：Agent三大框架全景调研 Image 3: 旺知识 Image 5: 图片 Image 6: 图片 **Qwen-Agent** 作为阿里推出的智能体框架，在**中文环境**和**电商场景**中具有天然优势。它集成了Qwen大模型系列的技术优势，特别是在多模态理解和处理方面表现优异，适合构建需要理解中文语境和商业场景的智能体应用。 Image 7: 图片 *   • **企业级知识管理与问答系统**：对于需要处理大量内部文档、提供准确知识检索的企业应用，**LlamaIndex**和**Haystack**是最成熟稳定的选择。它们提供了完整的RAG管道和强大的数据连接能力，适合生产环境部署。如果处理大量技术文档或复杂表格，**RAGFlow**的深度文档理解能力特别值得考虑。 *   • **复杂任务自动化与多步骤工作流**：对于需要分解执行、有多步骤决策需求的任务，**LangGraph**的状态机模型提供了精确控制，适合业务流程自动化。**AutoGen**的多智能体对话模式适合需要多个专业角色协作解决的复杂问题，如软件设计或综合决策。 *   • **深度研究与报告生成**：需要从多源信息中搜集、分析和生成综合报告的研究任务，**GPT-Researcher**提供了全面性保证和多源验证能力。对于学术研究场景，**dzhng/deep-research**的学术资源优化和专业数据库集成更加适用。 *   • **快速原型开发与可视化构建**：对于需要快速验证想法或低代码开发的团队，**Dify**和**Langflow**提供了可视化工作流构建能力，大幅降低开发门槛。**PromptAppGPT**则适合极快速的概念验证开发。 *   • **高度定制化与专业需求**：对于需要专业级逻辑推理和因果分析的任务，**camel-ai/owl**提供了强大的逻辑能力。对于多模态应用，**Qwen-Agent**的中文和多模态支持表现出色。 *   • **企业智能助手**：LlamaIndex（知识检索）+ LangGraph/autoGen（任务编排） *   • **研究分析平台**：GPT-Researcher/deep-research（信息搜集）+ Haystack（知识管理） *   • **业务流程自动化**：n8n（工作流自动化）+ Haystack（生产级检索）+ 企业内部系统 *   • **快速概念验证**：Dify/Langflow（可视化开发）/PromptAppGPT + 标准RAG组件 *   • **多模态能力成为标配**：新一代Agent框架如**Qwen-Agent**正积极整合文本、图像、音频和视频处理能力，使智能体能够理解和生成多种媒体形式的内容。Google的Gemini 2.5系列已经在跨模态处理与响应速度方面取得显著提升，实现了统一嵌入表示与跨模态注意力机制。 *   • **端到端训练提升性能**：传统模块化拼接方法（搜索模块+分析模块+写作模块）错误率叠加超过30%，而端到端训练方法直接将模型从问题训练到最终报告，错误率大幅降低76%。这一趋势使得Agent框架能够学习更加连贯的任务解决策略。 *   • **移动端与边缘部署**：随着**AutoGLM 2.0**等框架推出，智能体正从云端向移动设备扩展。智谱AI的解决方案通过云手机/云电脑支持，为AI配备专属云端设备，任务在后台24小时独立运行，不占用用户本地算力。 *   • **低成本与普惠化**：国产模型如GLM-4.5的成本较国外方案降低数量级，推动全民普惠应用。开源生态已吸引全球超10万开发者参与，形成医疗、法律等15个垂直领域优化分支。 *   • **Agentic RAG崛起**：传统RAG工作流程是静态单轮检索-生成，而Agentic RAG通过引入智能体机制实现了三大突破：动态决策引擎、多轮反思机制和验证闭环。这使系统能够像专业研究员般主动探索、验证与推理，在医疗、金融、科研等领域带来变革。 *   • **混合检索策略优化**：单纯向量检索已无法满足复杂需求，混合搜索结合向量搜索和关键词搜索（BM25），通过加权融合结果提供更准确的检索效果。重新排序技术也得到广泛应用，解决返回文档中部分不相关的问题。 *   • **图数据库增强推理**：基于知识图的RAG如**microsoft/graphrag**通过图数据库集成，提升了多跳推理能力，更适合复杂推理任务和学术研究场景。 *   • **处理能力专业化**：针对特定类型内容的深度处理能力越来越重要，如**RAGFlow**对表格和复杂文档结构的理解能力，使其在法律文档分析和合同审查场景中表现优异。 *   • **科研平民化**：DeepResearch工具通过降低专业门槛（如文献综述时间缩短70%）、重构成本结构（训练成本降低90%），使创新资源从\"技术寡头\"向\"大众开发者\"扩散。清华大学与北京航空航天大学联合团队的DeepSeek与DeepResearch工具，通过技术创新与开源策略，将数据采集、分析、可视化等复杂流程简化为\"类对话\"操作。 *   • **多智能体协作研究**：上海交大团队利用DeepResearch Agents实现科研自动化，通过规划Agent、检索Agent、验证Agent和写作Agent的协同工作，将科研人员从繁重的信息收集和整理工作中解放出来。 *   • **实时性与动态更新**：越来越多框架将支持流处理，能够处理流式数据并实时更新研究结果，非常适合新闻监测和社交媒体趋势分析场景。 *   • **可信度与验证机制**：为解决AI生成内容的可信度问题，新一代DeepResearch框架增加了自动生成文献引用、来源可信度评分、时效性评分和矛盾检测等功能。OpenAI的解法还包括前置\"灵魂拷问\"以锁定用户真实需求，以及思维链可视化让用户实时了解AI的思考过程。 *   • **云原生与异步架构**：新一代框架设计普遍采用云原生理念，支持分布式部署和弹性扩展。**AutoGLM 2.0**采用的端到端异步强化学习技术，显著提升移动端任务稳定性与跨应用操作鲁棒性。 *   • **联邦学习与隐私保护**：医院A的医疗Agent通过加密通道与医院B的研究Agent协作，实现知识共享时不暴露原始数据。这种联邦Agentic RAG架构在保护隐私的同时扩大了知识共享范围。 *   • **神经符号融合**：结合符号规则约束LLM幻觉和神经网络学习隐式模式的能力，提升系统可靠性和可解释性。 *   • **AI代理协议标准化**：新兴的AI Agent Protocols(A2A), Model Context Protocol(MCP)等协议致力于实现跨模块解耦和标准化交互，使不同框架和组件能够更加无缝地协作。 Image 11: 旺知识 Image 15 Image 16',\n",
       "  'score': 0.97891}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示搜索结果\n",
    "# 这将输出搜索到的文档列表，包含 URL 和内容摘要\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd7d5d",
   "metadata": {
    "id": "bafd7d5d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
