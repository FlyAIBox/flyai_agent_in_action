{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0d1bad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfe28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098df42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1406a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194a11bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1868.52 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 787.18 GB, Free: 1125.19 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {
    "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
   },
   "source": [
    "\n",
    "## LangGraph教程\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### **背景介绍**\n",
    "\n",
    "在 LangChain，我们的目标是让构建大型语言模型（LLM）应用变得简单。您可以构建的一种 LLM 应用就是**智能代理（Agent）**。构建智能代理非常令人兴奋，因为它们能够自动化以前不可能完成的各种任务。\n",
    "\n",
    "然而，在实践中，构建能够可靠执行这些任务的系统极其困难。在我们与用户合作将智能代理投入生产的过程中，我们发现通常需要更多的**控制**。例如，您可能需要智能代理始终优先调用某个特定的工具，或者根据其状态使用不同的提示词。\n",
    "\n",
    "为了解决这个问题，我们构建了 [**LangGraph**](https://langchain-ai.github.io/langgraph/) —— 一个用于构建智能代理和多智能体应用的框架。LangGraph 独立于 LangChain 包，其核心设计理念是帮助开发者为智能代理工作流添加更好的**精确性**和**控制力**，使其适合现实世界系统的复杂性。\n",
    "\n",
    "-----\n",
    "\n",
    "### **设置**\n",
    "\n",
    "在开始之前，请按照 `README` 文件中的说明创建环境并安装依赖项。\n",
    "\n",
    "-----\n",
    "\n",
    "### **聊天模型**\n",
    "\n",
    "在本课程中，我们将使用[**聊天模型（Chat Models）**](https://python.langchain.com/v0.2/docs/concepts/#chat-models)，它们的功能是接收一系列消息作为输入，并以聊天消息作为输出。LangChain 本身不托管任何聊天模型，而是依赖于第三方集成。 是 LangChain 中支持的第三方聊天模型集成列表！默认情况下，课程将使用[ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)  ，因为它既流行又性能出色。正如前面提到的，请确保您已设置好 `OPENAI_API_KEY`。\n",
    "\n",
    "我们将检查您的 `OPENAI_API_KEY` 是否已设置；如果未设置，系统会提示您输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bffd3e-2b2a-4168-a1d6-546a6e770b67",
   "metadata": {
    "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
   },
   "source": [
    "\n",
    "#### OpenAI国内代理\n",
    "\n",
    "为方便大家再国内使用OpenAI，Claude等国外模型\n",
    "\n",
    "和OpenAI国内代理API易社区争取到了如下权益：\n",
    "\n",
    "OpenAI国内代理地址\n",
    "https://api.apiyi.com/register/?aff_code=we80\n",
    "新用户注册送0.1美金，注册成功后在以下表格中填写你的账号，平台会再赠送2美金（5个工作日到账）\n",
    "\n",
    "---- 【腾讯文档】API易账号收集 https://docs.qq.com/form/page/DQm1qb1VBQU9wR2xq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9a52c8",
   "metadata": {
    "id": "0f9a52c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的依赖包\n",
    "# %pip install 是 Jupyter 中安装 Python 包的命令\n",
    "# --quiet 参数减少输出信息\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langchain_community==0.3.29 tavily-python==0.7.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50ac806-d94b-4930-9983-f34068671a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   /opt/conda\n",
      "lf                     /opt/conda/envs/lf\n",
      "flyai_agent_in_action * /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a15227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2a15227",
    "outputId": "2bc8601c-50bc-4353-fa6a-3c5d5c7ef112"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的模块\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 要设置的环境变量名称\n",
    "\n",
    "    功能:\n",
    "        - 检查环境变量是否已存在\n",
    "        - 如果不存在，则提示用户输入并设置\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):  # 检查环境变量是否已设置\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")  # 安全地获取用户输入\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {
    "id": "a326f35b"
   },
   "source": [
    "[这里](https://python.langchain.com/v0.2/docs/how_to/#chat-models)是一个有用的指南，介绍了您可以使用聊天模型做的所有事情，但我们在下面会展示一些重点。如果您按照 README 中的说明运行了 `pip install -r requirements.txt`，那么您已经安装了 `langchain-openai` 包。有了这个包，我们可以实例化我们的 `ChatOpenAI` 模型对象。如果您是第一次注册 API，您应该会收到[免费积分](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517)，可以应用于任何模型。您可以[在这里](https://openai.com/api/pricing/)查看各种模型的定价。笔记本将默认使用 `gpt-4o`，因为它在质量、价格和速度之间取得了良好的平衡[更多信息请参见这里](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini)，但您也可以选择价格较低的 `gpt-3.5` 系列模型。\n",
    "\n",
    "聊天模型有几个[标准参数](https://python.langchain.com/v0.2/docs/concepts/#chat-models)可以设置。最常见的两个是：\n",
    "\n",
    "* `model`：模型名称\n",
    "* `temperature`：采样温度\n",
    "\n",
    "`Temperature` 控制模型输出的随机性或创造性，其中低温度（接近 0）产生更确定性和专注的输出。这适合需要准确性或事实性响应的任务。高温度（接近 1）适合创造性任务或生成多样化响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19a54d3",
   "metadata": {
    "id": "e19a54d3"
   },
   "outputs": [],
   "source": [
    "# 导入 ChatOpenAI 类\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 创建 GPT-4o 聊天模型实例\n",
    "# model=\"gpt-4o\": 使用 GPT-4o 模型，这是 OpenAI 的最新模型\n",
    "# temperature=0: 设置温度为 0，使输出更加确定性和一致\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 创建 GPT-3.5 Turbo 聊天模型实例\n",
    "# model=\"gpt-3.5-turbo-0125\": 使用 GPT-3.5 Turbo 模型（2025年1月版本）\n",
    "# temperature=0: 同样设置为 0，确保输出的一致性\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {
    "id": "28450d1b"
   },
   "source": [
    "LangChain 中的聊天模型有许多[默认方法](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface)。在大多数情况下，我们将使用：\n",
    "\n",
    "* `stream`：流式返回响应的块\n",
    "* `invoke`：在输入上调用链\n",
    "\n",
    "如前所述，聊天模型接受[消息](https://python.langchain.com/v0.2/docs/concepts/#messages)作为输入。消息具有一个角色（描述谁在说消息）和一个内容属性。我们稍后会更多地讨论这个问题，但这里让我们先展示基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1280e1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1280e1b",
    "outputId": "08c10f33-0874-413f-870b-d2cf24c7dff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 17, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_5d7ee1b844', 'id': 'chatcmpl-CMPc9aVBMcxpL4GJtOOhHFer4Qj4X', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--60834869-de68-440a-b890-d1549e245e17-0', usage_metadata={'input_tokens': 17, 'output_tokens': 10, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入 HumanMessage 类，用于创建人类用户的消息\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 创建一个人类消息\n",
    "# content: 消息内容\n",
    "# name: 发送者的名称（可选）\n",
    "msg = HumanMessage(content=\"你好呀\", name=\"萤火AI百宝箱\")\n",
    "\n",
    "# 创建消息列表\n",
    "# 聊天模型通常接受消息列表作为输入\n",
    "messages = [msg]\n",
    "\n",
    "# 使用消息列表调用模型\n",
    "# invoke() 方法会发送消息给模型并返回响应\n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {
    "id": "cac73e4c"
   },
   "source": [
    "我们得到一个 `AIMessage` 响应。另外，请注意我们可以直接用字符串调用聊天模型。当字符串作为输入传递时，它会被转换为 `HumanMessage`，然后传递给底层模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27c6c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f27c6c9a",
    "outputId": "ee85a471-3314-43d1-c430-3066ee0844e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='学习LangChain的路线可以根据你的背景和目标进行调整。以下是一个通用的学习路线，适合对自然语言处理和人工智能有一定了解的学习者：\\n\\n### 1. 基础知识\\n- **Python编程**：确保你对Python有一定的掌握，因为LangChain是基于Python的。\\n- **自然语言处理（NLP）基础**：了解基本的NLP概念，如分词、词性标注、命名实体识别等。\\n- **机器学习基础**：熟悉基本的机器学习概念和常用算法。\\n\\n### 2. 初步了解LangChain\\n- **LangChain简介**：阅读LangChain的官方文档，了解其基本概念和功能。\\n- **安装与配置**：学习如何安装LangChain，并配置开发环境。\\n\\n### 3. 深入学习LangChain\\n- **核心模块**：了解LangChain的核心模块，如链（Chains）、代理（Agents）、提示（Prompts）等。\\n- **示例项目**：通过官方提供的示例项目，学习如何构建简单的LangChain应用。\\n- **API使用**：熟悉LangChain的API，学习如何调用和使用不同的功能。\\n\\n### 4. 实践与项目\\n- **小型项目**：尝试构建一个简单的项目，如聊天机器人或文本生成应用。\\n- **集成与扩展**：学习如何将LangChain与其他工具和库集成，如OpenAI的GPT模型。\\n- **优化与调试**：掌握调试技巧和性能优化方法，以提高应用的效率和稳定性。\\n\\n### 5. 高级主题\\n- **自定义组件**：学习如何创建自定义的链和代理，以满足特定需求。\\n- **大规模应用**：了解如何在大规模应用中使用LangChain，包括分布式计算和云部署。\\n- **最新研究**：关注LangChain的最新研究动态和社区更新，保持技术前沿。\\n\\n### 6. 社区与资源\\n- **参与社区**：加入LangChain的社区论坛或社交媒体群组，与其他开发者交流经验。\\n- **持续学习**：关注LangChain的更新日志和博客，学习新功能和最佳实践。\\n\\n通过以上步骤，你可以逐步掌握LangChain的使用，并能够开发出功能丰富的自然语言处理应用。记得在学习过程中多动手实践，以加深理解和提高技能。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 525, 'prompt_tokens': 11, 'total_tokens': 536, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_5d7ee1b844', 'id': 'chatcmpl-CMPcBn3nrRmpMVahCDQ6mCm7N9Khv', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--4777817c-eb9b-467b-bd57-d5f634984f6b-0', usage_metadata={'input_tokens': 11, 'output_tokens': 525, 'total_tokens': 536, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接使用字符串调用模型\n",
    "# 当传入字符串时，LangChain 会自动将其转换为 HumanMessage\n",
    "gpt4o_chat.invoke(\"LangChain学习路线\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc2f0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdc2f0ca",
    "outputId": "6adff1a8-56cb-4f18-fbbb-b8a95dae682e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain学习路线可以分为以下几个阶段：\\n\\n1. 初级阶段：\\n- 了解LangChain的基本概念和原理\\n- 学习LangChain的基本语法和数据结构\\n- 编写简单的LangChain程序，如Hello World程序\\n\\n2. 中级阶段：\\n- 深入学习LangChain的高级特性，如函数、模块、异常处理等\\n- 学习如何使用LangChain进行文件操作、网络编程等\\n- 开发一些小型的LangChain应用程序，如简单的文本编辑器、网络爬虫等\\n\\n3. 高级阶段：\\n- 学习LangChain的高级编程技巧，如元编程、并发编程等\\n- 深入研究LangChain的内部实现原理，如解释器、编译器等\\n- 参与开源项目或自己开发复杂的LangChain应用程序，如Web框架、游戏引擎等\\n\\n在学习过程中，建议多参与实际项目开发，通过实践来巩固所学知识，并不断提升自己的编程能力和解决问题的能力。同时，也可以参加LangChain社区的讨论和交流，与其他LangChain开发者分享经验和学习心得。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 359, 'prompt_tokens': 14, 'total_tokens': 373, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': None, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': None}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'id': 'chatcmpl-CMPcGBd6FgOy0kUdVHiBHNVGfAXSL', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a0e77d60-9a03-4a6a-af7a-a87c6d898fbd-0', usage_metadata={'input_tokens': 14, 'output_tokens': 359, 'total_tokens': 373, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 GPT-3.5 Turbo 模型调用\n",
    "# 同样可以直接传入字符串\n",
    "gpt35_chat.invoke(\"LangChain学习路线\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {
    "id": "582c0e5a"
   },
   "source": [
    "所有聊天模型的接口都是一致的，模型通常在每个笔记本启动时初始化一次。\n",
    "\n",
    "因此，如果您强烈偏好另一个提供商，您可以轻松地在模型之间切换，而无需更改下游代码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {
    "id": "3ad0069a"
   },
   "source": [
    "## 搜索工具\n",
    "\n",
    "您还会在 README 中看到 [Tavily](https://tavily.com/)，这是一个为 LLM 和 RAG 优化的搜索引擎，旨在提供高效、快速和持久的搜索结果。如前所述，注册很容易，并提供慷慨的免费层级。一些课程（在模块 4 中）将默认使用 Tavily，但当然，如果您想为自己修改代码，也可以使用其他搜索工具。\n",
    "\n",
    "**Tavily API 密钥 (API Key) 的注册和申请流程**通常非常直接，但具体步骤可能会有细微变化。以下是一般性的指导和您可能需要遵循的步骤：\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 访问 Tavily 官方网站\n",
    "\n",
    "首先，您需要访问 **Tavily 的官方网站**（您已经提供了链接：[https://tavily.com/](https://tavily.com/)）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 查找注册/登录或 API 页面\n",
    "\n",
    "在网站上，您需要找到以下选项之一：\n",
    "* **“Sign Up”（注册）** 或 **“Log In”（登录）** 按钮。\n",
    "* **“API”、“Developers”（开发者）** 或 **“Pricing”（定价）** 页面。\n",
    "\n",
    "通常，您需要先创建一个用户账户才能申请 API 密钥。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 注册新账户\n",
    "\n",
    "如果您是新用户：\n",
    "* 点击 **“Sign Up”** 或 **“注册”**。\n",
    "* 您可能需要提供一个 **电子邮件地址** 和设置一个 **密码**。\n",
    "* 有些服务也支持使用 **Google** 或其他第三方账户直接登录/注册。\n",
    "* 完成注册后，您可能需要**验证您的邮箱**（通过点击发送到您邮箱的链接）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 访问 API 仪表板/设置\n",
    "\n",
    "登录您的账户后，您需要进入到管理 API 密钥的区域，这通常被称为：\n",
    "* **“Dashboard”（仪表板）**\n",
    "* **“API Keys”（API 密钥）**\n",
    "* **“Settings”（设置）** 或 **“Profile”（个人资料）**\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 生成 API 密钥\n",
    "\n",
    "在这个 API 密钥管理页面：\n",
    "* 查找类似于 **“Create New Key”（创建新密钥）**、**“Generate API Key”（生成 API 密钥）** 或 **“Get Started”（开始使用）** 的按钮。\n",
    "* 点击该按钮，系统会**立即生成**一串独特的长字符，这就是您的 **Tavily API Key**。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 保存您的 API 密钥\n",
    "\n",
    "**重要提示：**\n",
    "* 生成的 API 密钥通常只会在**生成时显示一次**。\n",
    "* **请务必立即复制并安全地存储**（例如，在密码管理器或安全文档中）您的密钥。\n",
    "* 如果丢失，您可能需要生成一个新的密钥，而旧的密钥可能会被吊销。\n",
    "\n",
    "完成这些步骤后，您就可以在您的应用程序或开发环境中使用这个 **TAVILY\\_API\\_KEY** 来调用 Tavily 的搜索 API 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091dff13",
   "metadata": {
    "id": "091dff13"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# 设置 Tavily API 密钥\n",
    "# 这是使用 Tavily 搜索功能所必需的\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d69da9",
   "metadata": {
    "id": "52d69da9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tmp/ipykernel_182/4221156914.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "# 导入 Tavily 搜索结果工具\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 创建 Tavily 搜索实例\n",
    "# max_results=3: 设置最大返回结果数量为 3\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "# 执行搜索\n",
    "# 搜索 \"What is LangGraph?\" 并获取结果\n",
    "search_docs = tavily_search.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d06f87e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d06f87e6",
    "outputId": "5f9fe70c-6e2d-4f56-8217-563309f0118f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '【系统学习02】工作流vs Multi-Agent：Dify、LangGraph - 知乎专栏',\n",
       "  'url': 'https://zhuanlan.zhihu.com/p/1950863538311627763',\n",
       "  'content': 'Image 2: 血戎 本文把自己在 **Dify**、**LangGraph**、**AgentScope** 这三套工具上的实战感受放在一起对比，顺便聊聊 Multi-Agent 会不会像微服务那样走向“工程化”的未来。 | Dify (v1.6+) | DAG + 可视化编辑器 | 支持 Prompt 模板的变量替换 {{var}}，内置知识库检索（支持多源数据），工具调用需配置 OpenAPI 文档。 注意：内置的”Agent 节点”实际是工作流嵌套，非真正的 Multi-Agent | 上手快、低代码、适合业务流程 | 灵活性一般，多Agent弱（仅支持嵌套，无原生通信机制） | Multi-Agent 成功的关键不在于“有多少 Agent”，而在于 **通信方式、共享状态与角色分工**。 三大平台的 Multi-Agent 能力对比 | 平台 | 通信机制 | 多Agent支持 | 记忆 | 协作方式 | | 维度 | 工作流 | Multi-Agent | *   **Multi-Agent**：适合需要多角度判断、动态任务分解或角色并发协作的任务（例如复杂的市场分析、多角色对话系统、模拟谈判）。优点是灵活、能并行探索；缺点是调试和治理成本高。  2.   **可观测性**：日志、链路追踪、可视化监控会成为 Multi-Agent 平台标配（如：LangSmith）。 *   **方案**：**工作流**（流程清晰，各环节固定，无需多 Agent 协作） *   **方案**：**Multi-Agent**（需多角色协作，动态规划） *   **Multi-Agent**是你请来的一群网红大厨，灵感爆棚，但有时容易吵起来。 **未来趋势**：工作流和 Multi-Agent 不是非此即彼，而是互补共存。真正的高手，往往会两手都要抓——在流程性任务中用工作流保证稳定性，在探索性任务中用 Multi-Agent激发创造力。 **最终建议**：不要过早站队。先用工作流快速验证业务逻辑，再逐步引入 Multi-Agent 解决复杂场景。毕竟，AI 平台的终极目标，不是选择哪一派，而是让智能应用真正服务于人。 AI-Agent multi-agent Image 7: 血戎 AI Agent工作流实用手册：5种常见模式的实现与应用，助力生产环境稳定性 ====================================== 很多人认为使用AI Agent就是直接扔个提示词过去，然后等结果。做实验这样是没问题的，但要是想在生产环境稳定输出高质量结果，这套玩法就不行了。 核心问题是这种随意的提示方式根本扩展不… deephubmulti-agent 多 Agent 代理协同的三种架构 ============================= 原文： LangGraph: Multi-Agent Workflows 很高兴地介绍 langgraph 强大的用例 - 多代理工作流。在本博客中，我们将涵盖：&#34;multi-agen 多代理&#34;是什么意思？ 为什么“multi-agent 多… 混沌福王大模型端侧部署(On-Device AI) ===================== 前言手机与PC厂商从23年底，叙事中加入了一个概念--端侧AI 人工智能可以直接在移动设备上进行处理，无需连接到服务器或云端。它带来了许多好处，包括低延迟、更高的安全性和灵活性。即使设… 天晴Image 8: 复杂Multi-Agent系统设计思考 复杂Multi-Agent系统设计思考 =================== 一方小民',\n",
       "  'score': 0.98547},\n",
       " {'title': '基于LangGraph 构建Open Deep Research 架构设计与落地实践',\n",
       "  'url': 'https://blog.csdn.net/musicml/article/details/149551765',\n",
       "  'content': 'AI 搜索 Deep Research 已经成为最受欢迎的 AI 智能体应用之一。OpenAI、Anthropic、Perplexity 和谷歌都推出了 Deep Research 产品，这些产品能够利用各种上下文生成全面的报告。此外，还有很多开源的实现版本。 Research 非常适合 AI 智能体架构设计，因为它们可以灵活地应用不同的策略，利用中间结果指导探索过程。开放式深度研究使用 AI 智能体架构，并按照三步流程进行研究： 研究的目标是收集研究简报中请求的上下文信息。我们使用一个主管 AI 智能体进行研究。 **第二、研究子 AI 智能体** 如果我们将原始信息返回给主管，token 使用量可能会显著增加，主管将不得不解析更多的 token 以找到最有用的信息。因此，我们的子 AI 智能体清理其发现并将其返回给主管。 报告撰写的目的是利用子 AI 智能体收集的上下文信息满足研究简报中的请求。当主管认为收集的发现足以满足研究简报中的请求时，我们就可以开始撰写报告了。 #### **1、仅在易于并行化的任务中使用多 AI 智能体** #### 2、多 AI 智能体有助于在子研究主题之间隔离上下文 * “OpenAI 关于 AI 安全和对齐的哲学框架” * “Anthropic 关于 AI 安全和对齐的哲学框架” * “谷歌 DeepMind 关于 AI 安全和对齐的哲学框架” 在每次工具调用迭代中，单 AI 智能体需要同时处理来自三个独立主题的上下文。从 token 和延迟的角度来看，这是浪费。我们不需要关于 OpenAI 递归奖励建模方法的 token 来帮助我们生成关于 DeepMind 对齐哲学的下一个查询。另一个重要观察是，处理多个主题的单 AI 智能体会自然地在选择完成之前对每个主题进行**较少的深度研究**（搜索查询次数）。一个多 AI 智能体方法允许多个子 AI 智能体并行运行，每个子 AI 智能体都致力于一个独立的、专注的任务。将多 AI 智能体方法应用于研究可以捕捉到 Anthropic 报告的好处，并在我们自己的评估中得到突出：可以在每个子 AI 智能体中隔离子主题上下文。 #### 3、多 AI 智能体主管使系统能够调整所需的研究深度 💡多 AI 智能体主管允许灵活调整搜索策略。 研究是一项 token 密集型任务。Anthropic 报告称，他们的多 AI 智能体系统使用的 token 量比典型聊天应用多15倍！我们使用上下文工程来缓解这种情况。 我们将聊天历史压缩成一个研究简报，以防止之前消息的 token 膨胀。子 AI 智能体在将研究发现返回给主管之前，会修剪掉无关的 token 和信息。 如果没有足够的上下文工程，我们的 AI 智能体很容易遇到上下文窗口限制，从长的、原始的工具调用结果中。实际上，它还可以帮助节省 token 费用，避免 TPM 模型速率限制。 总之，开放式深度研究（Open Deep Research）是一种高效的研究工具，通过三步流程（范围界定、研究、报告撰写）灵活应对复杂的研究任务。它采用多 AI 智能体架构，子 AI 智能体并行处理子任务以提高效率，同时通过上下文工程优化 token 使用，减少成本和避免模型限制。此外，该工具可根据请求动态调整研究深度，确保高质量的响应。',\n",
       "  'score': 0.98538},\n",
       " {'title': '从零构建企业级多智能体系统：LangGraph 全流程实战+ 部署指南 ...',\n",
       "  'url': 'https://blog.csdn.net/sinat_28461591/article/details/147076125',\n",
       "  'content': '> 在大模型应用爆发的今天，如何构建一个可协作、可调试、可部署的多智能体系统？LangGraph 正在成为工程师的新宠。本文从原理剖析、流程建模、代码实战到前后端部署，手把手带你用 LangGraph 构建一个企业级 AI 助理系统，整合财务分析、法务审阅、报告生成与图表可视化等 Agent 模块，全面打通从用户输入到报告输出的全链路闭环。适合有工程思维的 AI 架构师、Agent 研发人员、以及大模型项目实践者。 * 七、总结：为什么 LangGraph 是 Agent 工程落地的必经之路 简而言之：**LangGraph 是一套专为「大模型系统化开发」而设计的状态驱动执行框架**，适用于多 Agent 协作、复杂任务控制、流程回滚、状态追踪等场景。 def legal_agent_node(state: dict) -> dict: 接下来，我们就将用 LangGraph 构建一个真实项目，从 0 到 1 实战部署一个多角色 Agent 系统。 def finance_agent_node(state: WorkflowState) -> WorkflowState: def legal_agent_node(state: WorkflowState) -> WorkflowState: from langgraph.graph import StateGraph * ✅ **LangGraph 服务**：调用多个 Agent + 工具执行任务 from langgraph_graph import app as graph_app  # 引入上一步构建的图 | 多线程调度 | LangGraph 原生支持 asyncio，可并发运行多个 Agent | ## 七、总结：为什么 LangGraph 是 Agent 工程落地的必经之路？ *LangGraph* 是 LangChain 生态中的高阶 API 框架，专为多角色协作 Agent 设计，核心思想是将复杂工作流分解为有向图结构，节点为原子操作，边为执行逻辑。*与*传统工作流引擎相比，*LangGraph* 支持 *Python* 函数/LLM 调用、内存共享状态管理和 LLM 实时决策，并集成 LangSmith 可视化调试工具。其架构演进从 2023 年 Q4 的原型设计到 2024 年 Q3 的多租户线程管理，逐步完善。*LangGraph* 通过增量更新策略优化状态管理，支持持久化检查点和多租户线 *LangGraph* 是一种用于*构建*复杂多模态对话系统的工具，它能够通过图结构的方式管理多个 Agent 和工具之间的交互关系。以下是关于 *LangGraph* 的一些具体*实战*案例和使用教程。 from langchain.agents import Tool, initialize\\\\_agent from *langgraph*.graphs import GraphBuilder import *langgraph* as lg',\n",
       "  'score': 0.9837}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示搜索结果\n",
    "# 这将输出搜索到的文档列表，包含 URL 和内容摘要\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd7d5d",
   "metadata": {
    "id": "bafd7d5d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
