{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d0d1bad",
      "metadata": {},
      "source": [
        "### 🔧 环境配置和检查\n",
        "\n",
        "#### 概述\n",
        "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
        "- 使用统一的conda环境\n",
        "- 通过国内镜像源快速安装依赖\n",
        "- 加速模型下载\n",
        "- 检查系统配置\n",
        "\n",
        "#### 配置步骤\n",
        "1. **Conda环境管理** - 激活统一的学习环境\n",
        "2. **包管理器优化** - 配置pip使用清华镜像源\n",
        "3. **模型下载加速** - 设置HuggingFace镜像代理\n",
        "4. **系统环境诊断** - 检查硬件和软件配置\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bfe28d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. 激活conda环境\n",
        "%%script bash\n",
        "# 初始化 conda\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate flyai_agent_in_action\n",
        "conda env list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098df42f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. 设置pip 为清华源\n",
        "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "%pip config list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1406a3d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. 设置HuggingFace代理\n",
        "%env HF_ENDPOINT=https://hf-mirror.com\n",
        "# 验证：使用shell命令检查\n",
        "!echo $HF_ENDPOINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194a11bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 🔍 环境信息检查脚本\n",
        "#\n",
        "# 本脚本的作用：\n",
        "# 1. 安装 pandas 库用于数据表格展示\n",
        "# 2. 检查系统的各项配置信息\n",
        "# 3. 生成详细的环境报告表格\n",
        "#\n",
        "# 对于初学者来说，这个步骤帮助您：\n",
        "# - 了解当前运行环境的硬件配置\n",
        "# - 确认是否满足模型运行的最低要求\n",
        "# - 学习如何通过代码获取系统信息\n",
        "\n",
        "# 安装 pandas 库 - 用于创建和展示数据表格\n",
        "# pandas 是 Python 中最流行的数据处理和分析库\n",
        "%pip install pandas==2.2.2 tabulate==0.9.0\n",
        "\n",
        "import platform # 导入 platform 模块以获取系统信息\n",
        "import os # 导入 os 模块以与操作系统交互\n",
        "import subprocess # 导入 subprocess 模块以运行外部命令\n",
        "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
        "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
        "\n",
        "# 获取 CPU 信息的函数，包括核心数量\n",
        "def get_cpu_info():\n",
        "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
        "    physical_cores = \"N/A\"\n",
        "    logical_cores = \"N/A\"\n",
        "\n",
        "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
        "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
        "        try:\n",
        "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
        "            import wmi\n",
        "            c = wmi.WMI()\n",
        "            for proc in c.Win32_Processor():\n",
        "                physical_cores = proc.NumberOfCores\n",
        "                logical_cores = proc.NumberOfLogicalProcessors\n",
        "        except:\n",
        "            pass # 如果 WMI 不可用，忽略错误\n",
        "\n",
        "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
        "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
        "        try:\n",
        "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_brand, stderr_brand = process_brand.communicate()\n",
        "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
        "\n",
        "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_physical, stderr_physical = process_physical.communicate()\n",
        "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
        "\n",
        "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_logical, stderr_logical = process_logical.communicate()\n",
        "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
        "\n",
        "        except:\n",
        "            cpu_info = \"Could not retrieve CPU info\"\n",
        "            physical_cores = \"N/A\"\n",
        "            logical_cores = \"N/A\"\n",
        "\n",
        "    else:  # Linux 系统\n",
        "        try:\n",
        "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
        "            with open('/proc/cpuinfo') as f:\n",
        "                physical_cores_count = 0\n",
        "                logical_cores_count = 0\n",
        "                cpu_info_lines = []\n",
        "                for line in f:\n",
        "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
        "                        if not cpu_info: # 只获取第一个 model name\n",
        "                            cpu_info = line.split(': ')[1].strip()\n",
        "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
        "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
        "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
        "                        logical_cores_count += 1\n",
        "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
        "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
        "                if not cpu_info:\n",
        "                     cpu_info = \"Could not retrieve CPU info\"\n",
        "\n",
        "        except:\n",
        "            cpu_info = \"Could not retrieve CPU info\"\n",
        "            physical_cores = \"N/A\"\n",
        "            logical_cores = \"N/A\"\n",
        "\n",
        "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
        "\n",
        "\n",
        "# 获取内存信息的函数\n",
        "def get_memory_info():\n",
        "    mem_info = \"\" # 初始化内存信息字符串\n",
        "    if platform.system() == \"Windows\":\n",
        "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
        "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
        "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
        "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
        "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
        "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
        "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
        "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
        "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
        "    else:  # Linux 系统\n",
        "        try:\n",
        "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
        "            with open('/proc/meminfo') as f:\n",
        "                total_mem_kb = 0\n",
        "                available_mem_kb = 0\n",
        "                for line in f:\n",
        "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
        "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
        "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
        "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
        "\n",
        "                if total_mem_kb > 0:\n",
        "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
        "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
        "                    if available_mem_kb > 0:\n",
        "                        available_mem_gb = available_mem_kb / (1024**2)\n",
        "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
        "                else:\n",
        "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
        "\n",
        "        except:\n",
        "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
        "    return mem_info # 返回内存信息\n",
        "\n",
        "# 获取 GPU 信息的函数，包括显存\n",
        "def get_gpu_info():\n",
        "    try:\n",
        "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
        "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # 如果命令成功执行\n",
        "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
        "            gpu_info_list = []\n",
        "            for line in gpu_lines:\n",
        "                name, memory = line.split(', ')\n",
        "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
        "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
        "        else:\n",
        "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
        "            try:\n",
        "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
        "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
        "                     # 简单解析输出中的 product 名称和显存\n",
        "                    gpu_info_lines = []\n",
        "                    current_gpu = {}\n",
        "                    for line in result_lshw.stdout.splitlines():\n",
        "                        if 'product:' in line:\n",
        "                             if current_gpu:\n",
        "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
        "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
        "                        elif 'size:' in line and 'memory' in line:\n",
        "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
        "\n",
        "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
        "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
        "\n",
        "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
        "                else:\n",
        "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
        "            except FileNotFoundError:\n",
        "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
        "    except FileNotFoundError:\n",
        "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
        "\n",
        "\n",
        "# 获取 CUDA 版本的函数\n",
        "def get_cuda_version():\n",
        "    try:\n",
        "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
        "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # 如果命令成功执行\n",
        "            for line in result.stdout.splitlines():\n",
        "                if 'release' in line: # 查找包含 'release' 的行\n",
        "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
        "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
        "    except FileNotFoundError:\n",
        "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
        "\n",
        "# 获取 Python 版本的函数\n",
        "def get_python_version():\n",
        "    return platform.python_version() # 获取 Python 版本\n",
        "\n",
        "# 获取 Conda 版本的函数\n",
        "def get_conda_version():\n",
        "    try:\n",
        "        # 尝试使用 conda --version 获取 Conda 版本\n",
        "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # 如果命令成功执行\n",
        "            return result.stdout.strip() # 返回 Conda 版本\n",
        "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
        "    except FileNotFoundError:\n",
        "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
        "\n",
        "# 获取物理磁盘空间信息的函数\n",
        "def get_disk_space():\n",
        "    try:\n",
        "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
        "        total_gb = total / (1024**3) # 转换为 GB\n",
        "        used_gb = used / (1024**3) # 转换为 GB\n",
        "        free_gb = free / (1024**3) # 转换为 GB\n",
        "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
        "    except Exception as e:\n",
        "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
        "\n",
        "# 获取环境信息\n",
        "os_name = platform.system() # 获取操作系统名称\n",
        "os_version = platform.release() # 获取操作系统版本\n",
        "if os_name == \"Linux\":\n",
        "    try:\n",
        "        # 在 Linux 上尝试获取发行版和版本\n",
        "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
        "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
        "            for line in lsb_info.stdout.splitlines():\n",
        "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
        "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
        "                    break # 找到后退出循环\n",
        "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
        "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
        "                     # 尝试获取 codename\n",
        "                     try:\n",
        "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
        "                         if codename_info.returncode == 0:\n",
        "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
        "                     except:\n",
        "                         pass # 如果获取 codename 失败则忽略\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        pass # lsb_release 可能未安装，忽略错误\n",
        "\n",
        "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
        "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
        "memory_info = get_memory_info() # 调用函数获取内存信息\n",
        "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
        "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
        "python_version = get_python_version() # 调用函数获取 Python 版本\n",
        "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
        "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
        "\n",
        "\n",
        "# 创建用于存储数据的字典\n",
        "env_data = {\n",
        "    \"项目\": [ # 项目名称列表\n",
        "        \"操作系统\",\n",
        "        \"CPU 信息\",\n",
        "        \"内存信息\",\n",
        "        \"GPU 信息\",\n",
        "        \"CUDA 信息\",\n",
        "        \"Python 版本\",\n",
        "        \"Conda 版本\",\n",
        "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
        "    ],\n",
        "    \"信息\": [ # 对应的信息列表\n",
        "        full_os_info,\n",
        "        cpu_info,\n",
        "        memory_info,\n",
        "        gpu_info,\n",
        "        cuda_version,\n",
        "        python_version,\n",
        "        conda_version,\n",
        "        disk_info # 添加物理磁盘空间信息\n",
        "    ]\n",
        "}\n",
        "\n",
        "# 创建一个 pandas DataFrame\n",
        "df = pd.DataFrame(env_data)\n",
        "\n",
        "# 打印表格\n",
        "print(\"### 环境信息\") # 打印标题\n",
        "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
      "metadata": {
        "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
      },
      "source": [
        "\n",
        "## LangGraph教程\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "### **背景介绍**\n",
        "\n",
        "在 LangChain，我们的目标是让构建大型语言模型（LLM）应用变得简单。您可以构建的一种 LLM 应用就是**智能代理（Agent）**。构建智能代理非常令人兴奋，因为它们能够自动化以前不可能完成的各种任务。\n",
        "\n",
        "然而，在实践中，构建能够可靠执行这些任务的系统极其困难。在我们与用户合作将智能代理投入生产的过程中，我们发现通常需要更多的**控制**。例如，您可能需要智能代理始终优先调用某个特定的工具，或者根据其状态使用不同的提示词。\n",
        "\n",
        "为了解决这个问题，我们构建了 [**LangGraph**](https://langchain-ai.github.io/langgraph/) —— 一个用于构建智能代理和多智能体应用的框架。LangGraph 独立于 LangChain 包，其核心设计理念是帮助开发者为智能代理工作流添加更好的**精确性**和**控制力**，使其适合现实世界系统的复杂性。\n",
        "\n",
        "-----\n",
        "\n",
        "### **课程结构**\n",
        "\n",
        "本课程由一系列模块组成，每个模块都专注于一个与 LangGraph 相关的主题。您会看到每个模块都有一个文件夹，其中包含一系列**笔记本（notebooks）**。每本笔记本都配有视频，以帮助您理解概念，但这些笔记本也是独立的，这意味着它们包含了详细的解释，可以脱离视频独立观看。每个模块文件夹还包含一个 `studio` 文件夹，其中包含一组图（graphs），这些图可以加载到 [**LangGraph Studio**](https://github.com/langchain-ai/langgraph-studio) 中，这是我们用于构建 LangGraph 应用的集成开发环境（IDE）。\n",
        "\n",
        "-----\n",
        "\n",
        "### **设置**\n",
        "\n",
        "在开始之前，请按照 `README` 文件中的说明创建环境并安装依赖项。\n",
        "\n",
        "-----\n",
        "\n",
        "### **聊天模型**\n",
        "\n",
        "在本课程中，我们将使用[**聊天模型（Chat Models）**](https://python.langchain.com/v0.2/docs/concepts/#chat-models)，它们的功能是接收一系列消息作为输入，并以聊天消息作为输出。LangChain 本身不托管任何聊天模型，而是依赖于第三方集成。 是 LangChain 中支持的第三方聊天模型集成列表！默认情况下，课程将使用[ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)  ，因为它既流行又性能出色。正如前面提到的，请确保您已设置好 `OPENAI_API_KEY`。\n",
        "\n",
        "我们将检查您的 `OPENAI_API_KEY` 是否已设置；如果未设置，系统会提示您输入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f9a52c8",
      "metadata": {
        "id": "0f9a52c8"
      },
      "outputs": [],
      "source": [
        "# 安装必要的依赖包\n",
        "# %%capture --no-stderr 用于隐藏安装过程中的输出信息\n",
        "# %pip install 是 Jupyter 中安装 Python 包的命令\n",
        "# --quiet 参数减少输出信息\n",
        "# -U 参数表示升级到最新版本\n",
        "%%capture --no-stderr\n",
        "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langchain_community==0.3.29 tavily-python==0.7.11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "q0z67qBCdFxT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q0z67qBCdFxT",
        "outputId": "153c92b1-7c4a-41bf-db49-11f9e462eec1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: langchain-openai\n",
            "Version: 0.3.32\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 0.3.75\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity, typing-extensions\n",
            "Required-by: langchain, langchain-community, langchain-openai, langchain-text-splitters\n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.3.29\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, httpx-sse, langchain, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n",
            "---\n",
            "Name: tavily-python\n",
            "Version: 0.7.11\n",
            "Summary: Python wrapper for the Tavily API\n",
            "Home-page: https://github.com/tavily-ai/tavily-python\n",
            "Author: Tavily AI\n",
            "Author-email: support@tavily.com\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: httpx, requests, tiktoken\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "%pip show  langchain_openai langchain_core langchain_community tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c2a15227",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2a15227",
        "outputId": "2bc8601c-50bc-4353-fa6a-3c5d5c7ef112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "OPENAI_BASE_URL: ··········\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的模块\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    \"\"\"\n",
        "    设置环境变量的辅助函数\n",
        "\n",
        "    参数:\n",
        "        var (str): 要设置的环境变量名称\n",
        "\n",
        "    功能:\n",
        "        - 检查环境变量是否已存在\n",
        "        - 如果不存在，则提示用户输入并设置\n",
        "    \"\"\"\n",
        "    if not os.environ.get(var):  # 检查环境变量是否已设置\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")  # 安全地获取用户输入\n",
        "\n",
        "# 设置 OpenAI API 密钥\n",
        "# 这是使用 OpenAI 模型所必需的\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
        "_set_env(\"OPENAI_BASE_URL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a326f35b",
      "metadata": {
        "id": "a326f35b"
      },
      "source": [
        "[这里](https://python.langchain.com/v0.2/docs/how_to/#chat-models)是一个有用的指南，介绍了您可以使用聊天模型做的所有事情，但我们在下面会展示一些重点。如果您按照 README 中的说明运行了 `pip install -r requirements.txt`，那么您已经安装了 `langchain-openai` 包。有了这个包，我们可以实例化我们的 `ChatOpenAI` 模型对象。如果您是第一次注册 API，您应该会收到[免费积分](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517)，可以应用于任何模型。您可以[在这里](https://openai.com/api/pricing/)查看各种模型的定价。笔记本将默认使用 `gpt-4o`，因为它在质量、价格和速度之间取得了良好的平衡[更多信息请参见这里](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini)，但您也可以选择价格较低的 `gpt-3.5` 系列模型。\n",
        "\n",
        "聊天模型有几个[标准参数](https://python.langchain.com/v0.2/docs/concepts/#chat-models)可以设置。最常见的两个是：\n",
        "\n",
        "* `model`：模型名称\n",
        "* `temperature`：采样温度\n",
        "\n",
        "`Temperature` 控制模型输出的随机性或创造性，其中低温度（接近 0）产生更确定性和专注的输出。这适合需要准确性或事实性响应的任务。高温度（接近 1）适合创造性任务或生成多样化响应。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e19a54d3",
      "metadata": {
        "id": "e19a54d3"
      },
      "outputs": [],
      "source": [
        "# 导入 ChatOpenAI 类\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 创建 GPT-4o 聊天模型实例\n",
        "# model=\"gpt-4o\": 使用 GPT-4o 模型，这是 OpenAI 的最新模型\n",
        "# temperature=0: 设置温度为 0，使输出更加确定性和一致\n",
        "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# 创建 GPT-3.5 Turbo 聊天模型实例\n",
        "# model=\"gpt-3.5-turbo-0125\": 使用 GPT-3.5 Turbo 模型（2025年1月版本）\n",
        "# temperature=0: 同样设置为 0，确保输出的一致性\n",
        "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28450d1b",
      "metadata": {
        "id": "28450d1b"
      },
      "source": [
        "LangChain 中的聊天模型有许多[默认方法](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface)。在大多数情况下，我们将使用：\n",
        "\n",
        "* `stream`：流式返回响应的块\n",
        "* `invoke`：在输入上调用链\n",
        "\n",
        "如前所述，聊天模型接受[消息](https://python.langchain.com/v0.2/docs/concepts/#messages)作为输入。消息具有一个角色（描述谁在说消息）和一个内容属性。我们稍后会更多地讨论这个问题，但这里让我们先展示基础知识。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b1280e1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1280e1b",
        "outputId": "08c10f33-0874-413f-870b-d2cf24c7dff8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='你好！有什么我可以帮助你的吗？', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 17, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_5d58a6052a', 'id': 'chatcmpl-CDo2rrzfzFs9YulezWYT2KspS9lxW', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--c4ea9394-a482-48be-97e8-c817e5178569-0', usage_metadata={'input_tokens': 17, 'output_tokens': 10, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 导入 HumanMessage 类，用于创建人类用户的消息\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# 创建一个人类消息\n",
        "# content: 消息内容\n",
        "# name: 发送者的名称（可选）\n",
        "msg = HumanMessage(content=\"你好呀\", name=\"萤火AI百宝箱\")\n",
        "\n",
        "# 创建消息列表\n",
        "# 聊天模型通常接受消息列表作为输入\n",
        "messages = [msg]\n",
        "\n",
        "# 使用消息列表调用模型\n",
        "# invoke() 方法会发送消息给模型并返回响应\n",
        "gpt4o_chat.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac73e4c",
      "metadata": {
        "id": "cac73e4c"
      },
      "source": [
        "我们得到一个 `AIMessage` 响应。另外，请注意我们可以直接用字符串调用聊天模型。当字符串作为输入传递时，它会被转换为 `HumanMessage`，然后传递给底层模型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f27c6c9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f27c6c9a",
        "outputId": "ee85a471-3314-43d1-c430-3066ee0844e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='学习LangChain的路线可以根据你的背景和目标进行调整。以下是一个通用的学习路线，适合对自然语言处理和人工智能有一定了解的学习者：\\n\\n### 1. 基础知识\\n- **Python编程**：确保你对Python有基本的理解，因为LangChain是基于Python的。\\n- **自然语言处理（NLP）基础**：了解基本的NLP概念，如分词、词性标注、命名实体识别等。\\n- **机器学习基础**：熟悉基本的机器学习概念和框架，如TensorFlow或PyTorch。\\n\\n### 2. 理解LangChain\\n- **LangChain简介**：了解LangChain的基本概念、目标和应用场景。\\n- **安装和设置**：学习如何安装LangChain及其依赖项。\\n\\n### 3. LangChain核心概念\\n- **链（Chain）**：学习如何构建和使用链来处理复杂的NLP任务。\\n- **提示（Prompt）**：了解如何设计和优化提示以获得更好的模型输出。\\n- **记忆（Memory）**：学习如何在对话中使用记忆来保持上下文。\\n- **代理（Agent）**：了解如何使用代理来处理动态任务和决策。\\n\\n### 4. 实践与应用\\n- **示例项目**：通过官方文档或社区资源中的示例项目，实践LangChain的使用。\\n- **自定义应用**：尝试构建一个简单的应用，如聊天机器人或文本生成工具。\\n- **集成与部署**：学习如何将LangChain应用集成到现有系统中，并进行部署。\\n\\n### 5. 深入学习与优化\\n- **高级功能**：探索LangChain的高级功能，如自定义链、复杂的代理策略等。\\n- **性能优化**：学习如何优化LangChain应用的性能，包括提示优化和模型选择。\\n\\n### 6. 社区与资源\\n- **参与社区**：加入LangChain的社区，如GitHub、论坛或Slack，获取最新资讯和支持。\\n- **持续学习**：关注LangChain的更新和新功能，保持学习和实践。\\n\\n通过以上步骤，你可以逐步掌握LangChain的使用，并能够应用于实际项目中。记得根据自己的进度和需求调整学习计划。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 500, 'prompt_tokens': 11, 'total_tokens': 511, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-CDo3DjOSyfuWnMqi8NgTn6v9Zn3jB', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--72e7d68e-6fb6-4bf1-8cfb-0132f9f35abb-0', usage_metadata={'input_tokens': 11, 'output_tokens': 500, 'total_tokens': 511, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 直接使用字符串调用模型\n",
        "# 当传入字符串时，LangChain 会自动将其转换为 HumanMessage\n",
        "gpt4o_chat.invoke(\"LangChain学习路线\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fdc2f0ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdc2f0ca",
        "outputId": "6adff1a8-56cb-4f18-fbbb-b8a95dae682e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain学习路线是一个逐步学习编程语言的过程，可以帮助你逐步掌握编程语言的基础知识和技能。以下是一个可能的LangChain学习路线：\\n\\n1. 学习基础知识：首先，你需要学习编程语言的基础知识，包括语法、数据类型、变量、运算符等。你可以通过阅读教科书、参加在线课程或观看教学视频来学习这些知识。\\n\\n2. 练习编程：一旦你掌握了基础知识，你可以开始练习编程。你可以通过编写简单的程序来巩固所学知识，并逐渐提高难度。\\n\\n3. 学习高级特性：一旦你熟悉了编程语言的基础知识，你可以开始学习一些高级特性，如函数、类、模块等。这些特性可以帮助你编写更复杂的程序。\\n\\n4. 实践项目：最后，你可以通过实践项目来应用所学知识。你可以选择一个感兴趣的主题，编写一个完整的程序，并将其部署到实际环境中。\\n\\n通过以上学习路线，你可以逐步掌握编程语言的基础知识和技能，并成为一名优秀的程序员。祝你学习顺利！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 404, 'prompt_tokens': 14, 'total_tokens': 418, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'id': 'chatcmpl-CDo3PsHiW4twU824Yk4VIiRiyzSaX', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--6a480cf1-d9b2-40d7-897f-5591fb47fb6a-0', usage_metadata={'input_tokens': 14, 'output_tokens': 404, 'total_tokens': 418, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 使用 GPT-3.5 Turbo 模型调用\n",
        "# 同样可以直接传入字符串\n",
        "gpt35_chat.invoke(\"LangChain学习路线\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c0e5a",
      "metadata": {
        "id": "582c0e5a"
      },
      "source": [
        "所有聊天模型的接口都是一致的，模型通常在每个笔记本启动时初始化一次。\n",
        "\n",
        "因此，如果您强烈偏好另一个提供商，您可以轻松地在模型之间切换，而无需更改下游代码。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad0069a",
      "metadata": {
        "id": "3ad0069a"
      },
      "source": [
        "## 搜索工具\n",
        "\n",
        "您还会在 README 中看到 [Tavily](https://tavily.com/)，这是一个为 LLM 和 RAG 优化的搜索引擎，旨在提供高效、快速和持久的搜索结果。如前所述，注册很容易，并提供慷慨的免费层级。一些课程（在模块 4 中）将默认使用 Tavily，但当然，如果您想为自己修改代码，也可以使用其他搜索工具。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "091dff13",
      "metadata": {
        "id": "091dff13"
      },
      "outputs": [],
      "source": [
        "# 设置 Tavily API 密钥\n",
        "# 这是使用 Tavily 搜索功能所必需的\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "52d69da9",
      "metadata": {
        "id": "52d69da9"
      },
      "outputs": [],
      "source": [
        "# 导入 Tavily 搜索结果工具\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# 创建 Tavily 搜索实例\n",
        "# max_results=3: 设置最大返回结果数量为 3\n",
        "tavily_search = TavilySearchResults(max_results=3)\n",
        "\n",
        "# 执行搜索\n",
        "# 搜索 \"What is LangGraph?\" 并获取结果\n",
        "search_docs = tavily_search.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d06f87e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d06f87e6",
        "outputId": "5f9fe70c-6e2d-4f56-8217-563309f0118f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'title': '从零构建企业级多智能体系统：LangGraph 全流程实战+ 部署指南 ...',\n",
              "  'url': 'https://blog.csdn.net/sinat_28461591/article/details/147076125',\n",
              "  'content': 'LangGraph 是 LangChain 生态中的高阶 API 框架，专为多角色协作 Agent 设计，核心思想是将复杂工作流分解为有向图结构，节点为原子操作，边为执行逻辑。与传统工作流引擎相比，LangGraph 支持 Python 函数/LLM 调用、内存共享状态管理和 LLM 实时决策，并集成 LangSmith 可视化调试工具。其架构演进从 2023 年 Q4 的原型设计到 2024 年 Q3 的多租户线程管理，逐步完善。LangGraph 通过增量更新策略优化状态管理，支持持久化检查点和多租户线\\n\\n革新！AI应用架构师助力数据分析师AI智能体在智能化数据洞察中革新\\n\\n最新发布\\n\\nAI天才研究院\\n\\n08-02\\n\\n927 [...] 一、LangGraph 是什么？为什么说它重新定义了 Agent 编排\\n 二、核心原理解析：图结构 + 状态驱动，才是智能体的最佳形式\\n 三、实战项目：打造一个企业级多角色 Agent 系统（含工具链）\\n  + 3.1 项目目标与场景模拟\\n  + 3.2 项目架构图与核心流程设计\\n  + 3.3 代码实战：节点构建与图结构编排\\n  + 3.4 工具调用与图表生成节点实战\\n  + 3.5 前后端部署与接口封装实战\\n  + 3.6 部署建议与工程总结\\n 四、部署实战：如何将 LangGraph 系统跑在本地或线上？\\n 五、AutoGen vs LangGraph：哪个才适合生产环境？\\n 六、最佳实践建议 + 常见问题汇总\\n 七、总结：为什么 LangGraph 是 Agent 工程落地的必经之路\\n\\n## 一、LangGraph 是什么？为什么说它重新定义了 Agent 编排\\n\\n> 单个模型 ≠ 有用的产品，真正复杂的智能体系统，需要 任务分解、角色分工、状态管理和异常回滚。\\n\\nLangChain 虽然为我们提供了丰富的工具封装和链式调用接口，但当面对以下场景时，它开始显得力不从心： [...] 多个 Agent 之间需要协同、顺序执行、互相等待\\n 工具调用需要在特定状态下才触发\\n 用户输入的状态可能随时间演变，甚至中断后要恢复执行\\n\\n这时候，LangGraph 作为 LangChain 的“下一代协作框架”应运而生。\\n\\n### 1.1 LangGraph 的定位：面向工程场景的智能体系统框架\\n\\nLangGraph 是由 LangChain 团队在 2024 年底正式推出的开源框架。它不是用来替代 LangChain，而是专为以下问题而生的：\\n\\n| 场景 | LangChain（传统链式编排） | LangGraph（图结构执行） |\\n --- \\n| 多 Agent 协同 | 不擅长 | 原生支持 |\\n| 任务状态转移 | 需手动编码判断 | 自动推理状态流转 |\\n| 调试和回滚 | 无状态追踪 | 可记录每一步状态并回滚 |\\n| 工程级部署 | 需配合自建组件 | 内置缓存 / 跳转 / 容错 |\\n\\n简而言之：LangGraph 是一套专为「大模型系统化开发」而设计的状态驱动执行框架，适用于多 Agent 协作、复杂任务控制、流程回滚、状态追踪等场景。',\n",
              "  'score': 0.62852204},\n",
              " {'title': 'LangGraph vs. OpenAI Agent SDK – 哪个代理构建框架更适合您？ 原创',\n",
              "  'url': 'https://blog.csdn.net/robot_learner/article/details/147444339',\n",
              "  'content': '工具的定义，可以参考这篇文章，写的比较详细了，比较方便的就是使用 tools 这个注解。雨飞：使用智普清言的Tools功能实现Tool _Agent_ _LangGraph_ 中最基础的类型是 StatefulGraph，这种图就会在每一个Node之间传递不同的状态信息。然后每一个节点会根据自己定义的逻辑去 _更_ 新这个状态信息。具体来说，可以继承 TypeDict 这个类去定义状态，下图我们就定义了有四个变量的信息。input:这是输入字符串，代表用户的主要请求。\\n\\n_LangGraph_ 实战入门学习\\n\\nhuang9604的博客\\n\\n07-05Image 50 2126 \\n\\n_LangGraph_ 是一个功能强大的库，用于 _构建_ 基于大型语言模型（LLM）的有状态、多参与者应用程序。它旨在创建 _代理_ 和多 _代理_ 工作流，以实现复杂的任务和交互。\\n\\n垂直智能体：企业AI落地的正确打开方式 最新发布\\n\\nchainso23的博客\\n\\n05-18Image 51 735 [...] ### 3. 深度解析：优势与劣势\\n\\n#### A. 架构与灵活性\\n\\n   LangGraph\\n       显式图结构：手动连接节点、边、循环与终止条件。\\n       适合科研、调试或合规审核——流程完全可见且可复现。\\n\\n   OpenAI Agent SDK\\n       隐式事件循环：您只需定义工具与检索，后端自动决策和路由。\\n       上手极其简单，但无法在执行流程中插入自定义路由逻辑。\\n\\n#### B. 模型与工具生态\\n\\n   LangGraph\\n       兼容所有 LangChain 支持的 LLM 与嵌入模型，可接入本地模型。\\n       担心供应商锁定或需要离线使用时尤为重要。\\n\\n   OpenAI Agent SDK\\n       深度集成 GPT‑4o 的函数调用、Code Interpreter、检索等。\\n       但只能使用 OpenAI 自家的模型。\\n\\n#### C. 托管与运维\\n\\n   LangGraph\\n       自己提供运行环境（本地、云端、私有网络均可）。\\n       您要管理自动扩缩容、重试、监控、安全等运维工作。 [...] 9.   6. 未来可能的演变\\n10.   7. 结论\\n11.   英文原文\\n12.   AI好书推荐\\n\\n展开全部Image 73\\n\\n收起Image 74\\n\\nImage 75\\n\\n### 目录\\n\\n1.   1. 为什么这很重要\\n2.   2. 快速一览对比\\n3.   3. 深度解析：优势与劣势\\n4.       1.   A. 架构与灵活性\\n    2.   B. 模型与工具生态\\n    3.   C. 托管与运维\\n    4.   D. 成本模式\\n    5.   E. 可观测性与调试\\n    6.   F. 治理\\n\\n5.   4. 优缺点汇总\\n6.       1.   LangGraph\\n    2.   OpenAI Agent SDK\\n\\n7.   5. 应该如何选择？\\n8.       1.   在以下情况时选择 LangGraph：\\n    2.   在以下情况时选择 OpenAI Agent SDK：\\n    3.   混合模式（常见生产模式）\\n\\n9.   6. 未来可能的演变\\n10.   7. 结论\\n11.   英文原文\\n12.   AI好书推荐',\n",
              "  'score': 0.6205532},\n",
              " {'title': '企业级AI落地必备！LangGraph结构化输出技术图谱：5步构建可信赖 ...',\n",
              "  'url': 'https://blog.csdn.net/Python_cocola/article/details/151080978',\n",
              "  'content': '公安备案号11010502030143\\n 京ICP备19004658号\\n 京网文〔2020〕1039-165号\\n 经营性网站备案信息\\n 北京互联网违法和不良信息举报中心\\n 家长监护\\n 网络110报警服务\\n 中国互联网举报中心\\n Chrome商店下载\\n 账号管理规范\\n 版权与免责声明\\n 版权申诉\\n 出版物许可证\\n 营业执照\\n ©1999-2025北京创新乐知网络技术有限公司\\n\\n登录后您可以享受以下权益：\\n\\n 免费复制代码\\n 和博主大V互动\\n 下载海量资源\\n 发动态/写文章/加入社区\\n\\n×\\n\\n评论\\n\\n被折叠的  条评论\\n为什么被折叠?\\n到【灌水乐园】发言\\n\\n查看更多评论\\n\\n添加红包\\n\\n成就一亿技术人!\\n\\nhope\\\\_wisdom\\n\\n发出的红包\\n\\n实付元\\n\\n使用余额支付\\n\\n点击重新获取\\n\\n扫码支付\\n\\n钱包余额\\n0\\n\\n抵扣说明：\\n\\n1.余额是钱包充值的虚拟货币，按照1:1的比例进行支付金额的抵扣。  \\n 2.余额无法直接购买下载，可以购买VIP、付费专栏及课程。\\n\\n余额充值\\n\\n确定取消\\n\\n举报\\n\\n 包含不实信息\\n 涉及个人隐私\\n\\n请选择具体原因（必选）\\n\\n 侮辱谩骂\\n 诽谤 [...] ```\\n\\n 1\\n 2\\n\\n### 最佳实践\\n\\n1. 选择合适的模式类型：\\n2.  需要运行时验证 → 使用Pydantic的BaseModel\\n    仅需静态类型检查 → 使用TypedDict\\n    需要流式输出 → 使用TypedDict或JSON Schema\\n3. 提供清晰的字段描述：字段的名称和描述对模型理解输出格式非常重要\\n4. 使用中文字段名：在中文应用场景中，使用中文字段名可以提高模型的理解准确性\\n5. 合理设置可选字段：使用Optional类型和默认值来处理不确定的信息\\n\\n### 总结\\n\\n结构化输出是现代AI应用开发中的重要技术。通过LangGraph的`.with_structured_output()`方法，我们可以轻松让大语言模型返回格式化的数据，提高应用的可靠性和可维护性。\\n\\n无论是使用Pydantic进行严格的数据验证，还是使用TypedDict进行灵活的字典操作，或是直接使用JSON Schema，都能满足不同场景下的需求。关键是根据具体的应用场景选择最合适的实现方式。 [...] ###### 文中涉及到的完整版的大模型 AI 学习资料已经上传CSDN，朋友们如果需要可以微信扫描下方CSDN官方认证二维码免费领取【`保证100%免费`】\\n\\nPython笔记领取【CSDN官方认证】\\n\\n微信名片\\n\\n确定要放弃本次机会？\\n\\n福利倒计时\\n\\n:\\n:\\n\\n立减 ¥\\n\\n普通VIP年卡可用\\n\\n立即使用\\n\\nPython\\\\_金钱豹\\n\\n关注\\n关注\\n\\n 15\\n\\n  点赞\\n 踩\\n 12\\n\\n  收藏\\n\\n  觉得还不错?\\n  一键收藏\\n 0\\n\\n  评论\\n 分享\\n\\n  复制链接\\n\\n  分享到 QQ\\n\\n  分享到新浪微博\\n\\n  扫一扫\\n 举报\\n\\n  举报\\n\\n参与评论\\n您还未登录，请先\\n登录\\n后发表或查看评论\\n\\n 关于我们\\n 招贤纳士\\n 商务合作\\n 寻求报道\\n 400-660-0108\\n kefu@csdn.net\\n 在线客服\\n 工作时间 8:30-22:00',\n",
              "  'score': 0.56258565}]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 显示搜索结果\n",
        "# 这将输出搜索到的文档列表，包含 URL 和内容摘要\n",
        "search_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bafd7d5d",
      "metadata": {
        "id": "bafd7d5d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
