{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27039d4",
   "metadata": {},
   "source": [
    "### 🔧 环境配置和检查\n\n#### 概述\n\n本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n\n- 使用统一的conda环境：激活统一的学习环境\n- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n- 加速模型下载：设置HuggingFace镜像代理\n- 检查系统配置：检查硬件和软件配置\n\n#### 配置\n\n- **所需环境及其依赖已经部署好**\n- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n\n# 1. 激活 conda 环境 (仅对当前单元格有效)\neval \"$(conda shell.bash hook)\"\nconda activate flyai_agent_in_action\n\necho \"=========================================\"\necho \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\necho \"=========================================\"\n\n# 2. 检查当前激活的环境\nCURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n\nif [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n    echo \"\"\n    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\nelse\n    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n    echo \"\"\n    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n    echo \"\"\n    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n    echo \"\"\n    echo \"%%script bash\"\n    echo \"# 必须在每个单元格都执行\"\n    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n    echo \"conda activate flyai_agent_in_action\"\nfi\n\necho \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36541eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/langchain-academy/blob/fly101/module-1/chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e",
   "metadata": {
    "id": "ee55d3da-c53a-4c76-b46f-8e0d602e072e"
   },
   "source": [
    "# Chain（链）\n",
    "\n",
    "## Review（回顾）\n",
    "\n",
    "我们此前构建了一个简单的有向图：包含节点、普通边，以及“条件边（根据判断分支）”。\n",
    "\n",
    "## Goals（目标）\n",
    "\n",
    "现在我们把 4 个核心概念串成一条“链”（Chain）（参考文档链接）：\n",
    "\n",
    "- 使用[聊天消息](https://python.langchain.com/docs/concepts/#messages)作为图的状态（state）\n",
    "- 在图的节点中使用[聊天模型](https://python.langchain.com/docs/concepts/#chat-models)\n",
    "- 将[工具](https://python.langchain.com/docs/concepts/#tools)绑定到聊天模型\n",
    "- 在图的节点中[执行工具调用](https://python.langchain.com/docs/concepts/#functiontool-calling)\n",
    "\n",
    "![链式结构示意图](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbab08dd607b08df5e1101_chain1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b",
   "metadata": {
    "id": "a55e2e80-a718-4aaf-99b9-371157b34a4b"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install --quiet -U langchain_openai langchain_core langgraph\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e",
   "metadata": {
    "id": "ae5ac2d0-c7b0-4a20-86e5-4b6ed15ec20e"
   },
   "source": [
    "## Messages（消息）\n",
    "\n",
    "聊天模型使用[`messages`](https://python.langchain.com/v0.2/docs/concepts/#messages)来承载对话上下文，其中每条消息都有“角色（role）”。\n",
    "\n",
    "LangChain 支持多种消息类型：`HumanMessage`（用户）、`AIMessage`（模型/助手）、`SystemMessage`（系统指令）、`ToolMessage`（工具调用返回）。\n",
    "\n",
    "它们分别表示：用户发言、模型回复、面向模型的系统指令、以及某个工具调用产生的消息。\n",
    "\n",
    "下面我们来创建一个消息列表。\n",
    "\n",
    "每条消息通常包含：\n",
    "\n",
    "- `content`：消息内容（必填）\n",
    "- `name`：消息作者（可选）\n",
    "- `response_metadata`：响应元数据（可选，通常由模型提供方自动填充于 `AIMessage`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "866b5321-a238-4a9e-af9e-f11a131b5f11",
    "outputId": "5e3a9b11-8a6e-4d35-84a9-73c120f7ee90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "你说你在研究海洋哺乳动物？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "是的，没错。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "好的，你想了解什么？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "我想了解在美国哪里最适合看虎鲸。\n"
     ]
    }
   ],
   "source": [
    "# 初学者提示：本示例演示如何构造“对话消息列表（messages）”，供聊天模型作为上下文输入。\n",
    "# - HumanMessage 表示“人类（用户）”消息\n",
    "# - AIMessage 表示“模型（助手）”消息\n",
    "# - 每条消息可包含：content（内容）、name（可选的说话者名）、response_metadata（模型返回时的元数据）\n",
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# 我们用一个列表来按时间顺序保存对话历史。\n",
    "messages = [AIMessage(content=f\"你说你在研究海洋哺乳动物？\", name=\"Model\")]\n",
    "# 追加一条来自用户的消息\n",
    "messages.append(HumanMessage(content=f\"是的，没错。\",name=\"Lance\"))\n",
    "# 再追加一条来自模型的消息\n",
    "messages.append(AIMessage(content=f\"好的，你想了解什么？\", name=\"Model\"))\n",
    "# 用户继续追问\n",
    "messages.append(HumanMessage(content=f\"我想了解在美国哪里最适合看虎鲸。\", name=\"Lance\"))\n",
    "\n",
    "# 以更易读的格式打印每条消息，便于检查结构与顺序\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e",
   "metadata": {
    "id": "0ca48df0-b639-4ff1-a777-ffe2185d991e"
   },
   "source": [
    "## Chat Models（聊天模型）\n",
    "\n",
    "[聊天模型](https://python.langchain.com/v0.2/docs/concepts/#chat-models)以“消息序列”作为输入，并支持上文提到的多种消息类型。\n",
    "\n",
    "可选模型提供商有很多（见上方链接）。这里以 OpenAI 为例。\n",
    "\n",
    "我们先检查环境变量 `OPENAI_API_KEY` 是否已设置；如果没有，将在 Notebook 中提示你安全输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2652d5ec-7602-4220-bc6e-b90783ab287b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2652d5ec-7602-4220-bc6e-b90783ab287b",
    "outputId": "68425b9c-858a-4d31-f4a4-1e85ff693b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n"
     ]
    }
   ],
   "source": [
    "# 初学者提示：许多大模型 SDK 通过环境变量读取密钥（如 OPENAI_API_KEY）。\n",
    "# 下面的辅助函数用于在未设置时，交互式地提示输入并写入当前进程环境变量。\n",
    "# 注意：Notebook 会话结束后，需重新设置或在系统环境中长期配置。\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # 若系统环境中没有该变量，则通过安全输入进行设置\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置 OpenAI 的 API Key，供 ChatOpenAI 使用\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5",
   "metadata": {
    "id": "ceae53d4-14f5-4bf3-a953-cc465240f5b5"
   },
   "source": [
    "我们可以加载一个聊天模型，并用我们的消息列表调用它。\n",
    "\n",
    "你会看到返回的是一条 `AIMessage`，其中包含特定的 `response_metadata`（响应元数据）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "95b99ad4-5753-49d3-a916-a9e949722c01",
    "outputId": "ea6f343d-a027-44fb-8c89-88fa1ae1c6bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
       "      pre.function-repr-contents {\n",
       "        overflow-x: auto;\n",
       "        padding: 8px 12px;\n",
       "        max-height: 500px;\n",
       "      }\n",
       "\n",
       "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
       "        cursor: pointer;\n",
       "        max-height: 100px;\n",
       "      }\n",
       "    </style>\n",
       "    <pre style=\"white-space: initial; background:\n",
       "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
       "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
       "\n",
       "AIMessage is returned from a chat model as a response to a prompt.\n",
       "\n",
       "This message represents the output of the model and consists of both\n",
       "the raw output as returned by the model together standardized fields\n",
       "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
       "      <script>\n",
       "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
       "        for (const element of document.querySelectorAll('.filepath')) {\n",
       "          element.style.display = 'block'\n",
       "          element.onclick = (event) => {\n",
       "            event.preventDefault();\n",
       "            event.stopPropagation();\n",
       "            google.colab.files.view(element.textContent, 154);\n",
       "          };\n",
       "        }\n",
       "      }\n",
       "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
       "        element.onclick = (event) => {\n",
       "          event.preventDefault();\n",
       "          event.stopPropagation();\n",
       "          element.classList.toggle('function-repr-contents-collapsed');\n",
       "        };\n",
       "      }\n",
       "      </script>\n",
       "      </div>"
      ],
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "result = llm.invoke(messages)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88d60338-c892-4d04-a83f-878de4a76a6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88d60338-c892-4d04-a83f-878de4a76a6a",
    "outputId": "4028ee13-4e57-43f5-abd5-d9656333e735"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='如果你想在美国观赏虎鲸，华盛顿州的圣胡安群岛是一个非常理想的地点。这片区域是北太平洋虎鲸（也称为逆戟鲸）在春季和夏季常驻的地方，尤其是捕鱼的时节。岛屿附近的海域生态多样，有丰富的食物来源，吸引了许多虎鲸来此捕食和生活。\\n\\n此外，阿拉斯加的内湾水域、加利福尼亚海岸，以及俄勒冈州和华盛顿州沿海也可能提供一些观赏虎鲸的机会，特别是在季节性迁徙期间。不过，圣胡安群岛被认为是观赏虎鲸的首选地点之一，因为这里有定期的船只观鲸旅游，可以近距离观察这些壮观的动物。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 64, 'total_tokens': 251, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_5d58a6052a', 'id': 'chatcmpl-CE8vgZ6OyVVPmlSruDc2sfIIX82K8', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a993107d-1070-4844-928c-16eef5d02d3d-0', usage_metadata={'input_tokens': 64, 'output_tokens': 187, 'total_tokens': 251, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3a29654-6b8e-4eda-9cec-22fabb9b8620",
    "outputId": "e2511420-ae4d-4d9a-e3f4-f2138449a9f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 187,\n",
       "  'prompt_tokens': 64,\n",
       "  'total_tokens': 251,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
       "  'input_tokens': 0,\n",
       "  'output_tokens': 0,\n",
       "  'input_tokens_details': None},\n",
       " 'model_name': 'gpt-4o',\n",
       " 'system_fingerprint': 'fp_5d58a6052a',\n",
       " 'id': 'chatcmpl-CE8vgZ6OyVVPmlSruDc2sfIIX82K8',\n",
       " 'service_tier': None,\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4718bd5c-5314-4405-a164-f1fe912ae306",
   "metadata": {
    "id": "4718bd5c-5314-4405-a164-f1fe912ae306"
   },
   "source": [
    "## Tools\n",
    "当您希望模型与外部系统交互时，工具很有用。\n",
    "外部系统（例如 API）通常需要特定的输入模式或有效负载，而不是自然语言。\n",
    "例如，当我们绑定 API 作为一种工具时，我们赋予模型对所需输入模式的感知。\n",
    "该模型将根据用户的自然语言输入选择调用工具。\n",
    "并且，它将返回符合该工具模式的输出。\n",
    "[许多LLM提供商都支持工具调用](https://python.langchain.com/docs/integrations/chat/) ，并且 [LangChain中的工具调用接口](https://blog.langchain.com/improving-core-tool-interfaces-and-docs-in-langchain/)非常简单。\n",
    "您可以简单地将任何 Python function 传递到 ChatModel.bind_tools(function) 。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a942b1",
   "metadata": {
    "id": "17a942b1"
   },
   "source": [
    "我们用一个极简示例演示工具调用。\n",
    "\n",
    "这里把 `multiply` 函数当作我们的“工具”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1",
   "metadata": {
    "id": "928faf56-1a1a-4c5f-b97d-bd64d8e166d1"
   },
   "outputs": [],
   "source": [
    "# 初学者提示：把一个 Python 函数以“工具（tool）”的形式绑定给聊天模型。\n",
    "# 作用：当任务需要结构化操作（如计算、查库、调 API），模型会优先生成工具调用参数并返回 tool_calls。\n",
    "# 关键点：\n",
    "# - 函数签名即是工具的“输入模式”，参数类型应明确；\n",
    "# - 返回值需可序列化、简洁明确；\n",
    "# - bind_tools([...]) 将工具元信息注册给模型。\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "# 将 multiply 工具绑定到聊天模型实例\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f9dba",
   "metadata": {
    "id": "8a3f9dba"
   },
   "source": [
    "如果我们传递一个输入 - 例如 \"What is 2 multiplied by 3\" - 我们会看到返回一个工具调用。\n",
    "\n",
    "工具调用具有与我们函数的输入模式以及要调用的函数的名称相匹配的特定参数。\n",
    "\n",
    "```\n",
    "{'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544",
   "metadata": {
    "id": "9edbe13e-cc72-4685-ac97-2ebb4ceb2544"
   },
   "outputs": [],
   "source": [
    "tool_call = llm_with_tools.invoke([HumanMessage(content=f\"What is 2 multiplied by 3\", name=\"Lance\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78178cb-fa43-45b5-be5e-5a22bda5a5e7",
   "metadata": {
    "id": "a78178cb-fa43-45b5-be5e-5a22bda5a5e7",
    "outputId": "bb1c3fce-3c00-4815-a1b9-a4f81c854880"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'call_lBBBNo5oYpHGRqwxNaNRbsiT',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e",
   "metadata": {
    "id": "21c10f9a-2372-486b-9305-55b7c41ecd6e"
   },
   "source": [
    "## Using messages as state\n",
    "\n",
    "有了这些基础，我们现在可以在图形状态中使用[`messages`](https://python.langchain.com/docs/concepts/#messages).\n",
    "\n",
    "\n",
    "我们将状态 `MessagesState` 定义为具有单个键 `messages` 的 `TypedDict` 。\n",
    "\n",
    "`messages` 只是一个消息列表，正如我们上面所定义的（例如 `HumanMessage` 等）。 我们将状态 `MessagesState` 定义为具有单个键消息的 `TypedDict` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca",
   "metadata": {
    "id": "3699dd5c-398c-43c7-b496-fd87e55e11ca"
   },
   "outputs": [],
   "source": [
    "# 定义图状态（State）：使用 TypedDict 定义键 messages\n",
    "# 含义：messages 用于保存对话历史（列表），元素类型是 AnyMessage（可包含 Human/AI/Tool 等消息）\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: list[AnyMessage]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e",
   "metadata": {
    "id": "211cba3e-ebba-4b91-a539-1cbc28b4a40e"
   },
   "source": [
    "## Reducers（归并器）\n",
    "\n",
    "这里会遇到一个小问题：\n",
    "\n",
    "按默认行为，每个节点返回的 `messages` 新值会[覆盖](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers)旧值。\n",
    "\n",
    "但在对话场景中，我们希望在运行过程中不断“追加”消息到 `messages`，而不是覆盖。\n",
    "\n",
    "为此可以使用[归并器（reducer）](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers)来定义“状态更新的合并规则”。\n",
    "\n",
    "- 如果没有为某个键指定 reducer，则该键的更新将采用“覆盖”语义。\n",
    "- 若要把消息追加到列表，应使用内置的 `add_messages` 归并器。\n",
    "\n",
    "做法是：在状态类型中为 `messages` 键添加 `add_messages` 作为元数据注解，从而实现“追加而非覆盖”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5",
   "metadata": {
    "id": "6b33eb72-3197-4870-b9a3-0da8056c40c5"
   },
   "outputs": [],
   "source": [
    "# Reducer（归并器）：指定当节点返回新的 messages 时，如何与已有状态合并。\n",
    "# add_messages 的语义是“追加消息而非覆盖”，保证完整的对话上下文得以保留。\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663e574-ba15-46be-a37c-48c8052d693b",
   "metadata": {
    "id": "3663e574-ba15-46be-a37c-48c8052d693b"
   },
   "source": [
    "将消息列表保存在图状态中非常常见，LangGraph 已内置 [`MessagesState`](https://langchain-ai.github.io/langgraph/concepts/low_level/#messagesstate)。\n",
    "\n",
    "该类型具备以下特性：\n",
    "\n",
    "- 预置一个 `messages` 键\n",
    "- 值类型为 `list[AnyMessage]`\n",
    "- 已默认使用 `add_messages` 归并器（自动追加）\n",
    "\n",
    "因此通常直接使用内置的 `MessagesState`，比自定义 `TypedDict` 更简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ab516ee-eab1-4856-8210-99f1fe499672",
   "metadata": {
    "id": "9ab516ee-eab1-4856-8210-99f1fe499672"
   },
   "outputs": [],
   "source": [
    "# LangGraph 内置 MessagesState：已包含 messages 键与 add_messages 逻辑。\n",
    "# 如需扩展，可继承后新增自定义键（如检索开关、工具开关、用户画像等）。\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class MessagesState(MessagesState):\n",
    "    # Add any keys needed beyond messages, which is pre-built\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c",
   "metadata": {
    "id": "36b0fff7-60a2-4582-8f12-3a3ab6633d6c"
   },
   "source": [
    "进一步了解：下面用一个最小示例单独演示 `add_messages` 归并器如何把新消息追加到已有列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23ffea76-16a5-4053-a1bc-91e0101d91dc",
    "outputId": "334b2884-a3cd-4d40-da31-98f286350d4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='您好！有什么可以为您效劳的？', additional_kwargs={}, response_metadata={}, name='Model', id='dee4d536-1ab0-4d40-a79c-9a7f76e8a791'),\n",
       " HumanMessage(content='我正在寻找有关海洋生物学的信息。', additional_kwargs={}, response_metadata={}, name='Lance', id='1562f9f6-0d99-4125-bfe7-d26e3eff2047'),\n",
       " AIMessage(content='好的，我很乐意帮忙。您具体对什么感兴趣？', additional_kwargs={}, response_metadata={}, name='Model', id='9d758673-f136-409c-873e-fabc4693241d')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial state\n",
    "initial_messages = [AIMessage(content=\"您好！有什么可以为您效劳的？\", name=\"Model\"),\n",
    "           HumanMessage(content=\"我正在寻找有关海洋生物学的信息。\", name=\"Lance\")\n",
    "                   ]\n",
    "\n",
    "# New message to add\n",
    "new_message = AIMessage(content=\"好的，我很乐意帮忙。您具体对什么感兴趣？\", name=\"Model\")\n",
    "\n",
    "# Test\n",
    "add_messages(initial_messages , new_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2",
   "metadata": {
    "id": "485adccc-f262-49dd-af4f-a30e9b6a48e2"
   },
   "source": [
    "## Our graph（我们的图）\n",
    "\n",
    "下面用内置的 `MessagesState` 搭建一条最简单的“链式”对话图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "b5306639-7e6a-44be-8471-8d2631701cfb",
    "outputId": "5dbfbbfd-ae1f-4637-da38-9a579157f6dc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 节点函数：接收并返回“状态增量（partial state）”\n",
    "# 入参 state：包含键 messages 的字典（对话历史）\n",
    "# 返回：{\"messages\": [...]} —— 追加一条模型回复（可能包含工具调用）\n",
    "# 注意：由于上文使用了 add_messages 作为 reducer，这里返回的列表会被“追加”到现有状态。\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"]) ]}\n",
    "\n",
    "# 构建图\n",
    "builder = StateGraph(MessagesState)\n",
    "# 注册节点：名称 \"tool_calling_llm\" 对应上面的节点函数\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "# 拓扑：从 START 进入该节点，再从该节点到 END，形成一条“链”\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_edge(\"tool_calling_llm\", END)\n",
    "# 编译图为可执行对象\n",
    "graph = builder.compile()\n",
    "\n",
    "# 可视化：通过 Mermaid 渲染图结构\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8909771-7786-47d6-a53d-6bbc3b365737",
   "metadata": {
    "id": "e8909771-7786-47d6-a53d-6bbc3b365737"
   },
   "source": [
    "当输入 `Hello!` 这样的问候语时，模型会直接自然语言回复，不会触发任何工具调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "983e2487-c0a5-40a2-afbc-aa53ff49fefc",
    "outputId": "c6d96e6a-7084-437b-9ba8-945c4ede7dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# 运行图（不涉及工具调用的普通对话）：\n",
    "# 期望：模型直接以自然语言回复，不返回 tool_calls。\n",
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello!\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba",
   "metadata": {
    "id": "3588688b-efd9-4dbc-abf2-7903e3ef89ba"
   },
   "source": [
    "当模型判断用户输入/任务需要某个工具所提供的能力时，它会选择触发相应的工具调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fe8b042-ecc8-426f-995e-cc1bbaf7cacc",
    "outputId": "8d801ca5-8184-49f3-c4b4-8c48d50906d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_ymaemHaIDegCIPK7qutwPkh0)\n",
      " Call ID: call_ymaemHaIDegCIPK7qutwPkh0\n",
      "  Args:\n",
      "    a: 2\n",
      "    b: 3\n"
     ]
    }
   ],
   "source": [
    "# 运行图（触发工具调用的请求）：\n",
    "# 期望：模型返回带有 tool_calls 的 AIMessage，工具名与参数应符合 multiply(a, b)。\n",
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Multiply 2 and 3\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fbae3",
   "metadata": {
    "id": "311fbae3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}