{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "# ğŸ”§ ç¯å¢ƒé…ç½®å’Œæ£€æŸ¥\n",
    "\n",
    "## æ¦‚è¿°\n",
    "æœ¬æ•™ç¨‹éœ€è¦ç‰¹å®šçš„ç¯å¢ƒé…ç½®ä»¥ç¡®ä¿æœ€ä½³å­¦ä¹ ä½“éªŒã€‚ä»¥ä¸‹é…ç½®å°†å¸®åŠ©æ‚¨ï¼š\n",
    "- ä½¿ç”¨ç»Ÿä¸€çš„condaç¯å¢ƒ\n",
    "- é€šè¿‡å›½å†…é•œåƒæºå¿«é€Ÿå®‰è£…ä¾èµ–\n",
    "- åŠ é€Ÿæ¨¡å‹ä¸‹è½½\n",
    "- æ£€æŸ¥ç³»ç»Ÿé…ç½®\n",
    "\n",
    "## é…ç½®æ­¥éª¤\n",
    "1. **Condaç¯å¢ƒç®¡ç†** - æ¿€æ´»ç»Ÿä¸€çš„å­¦ä¹ ç¯å¢ƒ\n",
    "2. **åŒ…ç®¡ç†å™¨ä¼˜åŒ–** - é…ç½®pipä½¿ç”¨æ¸…åé•œåƒæº\n",
    "3. **æ¨¡å‹ä¸‹è½½åŠ é€Ÿ** - è®¾ç½®HuggingFaceé•œåƒä»£ç†\n",
    "4. **ç³»ç»Ÿç¯å¢ƒè¯Šæ–­** - æ£€æŸ¥ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. æ¿€æ´»condaç¯å¢ƒ\n",
    "%%script bash\n",
    "# åˆå§‹åŒ– conda\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. è®¾ç½®pip ä¸ºæ¸…åæº\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. è®¾ç½®HuggingFaceä»£ç†\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# éªŒè¯ï¼šä½¿ç”¨shellå‘½ä»¤æ£€æŸ¥\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
    "#\n",
    "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
    "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
    "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
    "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
    "#\n",
    "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©æ‚¨ï¼š\n",
    "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
    "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
    "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "\n",
    "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
    "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
    "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
    "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
    "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
    "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
    "        try:\n",
    "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
    "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "\n",
    "\n",
    "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    if platform.system() == \"Windows\":\n",
    "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
    "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
    "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
    "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
    "\n",
    "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
    "        else:\n",
    "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "\n",
    "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
    "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
    "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_python_version():\n",
    "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
    "\n",
    "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
    "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
    "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
    "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
    "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
    "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
    "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
    "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
    "                     # å°è¯•è·å– codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
    "                     except:\n",
    "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
    "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
    "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
    "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
    "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
    "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "\n",
    "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "env_data = {\n",
    "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
    "        \"æ“ä½œç³»ç»Ÿ\",\n",
    "        \"CPU ä¿¡æ¯\",\n",
    "        \"å†…å­˜ä¿¡æ¯\",\n",
    "        \"GPU ä¿¡æ¯\",\n",
    "        \"CUDA ä¿¡æ¯\",\n",
    "        \"Python ç‰ˆæœ¬\",\n",
    "        \"Conda ç‰ˆæœ¬\",\n",
    "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
    "    ],\n",
    "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# æ‰“å°è¡¨æ ¼\n",
    "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
    "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/langchain-academy/blob/fly101/module-1/deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20242c4-0010-4065-89f6-0e0b16c7da6e",
   "metadata": {
    "id": "c20242c4-0010-4065-89f6-0e0b16c7da6e"
   },
   "source": [
    "# éƒ¨ç½²ï¼ˆDeploymentï¼‰\n",
    "\n",
    "## å›é¡¾ï¼ˆReviewï¼‰\n",
    "\n",
    "æˆ‘ä»¬å·²ç»æ„å»ºäº†ä¸€ä¸ªå¸¦æœ‰â€œè®°å¿†èƒ½åŠ›â€çš„æ™ºèƒ½ä½“ï¼ˆagentï¼‰ï¼š\n",
    "\n",
    "- `act`ï¼šè®©å¤§æ¨¡å‹è°ƒç”¨é¢„å…ˆæ³¨å†Œçš„å·¥å…·ï¼ˆToolsï¼‰\n",
    "- `observe`ï¼šå°†å·¥å…·çš„è¾“å‡ºç»“æœåé¦ˆç»™å¤§æ¨¡å‹\n",
    "- `reason`ï¼šè®©å¤§æ¨¡å‹åŸºäºå·¥å…·è¾“å‡ºè¿›è¡Œæ¨ç†ï¼Œå†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆç»§ç»­è°ƒç”¨å·¥å…·ï¼Œæˆ–ç›´æ¥å›å¤ï¼‰\n",
    "- `persist state`ï¼šä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹ï¼ˆin-memory checkpointerï¼‰ä¿å­˜å¯¹è¯ä¸çŠ¶æ€ï¼Œæ”¯æŒé•¿å¯¹è¯ä¸ä¸­æ–­ç»­è·‘\n",
    "\n",
    "## ç›®æ ‡ï¼ˆGoalsï¼‰\n",
    "\n",
    "ä¸‹é¢æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•åœ¨æœ¬åœ°é€šè¿‡ Studioï¼Œä»¥åŠåœ¨ `LangGraph Cloud` ä¸Šï¼Œå®é™…éƒ¨ç½²å¹¶è¿è¡Œè¿™ä¸ªæ™ºèƒ½ä½“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f348498b-f277-4514-b163-fe5ed9afe6fa",
   "metadata": {
    "id": "f348498b-f277-4514-b163-fe5ed9afe6fa"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install --quiet -U langgraph_sdk langchain_core\n",
    "%pip install --quiet langgraph_sdk==0.2.6 langchain_core==0.3.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0f4a7-82ee-4458-bd9a-e246ce2dc4ae",
   "metadata": {
    "id": "e4d0f4a7-82ee-4458-bd9a-e246ce2dc4ae"
   },
   "source": [
    "## æ ¸å¿ƒæ¦‚å¿µï¼ˆConceptsï¼‰\n",
    "\n",
    "éœ€è¦å…ˆç†è§£ä»¥ä¸‹æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "\n",
    "`LangGraph` â€”â€”\n",
    "- ä¸€ä¸ªç”¨äºæ„å»ºæ™ºèƒ½ä½“ï¼ˆagentï¼‰å·¥ä½œæµçš„ Python / JavaScript åº“ã€‚\n",
    "\n",
    "`LangGraph API` â€”â€”\n",
    "- å°† graphï¼ˆå·¥ä½œæµï¼‰ä»£ç æ‰“åŒ…æˆå¯æœåŠ¡åŒ–çš„æ¥å£ã€‚\n",
    "- æä¾›ä»»åŠ¡é˜Ÿåˆ—ï¼Œç”¨äºç®¡ç†å¼‚æ­¥æ‰§è¡Œã€‚\n",
    "- æä¾›çŠ¶æ€æŒä¹…åŒ–ï¼Œä¿è¯å¤šè½®äº¤äº’çš„ä¸Šä¸‹æ–‡è¿ç»­ã€‚\n",
    "\n",
    "`LangGraph Cloud` â€”â€”\n",
    "- å®˜æ–¹æ‰˜ç®¡çš„ LangGraph API äº‘æœåŠ¡ã€‚\n",
    "- æ”¯æŒä» GitHub ä»“åº“ç›´æ¥éƒ¨ç½² graphã€‚\n",
    "- æä¾›ç›‘æ§ä¸è¿½è¸ªèƒ½åŠ›ï¼ˆmonitoring & tracingï¼‰ã€‚\n",
    "- æ¯æ¬¡éƒ¨ç½²éƒ½ä¼šè·å¾—å”¯ä¸€è®¿é—® URLã€‚\n",
    "\n",
    "`LangGraph Studio` â€”â€”\n",
    "- LangGraph åº”ç”¨çš„é›†æˆå¼€å‘ç¯å¢ƒï¼ˆIDEï¼‰ã€‚\n",
    "- ä»¥å‰åç«¯åˆ†ç¦»æ–¹å¼å·¥ä½œï¼šå‰ç«¯æ˜¯å¯è§†åŒ–ç•Œé¢ï¼Œåç«¯ä¾èµ– APIã€‚\n",
    "- å¯åœ¨æœ¬åœ°è¿è¡Œï¼Œä¹Ÿå¯è¿æ¥äº‘ç«¯éƒ¨ç½²è¿›è¡Œè°ƒè¯•ã€‚\n",
    "\n",
    "`LangGraph SDK` â€”â€”\n",
    "- ç”¨äºä»¥ç¼–ç¨‹æ–¹å¼è®¿é—® LangGraph çš„ Python åº“ã€‚\n",
    "- æ— è®ºè¿æ¥æœ¬åœ°è¿˜æ˜¯äº‘ç«¯ï¼Œæ¥å£ä¿æŒä¸€è‡´ã€‚\n",
    "- æ”¯æŒåˆ›å»ºå®¢æˆ·ç«¯ã€ç®¡ç† assistantã€ç®¡ç† threadã€æ‰§è¡Œ run ç­‰ã€‚\n",
    "\n",
    "## æœ¬åœ°æµ‹è¯•ï¼ˆTesting Locallyï¼‰\n",
    "\n",
    "**âš ï¸ è¯´æ˜**\n",
    "\n",
    "æˆ‘ä»¬å·²æ›´æ–° Studioï¼Œä½¿å…¶å¯åœ¨æœ¬åœ°è¿è¡Œå¹¶é€šè¿‡æµè§ˆå™¨è®¿é—®ã€‚è¿™æ˜¯å½“å‰æ¨èçš„è¿è¡Œæ–¹å¼ï¼ˆæ›¿ä»£è§†é¢‘ä¸­å±•ç¤ºçš„æ¡Œé¢åº”ç”¨ï¼‰ã€‚æœ¬åœ°å¼€å‘æœåŠ¡å™¨çš„æ–‡æ¡£è§ï¼š[æœ¬åœ°å¼€å‘æœåŠ¡å™¨æ¦‚å¿µ](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) ä¸ [å¦‚ä½•è¿è¡Œæœ¬åœ° Studio](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server)ã€‚\n",
    "\n",
    "åœ¨æœ¬æ¨¡å—çš„ `/studio` ç›®å½•ä¸‹ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨æœ¬åœ°å¼€å‘æœåŠ¡å™¨ï¼š\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "å¯åŠ¨åï¼Œä½ åº”çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š\n",
    "```\n",
    "- ğŸš€ API: http://127.0.0.1:2024\n",
    "- ğŸ¨ Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- ğŸ“š API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "ç„¶ååœ¨æµè§ˆå™¨æ‰“å¼€ Studio ç•Œé¢ï¼š`https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75ebd4-91fe-42c5-8122-c81e72133477",
   "metadata": {
    "id": "fa75ebd4-91fe-42c5-8122-c81e72133477"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b281d8-bd07-4721-922c-347838ceee6b",
   "metadata": {
    "id": "18b281d8-bd07-4721-922c-347838ceee6b"
   },
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c96f353-5dc3-41c8-a3e4-6bf07ca455f8",
   "metadata": {
    "id": "4c96f353-5dc3-41c8-a3e4-6bf07ca455f8"
   },
   "outputs": [],
   "source": [
    "# æœ¬åœ°å¼€å‘æœåŠ¡å™¨çš„ URL\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# æŸ¥è¯¢å½“å‰æœåŠ¡ä¸­å¯ç”¨çš„æ‰€æœ‰å›¾ï¼ˆassistantsï¼‰\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1352fa-68ad-4963-890e-c95d93570917",
   "metadata": {
    "id": "6a1352fa-68ad-4963-890e-c95d93570917",
    "outputId": "55ac40c6-f39d-46d2-cd97-805654854084"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       " 'graph_id': 'agent',\n",
       " 'config': {},\n",
       " 'metadata': {'created_by': 'system'},\n",
       " 'name': 'agent',\n",
       " 'created_at': '2025-03-04T22:57:28.424565+00:00',\n",
       " 'updated_at': '2025-03-04T22:57:28.424565+00:00',\n",
       " 'version': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistants[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c28a0-d712-496c-b191-7d620589ba33",
   "metadata": {
    "id": "ba9c28a0-d712-496c-b191-7d620589ba33"
   },
   "outputs": [],
   "source": [
    "# åˆ›å»ºçº¿ç¨‹ï¼ˆthreadï¼‰ä»¥è·Ÿè¸ªä¸€æ¬¡è¿è¡Œï¼ˆrunï¼‰çš„çŠ¶æ€\n",
    "thread = await client.threads.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e4177-3644-43fa-a2f1-08f73292d1a6",
   "metadata": {
    "id": "2e7e4177-3644-43fa-a2f1-08f73292d1a6"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [`client.runs.stream`](https://langchain-ai.github.io/langgraph/concepts/low_level/#stream-and-astream) æ¥è¿è¡Œæ™ºèƒ½ä½“ï¼Œéœ€è¦æä¾›ï¼š\n",
    "\n",
    "- `thread_id`ï¼šçº¿ç¨‹æ ‡è¯†ï¼Œç”¨äºå…³è”æœ¬æ¬¡è¿è¡Œçš„çŠ¶æ€\n",
    "- `graph_id`ï¼šè¦è¿è¡Œçš„å›¾ï¼ˆå³æ™ºèƒ½ä½“ï¼‰çš„æ ‡è¯†\n",
    "- `input`ï¼šè¾“å…¥æ•°æ®ï¼ˆä¾‹å¦‚æ¶ˆæ¯ï¼‰\n",
    "- `stream_mode`ï¼šæµå¼è¿”å›çš„æ¨¡å¼\n",
    "\n",
    "å…³äºæµå¼å¤„ç†ï¼ˆstreamingï¼‰æˆ‘ä»¬ä¼šåœ¨åç»­æ¨¡å—è¯¦ç»†å±•å¼€ã€‚è¿™é‡Œå…ˆç†è§£ï¼šå½“è®¾ç½® `stream_mode=\"values\"` æ—¶ï¼Œæˆ‘ä»¬ä¼šåœ¨å›¾çš„æ¯ä¸€æ­¥åï¼Œ[æµå¼](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/)è¿”å›â€œå®Œæ•´çŠ¶æ€å€¼â€ã€‚\n",
    "\n",
    "å½“å‰æ­¥çš„çŠ¶æ€å­˜æ”¾åœ¨è¿”å›å— `chunk.data` ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a4480-66b3-48bf-9158-191a7b8c1c18",
   "metadata": {
    "id": "f65a4480-66b3-48bf-9158-191a7b8c1c18",
    "outputId": "33ede59a-7de8-4da4-e40e-46cb2272dcc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': 'cdbd7bd8-c476-4ad4-8ab7-4ad9e3654267', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-06c7243c-426d-4c81-a113-f1335dda5fb2', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 2}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '988cb170-f6e6-43c1-82fd-309f519abe6d', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}\n",
      "{'content': 'The result of multiplying 3 by 2 is 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-7bda0aa0-6895-4250-9625-18419c5dc171', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# æ„é€ è¾“å…¥\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
    "\n",
    "# ä»¥æµå¼æ–¹å¼è¿è¡Œå¹¶æ‰“å°æ¯ä¸€æ­¥æœ€æ–°ä¸€æ¡æ¶ˆæ¯\n",
    "async for chunk in client.runs.stream(\n",
    "        thread['thread_id'],\n",
    "        \"agent\",\n",
    "        input=input,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "    if chunk.data and chunk.event != \"metadata\":\n",
    "        print(chunk.data['messages'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8b850-750c-4054-95e4-1c457a12ec8a",
   "metadata": {
    "id": "dfa8b850-750c-4054-95e4-1c457a12ec8a"
   },
   "source": [
    "## äº‘ç«¯æµ‹è¯•ï¼ˆTesting with Cloudï¼‰\n",
    "\n",
    "å¯ä»¥é€šè¿‡ LangSmith å°† graph éƒ¨ç½²åˆ°äº‘ç«¯ï¼Œå‚è€ƒå®˜æ–¹æŒ‡å—ï¼š[ä» GitHub éƒ¨ç½²åˆ° LangGraph Cloud](https://langchain-ai.github.io/langgraph/cloud/quick_start/#deploy-from-github-with-langgraph-cloud)ã€‚\n",
    "\n",
    "### åœ¨ GitHub åˆ›å»ºæ–°ä»“åº“\n",
    "\n",
    "- è¿›å…¥ä½ çš„ GitHub è´¦æˆ·\n",
    "- ç‚¹å‡»å³ä¸Šè§’â€œ+â€å¹¶é€‰æ‹©â€œNew repositoryâ€\n",
    "- å‘½åä»“åº“ï¼ˆä¾‹å¦‚ï¼š`langchain-academy`ï¼‰\n",
    "\n",
    "### å°†æœ¬åœ°é¡¹ç›®å…³è”åˆ° GitHub è¿œç¨‹ä»“åº“\n",
    "\n",
    "- å›åˆ°ä½ æœ¬åœ°å…‹éš†æœ¬è¯¾ç¨‹ä»£ç çš„ç»ˆç«¯\n",
    "- å°†æ–°å»ºçš„ GitHub ä»“åº“æ·»åŠ ä¸ºè¿œç¨‹ï¼š\n",
    "\n",
    "```\n",
    "git remote add origin https://github.com/your-username/your-repo-name.git\n",
    "```\n",
    "- æ¨é€ä»£ç ï¼š\n",
    "```\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "### åœ¨ LangSmith è¿æ¥ä½ çš„ GitHub ä»“åº“\n",
    "\n",
    "- æ‰“å¼€ [LangSmith](https://smith.langchain.com/)\n",
    "- åœ¨å·¦ä¾§é¢æ¿ç‚¹å‡» `deployments`\n",
    "- ç‚¹å‡» `+ New Deployment`\n",
    "- é€‰æ‹©ä½ åˆšåˆ›å»ºçš„ GitHub ä»“åº“ï¼ˆå¦‚ `langchain-academy`ï¼‰\n",
    "- å°† `LangGraph API config file` æŒ‡å‘å¯¹åº”æ¨¡å—çš„ `studio` ç›®å½•\n",
    "- ä¾‹å¦‚ module-1ï¼š`module-1/studio/langgraph.json`\n",
    "- è®¾ç½®æ‰€éœ€çš„ API å¯†é’¥ï¼ˆå¯ä» `module-1/studio/.env` å¤åˆ¶ï¼‰\n",
    "\n",
    "![Deployment step 2](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4fd61c93d48e5d0f47_deployment2.png)\n",
    "\n",
    "### ä½¿ç”¨ä½ çš„éƒ¨ç½²\n",
    "\n",
    "å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼ä¸äº‘ç«¯éƒ¨ç½²äº¤äº’ï¼š\n",
    "\n",
    "- é€šè¿‡ [SDK](https://langchain-ai.github.io/langgraph/cloud/quick_start/#use-with-the-sdk)\n",
    "- é€šè¿‡ [LangGraph Studio](https://langchain-ai.github.io/langgraph/cloud/quick_start/#interact-with-your-deployment-via-langgraph-studio)\n",
    "\n",
    "![Deployment step 3](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbad4fa159a09a51d601de_deployment3.png)\n",
    "\n",
    "å¦‚æœåœ¨æœ¬ Notebook ä¸­ä½¿ç”¨ SDKï¼Œè¯·ç¡®ä¿å·²è®¾ç½®ç¯å¢ƒå˜é‡ `LANGSMITH_API_KEY`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ed351",
   "metadata": {
    "id": "646ed351"
   },
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    # è‹¥æœªè®¾ç½®ï¼Œåˆ™é€šè¿‡äº¤äº’æ–¹å¼å®‰å…¨è¯»å–å¹¶è®¾ç½®ç¯å¢ƒå˜é‡\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dda16c-c87f-4c03-b910-d647e83400b2",
   "metadata": {
    "id": "97dda16c-c87f-4c03-b910-d647e83400b2"
   },
   "outputs": [],
   "source": [
    "# å°†æ­¤å¤„æ›¿æ¢ä¸ºä½ åœ¨äº‘ç«¯éƒ¨ç½²çš„ graph URL\n",
    "URL = \"https://langchain-academy-8011c561878d50b1883f7ed11b32d720.default.us.langgraph.app\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# æŸ¥è¯¢äº‘ç«¯éƒ¨ç½²ä¸­æ‰˜ç®¡çš„æ‰€æœ‰å›¾ï¼ˆassistantsï¼‰\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa37c0-92fe-4e80-9d5a-80a77b1e3dae",
   "metadata": {
    "id": "aefa37c0-92fe-4e80-9d5a-80a77b1e3dae"
   },
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€ä¸ªå¾…è¿è¡Œçš„æ™ºèƒ½ä½“ï¼ˆassistantï¼‰\n",
    "agent = assistants[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b810376e-f20f-443a-b1ca-d6793f358f82",
   "metadata": {
    "id": "b810376e-f20f-443a-b1ca-d6793f358f82",
    "outputId": "096632c6-52d7-4de3-8939-ec4d2b59f713"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'assistant_id': 'fe096781-5601-53d2-b2f6-0d3403f7e9ca',\n",
       " 'graph_id': 'agent',\n",
       " 'created_at': '2024-08-23T17:58:02.722920+00:00',\n",
       " 'updated_at': '2024-08-23T17:58:02.722920+00:00',\n",
       " 'config': {},\n",
       " 'metadata': {'created_by': 'system'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d65d84-1bcf-4af4-a7c9-55e73d6c1947",
   "metadata": {
    "id": "32d65d84-1bcf-4af4-a7c9-55e73d6c1947",
    "outputId": "462e0cb5-061d-4e5d-a79e-a9c65bfe216f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Multiply 3 by 2.', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '8ea04559-f7d4-4c82-89d9-c60fb0502f21', 'example': False}\n",
      "{'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_EQoolxFaaSVU8HrTnCmffLk7', 'function': {'arguments': '{\"a\":3,\"b\":2}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27'}, 'type': 'ai', 'name': None, 'id': 'run-b0ea5ddd-e9ba-4242-bb8c-80eb52466c76', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 3, 'b': 2}, 'id': 'call_EQoolxFaaSVU8HrTnCmffLk7', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}\n",
      "{'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '1bf558e7-79ef-4f21-bb66-acafbd04677a', 'tool_call_id': 'call_EQoolxFaaSVU8HrTnCmffLk7', 'artifact': None, 'status': 'success'}\n",
      "{'content': '3 multiplied by 2 equals 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_3aa7262c27'}, 'type': 'ai', 'name': None, 'id': 'run-ecc4b6ad-af15-4a85-a76c-de2ed0ed8ed9', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# åˆ›å»ºçº¿ç¨‹ï¼ˆthreadï¼‰ä»¥è·Ÿè¸ªä¸€æ¬¡è¿è¡Œï¼ˆrunï¼‰çš„çŠ¶æ€\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# æ„é€ è¾“å…¥\n",
    "input = {\"messages\": [HumanMessage(content=\"Multiply 3 by 2.\")]}\n",
    "\n",
    "# ä»¥æµå¼æ–¹å¼è¿è¡Œå¹¶æ‰“å°æ¯ä¸€æ­¥æœ€æ–°ä¸€æ¡æ¶ˆæ¯\n",
    "async for chunk in client.runs.stream(\n",
    "        thread['thread_id'],\n",
    "        \"agent\",\n",
    "        input=input,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "    if chunk.data and chunk.event != \"metadata\":\n",
    "        print(chunk.data['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cb34d-c3b8-4446-a7e3-5fe938abf99b",
   "metadata": {
    "id": "445cb34d-c3b8-4446-a7e3-5fe938abf99b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}