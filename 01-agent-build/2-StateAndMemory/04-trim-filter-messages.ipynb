{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1867.88 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 651.70 GB, Free: 1260.66 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
   "metadata": {
    "id": "c52ea2f9-03ff-4647-b782-46867ebed04e"
   },
   "source": [
    "# 过滤与裁剪对话消息（Filtering and Trimming Messages）\n",
    "\n",
    "## 回顾\n",
    "\n",
    "到目前为止，我们已经进一步理解了以下内容：\n",
    "\n",
    "- 如何自定义图状态（Graph State）的数据模式（Schema）\n",
    "- 如何定义自定义的状态归约器器（State Reducers）\n",
    "- 如何在一个项目中使用多种图状态 Schema\n",
    "\n",
    "## 目标\n",
    "\n",
    "接下来，我们将在 LangGraph 中把这些概念用于实际模型调用。\n",
    "\n",
    "在后续章节里，我们会逐步构建一个支持长期记忆的聊天机器人。\n",
    "\n",
    "**因为机器人依赖消息来对话，我们先介绍在图状态中处理消息的几种高级方式：过滤（Filter）与裁剪（Trim）。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda",
   "metadata": {
    "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install --quiet -U langchain_core langgraph langchain_openai\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
    "outputId": "86b1e4e0-4df3-42df-cdd2-529a5ca6a231"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "# 小工具：若环境变量未设置，则在运行时提示手动输入并写入到进程环境变量\n",
    "# 生产环境建议通过更安全的方式（如 .env、密钥管理服务、CI/CD 注入）设置\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# OpenAI API 密钥（用于 ChatOpenAI 调用）\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864",
   "metadata": {
    "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[调用追踪（tracing）](https://docs.smith.langchain.com/concepts/tracing)。\n",
    "\n",
    "本笔记会将追踪数据记录到项目 `langchain-academy` 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd020c79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd020c79",
    "outputId": "ae7dec59-b178-4045-fd29-de21c5547aaf"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 用于链路追踪与可视化分析\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# 指定追踪项目名，便于区分不同实验/环境\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7",
   "metadata": {
    "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7"
   },
   "source": [
    "## 将消息作为状态（Messages as State）\n",
    "\n",
    "首先，我们先定义一些对话消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
    "outputId": "1444c716-76be-4eea-c649-029780eaa50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你之前说你在研究海洋哺乳动物，对吗？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "是的，我知道鲸鱼。但我还应该了解哪些其他的？\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# 定义一组对话消息（先是 AI，再是人类），用于后续示例\n",
    "messages = [AIMessage(f\"你之前说你在研究海洋哺乳动物，对吗？\", name=\"Bot\")]\n",
    "messages.append(HumanMessage(f\"是的，我知道鲸鱼。但我还应该了解哪些其他的？\", name=\"Lance\"))\n",
    "# 以更易读的格式打印消息，便于调试与理解\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c",
   "metadata": {
    "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c"
   },
   "source": [
    "回顾一下：我们可以直接把这组消息传给聊天模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
    "outputId": "23c82818-51f1-414b-ea15-a7965cbde0f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='除了鲸鱼之外，海洋哺乳动物还有海豚、海豹、海狮、海象、儒艮和江猪。\\n\\n1. **海豚**：它们是高度智能的生物，属于齿鲸亚目，许多种类有着复杂的社会结构。\\n\\n2. **海豹**：属于鳍足类，常见于海岸和帆船上。它们通过水中的鳍进行游泳。\\n\\n3. **海狮**：也属于鳍足类，区别在于它们具有外耳壳，并能利用鳍在陆地上行走。\\n\\n4. **海象**：以巨大的獠牙和健壮的体型闻名，它们主要依靠饮食中的软体动物。\\n\\n5. **儒艮**：一种草食性海洋哺乳动物，以海草为食，有时易被误认为是“美人鱼”。\\n\\n6. **江猪**：即淡水豚类，它们生活在淡水区域，并以其独特的生态角色闻名。\\n\\n每种海洋哺乳动物都有独特的行为、生态角色和进化特征。了解这些生物不仅能够对它们的保护提供帮助，亦能丰富对海洋生态系统的整体认识。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 45, 'total_tokens': 335, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_8458c98457', 'id': 'chatcmpl-CLKxqFJmzs6p4f3rmg3wNrFUYDMzR', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--415fc185-35d1-4672-915f-f57c28fd387a-0', usage_metadata={'input_tokens': 45, 'output_tokens': 290, 'total_tokens': 335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化聊天模型；gpt-4o 仅为示例，可根据账号权限与成本选择其他模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# 直接将完整消息历史传入模型（注意：长对话会推高 token 与延迟）\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec",
   "metadata": {
    "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec"
   },
   "source": [
    "我们可以在一个使用 `MessagesState` 的简单图（Graph）里运行聊天模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
    "outputId": "b5fb0b94-b85f-4e4c-c4ea-48ab0afd87e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图可视化：\n",
      "❌ Pyppeteer 渲染失败: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "📝 图结构（Mermaid 文本格式）：\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tchat_model(chat_model)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> chat_model;\n",
      "\tchat_model --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "🔗 图结构信息：\n",
      "节点: ['__start__', 'chat_model', '__end__']\n",
      "边: [Edge(source='__start__', target='chat_model', data=None, conditional=False), Edge(source='chat_model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "💡 手动渲染说明：\n",
      "1. 复制上面的 Mermaid 文本\n",
      "2. 访问 https://mermaid.live/\n",
      "3. 粘贴文本到编辑器中查看图形\n",
      "4. 或者使用支持 Mermaid 的 Markdown 编辑器\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# 定义一个图节点：接收状态中的所有消息，直接调用聊天模型\n",
    "# 注意：此处未做任何过滤/裁剪，长对话会不断增长\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
    "\n",
    "# 构建最简图：START -> chat_model -> END\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# 展示图结构\n",
    "# 图可视化\n",
    "print(\"图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
    "outputId": "988612ad-e9b8-410e-8541-53f1ad26ea06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你之前说你在研究海洋哺乳动物，对吗？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "是的，我知道鲸鱼。但我还应该了解哪些其他的？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "除了鲸鱼之外，还有几个主要的海洋哺乳动物类别值得关注：\n",
      "\n",
      "1. **海豚**：海豚是鲸目中的一部分，与鲸鱼关系密切。它们以智慧和社会行为而闻名。\n",
      "\n",
      "2. **海狮**：海狮属于鳍足动物，通常较为活跃，并且喜欢在海岸线附近活动。\n",
      "\n",
      "3. **海豹**：与海狮类似，海豹也是鳍足动物。它们通常在更冷的水域中被发现，并以其流线型的身体著称。\n",
      "\n",
      "4. **海象**：海象以其巨大的体型和显著的长牙而闻名，主要栖息在北极和亚北极地区。\n",
      "\n",
      "5. **儒艮和海牛**：也被称为美人鱼或海牛，儒艮和海牛是食草动物，常在热带和亚热带水域被发现。\n",
      "\n",
      "这些动物都有独特的适应性和行为模式，使它们在海洋环境中成功生存。如果你对海洋哺乳动物的生态、保护和行为感兴趣，这些都是很好的研究对象。\n"
     ]
    }
   ],
   "source": [
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7",
   "metadata": {
    "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7"
   },
   "source": [
    "## 归约器（Reducer）\n",
    "\n",
    "在处理消息时，一个实际挑战是管理“长对话”。\n",
    "\n",
    "如果不加控制，随着对话历史不断增长，模型每次都会接收越来越长的消息列表，导致令牌（token）消耗增大、延迟升高。\n",
    "\n",
    "为了解决这个问题，我们有几种常用方法。\n",
    "\n",
    "首先，回顾之前的技巧：使用 `RemoveMessage` 配合 `add_messages` 归约器来删除不需要的历史消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
    "outputId": "4cd3968a-cff6-41f8-8d19-7ef79c3f815b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIEAAAFNCAIAAACon5t8AAAQAElEQVR4nOydB2AT1R/H32Wv7g0ttLRAQUbBQhG1ggUqowJ/UURABAQRQRAQEWWryPyrKH/AhYAIQpGhiKKCgyVo2WApLasDCp1pkia5u/8vuTakJalNcqOX3ocaL++9u9y97733fm9LSJJEApwiQQJcI2jAPYIG3CNowD2CBtwjaMA9jGtw64bh/NGSO/kms5Ewm5DZRIpEGEGQmAiRBMLEGIlbjGMMw8BKtjpiCJFiCYabKaPZ8pU6hbqg7ViEYYTlFIyEq1lPt14Hwf+pz6rwmMX17i9Wh7QhEmMEXsNFKhOJJKRCLYporuiSGiQSiRCTYAzVD3KztAe2F5bcxOFYIsPkSvgTIQIDGariwvopEiMCp24EotrqiFuiUCRBhNniTFp9EEQCUX3H1kivcQpRdWwJj1n+2b4i6lxkOb1ag7vy1LiCHRKFiDCbKytJo57ATUiqQKFRisETIxEz0K9B0S19+vu5lTrkEyRu/6Bv555BiOcc2FZw+XSFoYIMjpA+/WpzRDc0a7D9/asFV0zhLeRDJkch76L4tn7P2gJtEd51gH9iz2BEH3Rq8PEb2SQixr8dh7yXS6fKf9p0M7S5/IlJtL1ktGnwxaIc32Dp4BeZyjQbFJ+8mXVfd/8H+tGTGujRYN2sy2Ex8oEvNAoBKD6dk60JlAx9pRnyGBqsrs/nZ4c0a1wCAGMXtSi7bf5xYz7yGE81+P7zXDA3mbPbGjLj3m5xKaOivNSIPMNTDS6f1o+YTUN65CmxHdVfvXsdeYZHGmx8+4pfiESpbrwNHo+NioDy9Pj+O8gDPNKg9LY5bVwYatw0jVVmHChBHuC+Bt99lidXIf8QJWrcDBjXxGQgi+4YkLu4r0HeZX1ECxVil1mzZu3atQu5Tu/evXNzcxEzKDSiwzuKkLu4r4FRTyYk+yJ2OX/+PHKd/Pz84uJixBiB4TJoHkbu4mYdLf+Kfseq3JdWMNUscejQoQ0bNpw7dy44OLhjx46TJ0+Gg8TERMpXo9EcPHhQq9Vu2rTpyJEjly9fBt9HHnnkxRdfVCgUEGDmzJlisTgiIgIu8sILL6xdu5Y6EcKsWLEC0c2xH25n/FwyYambseFmOrieqROLEUNcvHhxypQpXbp02b59O8RmZmbm/PnzkVUY+JwzZw4IAAdbtmxZv379yJEj33vvPQi/f//+devWUVeQSqVZVlauXDlkyBAIAI6QiTEhABAZp8AJ5DZumpUGLQFdH4gZTp48Ca/zmDFjoPMkPDy8bdu2EJv3BhsxYkRKSkpMTAz19dSpU4cPH3755ZeRtUcoLy9v48aNVLJgmsAQpaXTwl3c1IAkCGtHCSMkJCQYDIapU6cmJSUlJydHRUXZciF74GWHjGjevHmQUMxmS49PYGCgzRe0YUcAC9AL5EGzm5t5kUItxgmmBujFx8d/8MEHISEhq1atGjx48MSJE+EdvzcY+ELmAwF27tx54sSJ0aNH2/vK5XLEFmXFBk9eSDc1CI+RE2YGB0l2794d8v09e/ZASVBaWgppgnrTbcB7l56ePnToUNAA8itwKS8vRxyRe8mAsa9B83gfgkB38isRA/z111+Qs8MBJIUBAwZMnz4d4hfsS/swJpNJr9eHhoZSX41G42+//YY44lqmXiJDbuN+/UAsxU786FE7iTMg5wFzaMeOHWDUnz17FuwfEAMMTcheINKPHj0KOQ8U19HR0bt3775x40ZJScnChQuhFCkrK6uoqLj3ghASPsFwgqshBridawiKcF8E9zUIbiK9elGPGAAMHshhli9fDpXb8ePHq9VqyPclEov5AMbS8ePHIWVAInjnnXeg1AXTc9CgQV27dp00aRJ87dWrF1hEtS4YGRmZlpa2Zs0aKEIQAxi0qEuq+31q7vejGfT4J7NzJv3Xm3uP68MvW27+81f5i8vcjwf304FCKVZqRFtXXEONm4snyuMSNMgDPGr6T5sQvm1lXh0BevTo4dAdx3HI0DEnxgTYmv7+/ogBoPYHJpZDLyjVocLh8JZatGjx2WefOTzr9923wDbpPTwceYCnffpbV16t1BHPvhnj0Nc9e9HHxwcxhrNbqqysdFalAGGghcqh14fTsnoODb4vyaM3hoZxFWteuxzfVdPjiUbXmbN+QY7SRzx0mqdduTSMq5iwJPb8kfJLJz3qS+IdW1ZcIQnScwEQjWO8Ppqe9UC/gM4pvB9dWh82Ls5RaaRPTKZnNAmdYx1Xz8gKCpcMnRGNvBrIgqDkHjU3BtEEzWN+P5uXpdeixF4BSX29MEHsWZd39aKueRtF2jg6x1PRP/b90J7CU7+Wgi3RrJWy97OhMjnvR75c+6f8yLfFd/KMcpXoiVea+gfS3CLL1ByQg9tuZmZooc9ZJEJyNfINkik1EplcbLZrbYWeOBy3uxWraW67HdtkDYfu1CSP2l7UHBy7E6kDsQijWtprecG9EYTdNW3hJchowA1aXFtiNugIAkdqX3H3gcGtEhgxmpnSwMbvOwtvZFVAv5vJaJkhY6/B3Uk41K1gGIlIdI8G1li920lSPZnJKoFlOhSBqIi3vw5J2h+IJBjV0l5rIpRtWlUtd4lMJBaTEinmFyxrfp8yITkQMQnjGjANtLCmpqZCpybiLbzPrKFvh2pS5S+CBtwjaMA9vNcAOjWhvRPxGSEdcI+gAfcIGnCPUB5wj5AOuEfQgHsEDbhH0IB7hDKZe4R0wD2CBtzDew1wHBc04BJIBGLm5iayBe814HsiQIIGDQFBA+4RNOAefj+AF1TQkJAOGgL8fgCSJCMiIhDP4bcGUDlgblUi1uB5DVMiqTV/n48IGnCPoAH3CBpwj6AB9wgacA+zO70wDdimBEHwfQoFvzVAXpEUBA24h/+NLYIGnCNowD2CBtwjaMA9ggbcI2jAPV6gAV/n6Xfq1AmzYlsWASrMPXv2XLlyJeIbfK2jJSUlURqIrMBBaGhorSWv+QJfNRg+fHhQUI0lktq0adO+fXvEQ/iqwcMPP9y2bVvbV19f32HDhiF+wuP2olGjRtk2PIiLi4PcCfETHmsAxXK7du3gQK1W8zcRIPbtIqPeeHhvcaWWrLmDvXULB/td1hGqWpOrxkpd1p3d7fZpLysrOXX6jFwu75aUVLUaVNUiX/aQVZe3W6xLhCGCrLmvu2W5L9IvRNqtL537U9cHVjX4amlO8S1cIkEkhuEmu5ugFIB7oVY2q14njYoyZNMAq45P2y1DGIK0WEdijLCqiokQSdiuWSOK7TWggtnW8aKQyhGOkwSO2nbzYXPJXPbqaNvfv15RgY+cE4caNnk5ZT9vvuUTJL2/B7NLqNlgKR1sXpZjriQGT45FPGHz4qxOj/p17ROCmIelMrm4AO//Am0b0LNA05aKU7+VIlZgQ4Mj3xVKpJhM5sGWMazTrnuQyf3dF12DjfLAoKsqMHmET4DSfu1VRmFDA8sapGw9D13goABbr03j3QK84SBo4BhP9pxzFVbyIgxhbD4THdSojjMMGxpADYR3PUUEWdXCwQKs5EUYiYl4lg7YhBUNSIwkeJYORCxmnqyUB2IklvAsHbCZd7JSHuAIN/MsHZCIvRsWbFPuETRwjLfVD0hLw6BgFzmFjXZTay+Za9mrTqd75925/dOSZ742KTs7q2dK4unTGeA+f8FrM16diJiHZPGlYaX/gHC5ynnm7Mn9+/eOfm7C+HEv+/sHPDvy+dDQ2nuzLlg4a+/3uxBDkI2+TNbpLFuy90rpCwLAAYhxb5h//jnfpcsDiP80xLEt3373zcJFr8PB4Cd618qLbIBLfkHesuWL0gb2oFz2/bBn4qTn+vZ/CD63p2+2tY7Mmz8TrrZ23QdwSmlpfTdWFbEYM6z8ksgygqH+wQf0Hzx3zmI4+CZ9/9IlHzoMs2/vIfh8dcacPbsOwsFPP+9bsnRBq5bxmzftfn7sS6DBh6tXUCGlUml2Thb8vb1opVpd3w2/SYxAbMFSeUAw3Faxd+/ODh06TZ0yKyAgsHOnLqNHTdi58+vi4iJkHZJdUJC3YN7S7t2T67/gFJv1ZN7PjQUIgjh77lSXxLtlQ6dOXcDx9Jmq7Kt5sxiFQoEaKt5QRzMajSaT6dPPVsOfvTuVDgCZnOZdRunFGzSAd1ylUvXp3T85ucaOjU0i3N9hl83GdlY0EFnGFjJKbGyrcm15p4RE6iski/z83NBQD8YrEuw12rFWHtD8Wsnl8pCQ0BMnjmacPGE2m8eNnXTo0EGoskExcObMSTBGp82YAHkUchdoN2UtIbBkFzHRhzP8mTF/ZxyfM3e63qBv3z5h3ZovoQ4BVYoZMydWVGjfWrRS3rCLARtsjDc9uLXw3LHSZ+c19NG+9ui1+NZlOZPfY+OeWWk3xRDfhlWwWk9mpS+TRHwbVmGtJ3vVODue9iezdcusaMDD/mQ2EfoyuYetvkzeFcosws7Yd8S7QhnDvMsusgyg5d1YR8tL40Vjfi0Vf76NdSSRl435FQljfuuCFQ0I/o35ZRPBNuUeQQPuYaV+ICWlCr51XIuRiK2NONmImibRctzE3lARWrh2rpy1GgIbv9O6s59IjGX8ehvxhwt/lgWGsbSwAEtad+vne+a3+o5x45xj3+dri41Pz2iGWIG99YtKbuk3LckNaSqPbKP285fVqAFVz0O9t17kdIbq3VMsKxJR54pqBr57rqOrYNaxyBhWfQkIQJjv3DRePa81VODjF7PX68fqGlKFudq96wv1ZYTZxOZcoypqrdp1LyIJkkiQf5j0qanNEYvwdY1ZGzNnzkxNTU1JSUG8hff1Ay/YtlTQgHsEDbhH0IB7BA24R9CAewQNuIf3GphMJqlUiviMkA64R9CAewQNuEfQgHuEMpl7hHTAPYIG3CNowD38vnsQQCwW824R4VrwXgO+JwIkaNAQEDTgHkED7hE04B5BA+7h9wMQBNGqVSvEc/itgUgkyszMRDyH5zVMiQSyI8RzBA24R9CAewQNuEfQgHsEDbiH9xrgON+24rwH3q+1DP0HfE8KvNfAC7Ij/je2CBpwjqAB9wgacI+gAfcIGnCPoAH3eIEGfJ2n37lzZ+qAGuBFPUWHDh3Wr1+P+AZf62gtW7ZE1n40zAocqNXqMWPGIB7CVw2GDRvm4+Nj7xIbG5ucnIx4CF81GDRoUFRUlO2rXC5/5plnED/hcXvR6NGjIf+hjkGPPn36IH7CYw1SUlJiYmKQ1TSCrAnxlnrZpjkXyghTjVUOHS1EXMPNQQCsaq8lzPlC0tYQWN0u9vwndaKpdKtKoWwX0yv7dAWJ6rpDJ4snO3LGHG8LZR/UbpEwZ3tIEYoArGnUv+/Q+S+26ZZlOUU3cTD/cLPTu6kn9TqlPjHn9kLU9T/xX5f8qgcikeUHJVIU20nTa2h4HSHrSgeblmYbK4jeI8LCY3yQgFucPVT09y9FwRF3EpKDnIVxmg7WL8gWy9CgiS2QgMdsXpLVPF7x2LOO94502oBYXAAAD9JJREFUXCafO1JsqCAEAeiiS2pwzlmDM1/HGlz4s0yh8YZtfRsILRP8SQKdP1Hk0NdxeVBpwMT8H1PeoMBEqKzA8YLTjiPabCRIQtg1gk5wM+ZsDwjhZeceQQPucayBZfsanq//29AQiZxujuVYA5KAaoNQHtAJ4XwTaSEv4h7HlQCqewoJsILjdEAQ/NtVroEDrzR0uDr0clYZJpGQDOiFRLgTM6eOvAgJ0AhpbR916OXELqra3ESADZxpINimNINVD4W6F5HTM2jiyaF9P/n0I8QTDhzc3zMlsaSkuO5g8xe8NuPVicgV6shVHGuAcb3v94KFs/Z+vwt5F6RLZTLJ9QjIf/45jxoNtNWTcRzftv3LLzasg+O2bdo/N+qF9u0Tqn5DIt3xzdY1a9+TyWTt2iW8Pmuhn68fuOfkXN69Z/vfGccLCvKim7fo12/QwMeHgDvkBvC5bPmi/635755dB+v4UUgukMk+0O3hZSsWicXi+Nb3zZ+3ZOeubXAbvr5+qX0GTHhhCpULX7t25b333828dEEslkRHt4Db65SQSF1kzdr3f9z/nUqpSkl5LDKyxq5c+37Ys3tPek5OVkxM3KM9+zzxn2Fu24sY5mJ5gLleTV738apdu7YtXLD8zdlvh4SEvfb6ZHhsyuvX336qqNAueXfVqzPmnj178vPP/0e5f7R6xfHjR6a8/Nq7iz8AAd7/YMnRY4fAfd9ey+erM+bULQCyjiw6e+4U/G3b+v2a1RvhYMor4wgC/3b3r/Pmvvv1tk3HrBcsLi6aNHl0aGj4urWbP1r1eYB/4KK3Zut0OvDatXv7rt3b4B5Wr94QEdF0w8aPbRf/6ed9S5YuaNUyfvOm3c+PfWl7+uYPV69ADECPXVRaVgoPPHXKrC6J3eBrUtKDOl3FnaLbzZpFw1eVSj1yxFgq5KHDv54+k0Edz5mzGIJFhDeBY3gr9+3b/efxw92SHkSuYDQaJ700QyqV+vn5t4iJM+Pm0c9NoC7o7x9wOftSt24PQQKVyeUzpr9JLTgFr8KQp1Ih6oc9PWrHN1seSe71SLJle7XHUtMuXDh748Y16sp79+7s0KETPBQcBwQEjh41YenyhSOeGQPHyHUgcyddqh9AHc0lDa7kXIbP+Pj7qi4qkSxcsMzm275dgu3Yz9ffWFlpu68dO7Yc+/PQ9etXKQd4E5GLNG0aZVvvWqlSBQUG27zUKrVWWw4H2TlZLVvG21b8UqvVUZHNMzMvQKTk5l7v+9jjtlNatWpDHRAEAanq2ZHjbF6dOnUBR3iBKMFcxbKhpEtt19bcC9Uf6lEVcoXj37DrmrblcfA8s2ZPMZmM456flJCQ6KPxmTxlLHKdWo0wDttkiu7cBqnsXRRKpU6vq6iogGJMqVTddVcoqQNIXiaT6dPPVsOf/YmQrSG3sDT+uNR2TeCu9Ser1ZYRfZCx1P+UzEsXL148t3zZ6vs7d6VcQMiQ4FDEACq12lBZY2iJXqeLbNoMEgSU5JV2Xnq9jjpQKBQqlapP7/7JNd/6JhGRyC0sbXYulsnIpbaKuLjW8LKfOv039RXSOLzjP/zwbR2nlJZatvK1RfqVK9nwh5ihdau2kNHDe019LSsvu3otJyYmFhJlWFjEuXOnbSGPHvvDdhwb26pcWw7lCvXX7r6OkNGFhoYhdyGQa/UD5FJdWaPR9O7VD+yi7/ftzjh5YtWHy/7661ibNu3qOAWMUZBt69cbIUbAgoJToDwvuJmPrJMJQkJCT5w4CpeiZa5ZWtoTYJitWPn2zZsFoPTid+dCttmv7yDw6tmj92+//wLVYzj+assX58+fsZ01buykQ4cOQlURss0zZ04uXPT6tBkTII9CbmGJUidvtZN04HofDph3kK3Dc06bPsFyx/OXUUaRM8LCwt+Y/db5C2cGDnp09puvgPH3+OND4G0dNdpSRRj+zBioN8yZO11v0COPiWwaBaYqmPlPPzNg6rTx4PL+e59QcxdGDB/bv98geAOgUnLk6O8TX5yGqiu0UL9Zt+bL06czBj/Re8bMiaDiW4tWwvuB6MbxeNMNi67iBBrC7jbC3s0XCy537unXPS34Xi8neRESWk1pxjrzwhXb1DL+HmsQ/Qdpj/dw5vXaa/MferAH4g8i5IptapltihoE69ZtduYFTQ6IP0D0uzbWEQoJx8NTWYdqyfACLNmQa2O8LP8JJQKdWKLUpXoy1OmE3mTWcFYeCGNbaMbl/mQCUo2QEOgFszadOsKpXSSkA3qBqjDhUlsFyXmHcmPCaXuRkA5Yw9n8A2HML3s4G1dBCgNOWcOxBjIpZhbmZdKKWEJiYsdejssDuQYjzLxfT71hQaLAJo732nasQcdkH125oAFtnDt2B2yc1gl+Dn0daxDbIUATIEl/n6kO3sZGxs/FbbuqnfnWtX7RNx/duJNn6NgjKL5rABJwHRzHj/9QmPmXtu9zYS3aOV1/6F/WkPpm9fWbV424mSSct2Vjznvd4NqY47WznK3OVd91nhz8qMNTazree1atRcWwezvea16h9hPZ+da6uLVHnpQpRe2S1Q+khtX5LPWoEOuL9Vq940LdOggGu7fTDbO7vZoPWXWr1pEbtWeiY9ZLVZ1b7YlVK0ZWn25//dX/+6hLIvzraos++x+tfam7B9UPTlY3p5HIdmO1fqvKwXoyVn1HlCtmGbFShXVeve0Uyw+FNpWhelCvcdfKAKWyoeZGZfpcpV+nkCb1etqGCe/niHvB1rGCBtwjaMA9ggbcw3sNTCaToAHHCOmAewQNuEfQgHugPLDNR+MpQjrgHkED7hE04B6hPOAeIR1wj6AB9/D77glr956z9RL5Ar818ILCAPFdAy/IiJCgQUNA0IB7hPKAe4R0wD28t03j4+MRz+G3BlAzuHjxIuI5PK9hSiS0LHDELYIG3CNowD2CBtwjaMA9ggbcw3sNcJz38+Z4v0GvWCzme1LgvQZekB3xv7FF0IBzBA24R9CAewQNuEfQgHsEDbjHCzTAeLpwXZ8+fajaWXFxsVwuhw41o9EYHR2dnp6O+AZf04FGo7l2rWoDp0rrJkegxJgxYxAP4Ws9edCgQZAO7F0iIyP79++PeAhfNXj66aebNr27kReUCk8++STiJ3zVQCaTPfXUU7adUUCPgQMHIn7C4zY7SAqQ/yBr02laWhqogvgJv9tNhw0bplKpmjVrNnjwYMRb2LBNM34u+idDqy0xmSqr1gOzXxXsX1fkstxirdXA6rFkl9OLI+vWDk52msEwy8KuIhFSqEUBYbLEFP/IVhrEMMxq8PV/rxXesGwoJpGLFT4yVYBCrpGK5FKx/cpstWMEq7kIl6Mos3fBqKXZMIy4d78A62pdWO0l0khMhDnZ54TALWNYDeVGfZHeoDMRRkIsxeISVL2GRSDGYEqDPetyr17US2Ti4BY+wVE8XpIw90JhaYEWROz+eHDHh/wRAzCiwdrXswmcjE5sovTh8RJn9hRculN0tTwgXDLs1eaIbujX4KPpWb5hqqj27u8r2WDJOnwdkcTzb7VAtEKzBh9Oy4ruEq7xVyIv5dKR6wo5NvINOlMDnbbph6+AAGFeLADQ8oEoo5n8+I0sRB+0abBm5mX/JhqNvwp5O7Fdo3Ac27LiKqIJejRIX3Udk4gi24WgxkH8I9G3c01ZZ8oRHdCjQX5OZeuHm6HGhE+Y+pevbiE6oEGDL5dclat4PzHPVZp3CDUbyT/330YeQ4MGpYWmiLZBqKGybNWw9D1LEQMo/BSnfy1DHuOpBr9uv4lEmCbAm20hZzTvGGqooGFvV081yDmvkyl4PzDAPcRSMSZCB9MLkGd4Gn0VpXhAlBoxA46bv/9pzYXMQyUlBTHNO3ZPerJt6wcpr3mLU1NTxlfoSn785RO5TNm6ZbeBfaf5+lp2Si+4lb0lfeHNwpy4Fvf3eoTZHmaRBMu7rEee4Wk6gPZHvzCm6gTffLv89yNfPZT05OzpO9vf9+iGLbNOn/2F8hKLpQf/2ATNzAtf/3Hmy1/nXD31w4GPkWXWuOmTDVP9/UJnvry1f59JEKa8nIZi0xlytay8yNPsyCMNDHoTfKqZqRhDb8OJk989+vCoB7r+R63yS7r/8U4dUvcf/NQWIDgwstcjo5VKH3j9W8d1u5Frmah85vyBktKbj/d9JcA/PDy0xeABM/QGeqx4h8iUUtzsaWOPRxroihmcA3M974LZbGwVl2RziY3unH8zq0JXSn2NbNrG5qVU+hoqtXBw+851mVQRGFDV3O/rE+zvx2DroVgmrufmMXXgUXkA3QMYY3v8GvSWOP3ok/G13Mu1dyBZWA8dPLxOXyaT18gbpRIFYgyCIEVi5CEeaeAbLIW+K4PeqFDS309AFbBDBr4eHBhl7x7gF17HWSqlb2Wlzt7FUFmBGMNkMIklnKYDZDEMkPaWTtGcfg1CgppJpZahK2DeUC7l2iJoaZfL6zIBAvwjoC8SsqyIsDj4mpufWVZeiBjDpDMp1Z7aNZ6eL1Ng5bcYedEgrvv0HLf/wKfZV0+azEawiNatn7zj23+p8d7XJlkikW3budhoNJSWFW76+k2Vyg8xBm7Eg5rKkWd4mg7CouS52UbEDD0fHtkkotWB3zdcunxcodBER7V/cuDsuk9RKjRjR6z87scP33z7USicwTz9+/QPzO2+CkZRYi9PO5k97Uer0Bo/n3utXe8Y1PjIPXdbe0f7wuJY5Bme5kVqjQwyxJy/8lHjo6xQG9WShroRDU093QYE/bqtrnJvzecv3chzsNITQeCQCsVix/cwa2q6Rk3bWJJffvvil983OPF0sEMjxfSXvoS6nkOv4rwyEif7jWmCPIaePv3P5+cgsSQm0fENlZXfhtqWQy+jqVImdVymBQbQ8Hg29PpyZxXmCl2ZWuXr0MvPN9TZK3LhwJXYjqo+w2kY+0XbuAro0I/t3kSp8dRI4AU5J/PxCiNdg1xo69PvnhaQfTQPNQJKCyt0hQYaRxnRpkHnR4PaJPmc/TEHeTV6reF6xq2XVsYh+qB5jFdmRvn+L2/G92hWa6KSd5D3T2HRNe0kWgVATIx1PLz39t/7SzRBiuj7GRyrzD5Zh66bTeYJS2gWADE37nrdG5eNBtIvXBXVjvcDT7OP5+pKjEERsmGvMjJ+h8H5B3/sunX6jzLCjCRKkW+wKijaT67kzTDssqKKkutaXUml2Yir/cSPPRsSEcPUZBDG5+GcP1Zy/KcSfbnZXImgBxzqQyIMEbiTJhyHtSVnjsi2w7rzkHcnlNjtCO8caid5akqPTI4FNpH1GR7qE8Cswc3qPP1Lp0qLb5krKwjSaf/b3UiioreWo8OQzqkWygl2P1GFWIpUGnFYc0WTWPYGzvJ1rQRvopEODWpQCBpwj6AB9wgacI+gAfcIGnDP/wEAAP//ZpSGYQAAAAZJREFUAwCxkCa7cdITQQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# 过滤节点：仅保留最近 2 条消息，删除更早的历史\n",
    "# 使用 RemoveMessage + add_messages 归约器的组合语义\n",
    "\n",
    "def filter_messages(state: MessagesState):\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "# 调用节点：将过滤后的消息传给模型\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 构建图：START -> filter -> chat_model -> END\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"filter\", filter_messages)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"filter\")\n",
    "builder.add_edge(\"filter\", \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# 可视化\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
    "outputId": "a6b1c515-165c-4128-d1d5-5d0c4c79953b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你之前说你在研究海洋哺乳动物，对吗？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "是的，我知道鲸鱼。但我还应该了解哪些其他的？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "除了鲸鱼之外，海洋哺乳动物还有很多种类。以下是一些主要的类别和例子：\n",
      "\n",
      "1. **海豚**：海豚是非常聪明的海洋哺乳动物，属于齿鲸类。常见的有瓶鼻海豚和普通海豚。\n",
      "\n",
      "2. **鼠海豚**：鼠海豚类似于海豚，但通常体型较小，且身体更为圆润。常见的种类包括港湾鼠海豚。\n",
      "\n",
      "3. **海豹**：海豹是鳍脚类动物之一，适应水陆两栖生活。常见的有斑海豹、灰海豹和象海豹。\n",
      "\n",
      "4. **海狮和海狗**：虽然名字中有“狮”和“狗”，但它们都是鳍脚类动物。海狮的特点是有外耳廓而海豹没有，常见的有加州海狮和南美海狮。\n",
      "\n",
      "5. **海象**：海象是大型鳍脚类动物，拥有长长的獠牙和叫作胡须的触须，是北极地区的代表性物种。\n",
      "\n",
      "6. **儒艮和美人鱼（海牛）**：这些是温暖水域的食草动物。儒艮和美人鱼在生态上非常重要，帮助保持海草床的健康。\n",
      "\n",
      "7. **极地熊（北极熊）**：虽然北极熊主要在陆地或冰面上活动，它们被认为是海洋哺乳动物，因为大部分生活依赖于海洋环境。\n",
      "\n",
      "这些动物各有独特的适应性和生态角色，是海洋生态系统的重要组成部分。了解每一种类的生活习性和保护措施会对研究海洋哺乳动物非常有帮助。\n"
     ]
    }
   ],
   "source": [
    "# Message list with a preamble\n",
    "messages = [AIMessage(\"你好。\", name=\"Bot\", id=\"1\")]\n",
    "messages.append(HumanMessage(\"你好。\", name=\"Lance\", id=\"2\"))\n",
    "messages.append(AIMessage(\"你之前说你在研究海洋哺乳动物，对吗？\", name=\"Bot\", id=\"3\"))\n",
    "messages.append(HumanMessage(\"是的，我知道鲸鱼。但我还应该了解哪些其他的？\", name=\"Lance\", id=\"4\"))\n",
    "\n",
    "# Invoke\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d",
   "metadata": {
    "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d"
   },
   "source": [
    "## 过滤消息（Filtering Messages）\n",
    "\n",
    "如果你不需要或不想修改图状态本身，可以只对“传入聊天模型的消息列表”进行过滤。\n",
    "\n",
    "例如，仅把最后一条消息传给模型：`llm.invoke(messages[-1:])`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
    "outputId": "f05f73bc-05e6-47a5-b65e-0a06071aa7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图可视化：\n",
      "❌ Pyppeteer 渲染失败: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "📝 图结构（Mermaid 文本格式）：\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tchat_model(chat_model)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> chat_model;\n",
      "\tchat_model --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "🔗 图结构信息：\n",
      "节点: ['__start__', 'chat_model', '__end__']\n",
      "边: [Edge(source='__start__', target='chat_model', data=None, conditional=False), Edge(source='chat_model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "💡 手动渲染说明：\n",
      "1. 复制上面的 Mermaid 文本\n",
      "2. 访问 https://mermaid.live/\n",
      "3. 粘贴文本到编辑器中查看图形\n",
      "4. 或者使用支持 Mermaid 的 Markdown 编辑器\n"
     ]
    }
   ],
   "source": [
    "# 仅传入“最后一条消息”给模型，避免把整段历史都送入模型\n",
    "# 注意：这种方式适用于上下文依赖较弱的问答；若强依赖上下文，请考虑裁剪（trim）而非简单过滤\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n",
    "\n",
    "# 构建图\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# 展示图结构\n",
    "# 图可视化\n",
    "print(\"图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5",
   "metadata": {
    "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5"
   },
   "source": [
    "接下来，我们把现有消息列表继续扩展：先加入上面的 LLM 回复，再追加一个追问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16956015-1dbe-4108-89b5-4209b68b51ca",
   "metadata": {
    "id": "16956015-1dbe-4108-89b5-4209b68b51ca"
   },
   "outputs": [],
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"再告诉我一些关于独角鲸的事吧！\", name=\"Lance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85563415-c085-46a8-a4ac-155df798c54e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85563415-c085-46a8-a4ac-155df798c54e",
    "outputId": "cc792e3f-a61c-4047-8954-74b6620fd095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你好。\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "你好。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你之前说你在研究海洋哺乳动物，对吗？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "是的，我知道鲸鱼。但我还应该了解哪些其他的？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "除了鲸鱼之外，海洋哺乳动物还有很多种类。以下是一些主要的类别和例子：\n",
      "\n",
      "1. **海豚**：海豚是非常聪明的海洋哺乳动物，属于齿鲸类。常见的有瓶鼻海豚和普通海豚。\n",
      "\n",
      "2. **鼠海豚**：鼠海豚类似于海豚，但通常体型较小，且身体更为圆润。常见的种类包括港湾鼠海豚。\n",
      "\n",
      "3. **海豹**：海豹是鳍脚类动物之一，适应水陆两栖生活。常见的有斑海豹、灰海豹和象海豹。\n",
      "\n",
      "4. **海狮和海狗**：虽然名字中有“狮”和“狗”，但它们都是鳍脚类动物。海狮的特点是有外耳廓而海豹没有，常见的有加州海狮和南美海狮。\n",
      "\n",
      "5. **海象**：海象是大型鳍脚类动物，拥有长长的獠牙和叫作胡须的触须，是北极地区的代表性物种。\n",
      "\n",
      "6. **儒艮和美人鱼（海牛）**：这些是温暖水域的食草动物。儒艮和美人鱼在生态上非常重要，帮助保持海草床的健康。\n",
      "\n",
      "7. **极地熊（北极熊）**：虽然北极熊主要在陆地或冰面上活动，它们被认为是海洋哺乳动物，因为大部分生活依赖于海洋环境。\n",
      "\n",
      "这些动物各有独特的适应性和生态角色，是海洋生态系统的重要组成部分。了解每一种类的生活习性和保护措施会对研究海洋哺乳动物非常有帮助。\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "再告诉我一些关于独角鲸的事吧！\n"
     ]
    }
   ],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23349705-a059-47b5-9760-d8f64e687393",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23349705-a059-47b5-9760-d8f64e687393",
    "outputId": "a54ab476-d742-47c8-86ca-3d5c35deaaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你好。\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "你好。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "你之前说你在研究海洋哺乳动物，对吗？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "是的，我知道鲸鱼。但我还应该了解哪些其他的？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "除了鲸鱼之外，海洋哺乳动物还有很多种类。以下是一些主要的类别和例子：\n",
      "\n",
      "1. **海豚**：海豚是非常聪明的海洋哺乳动物，属于齿鲸类。常见的有瓶鼻海豚和普通海豚。\n",
      "\n",
      "2. **鼠海豚**：鼠海豚类似于海豚，但通常体型较小，且身体更为圆润。常见的种类包括港湾鼠海豚。\n",
      "\n",
      "3. **海豹**：海豹是鳍脚类动物之一，适应水陆两栖生活。常见的有斑海豹、灰海豹和象海豹。\n",
      "\n",
      "4. **海狮和海狗**：虽然名字中有“狮”和“狗”，但它们都是鳍脚类动物。海狮的特点是有外耳廓而海豹没有，常见的有加州海狮和南美海狮。\n",
      "\n",
      "5. **海象**：海象是大型鳍脚类动物，拥有长长的獠牙和叫作胡须的触须，是北极地区的代表性物种。\n",
      "\n",
      "6. **儒艮和美人鱼（海牛）**：这些是温暖水域的食草动物。儒艮和美人鱼在生态上非常重要，帮助保持海草床的健康。\n",
      "\n",
      "7. **极地熊（北极熊）**：虽然北极熊主要在陆地或冰面上活动，它们被认为是海洋哺乳动物，因为大部分生活依赖于海洋环境。\n",
      "\n",
      "这些动物各有独特的适应性和生态角色，是海洋生态系统的重要组成部分。了解每一种类的生活习性和保护措施会对研究海洋哺乳动物非常有帮助。\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "再告诉我一些关于独角鲸的事吧！\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "当然可以！独角鲸是一种海洋哺乳动物，生活在北极附近的寒冷海域。它们以其独特的螺旋形长牙而闻名，这根长牙其实是雄性独角鲸左侧牙齿的延伸，可以长达3米。虽然长牙的确切功能尚未完全理解，但科学家认为它可能用于争夺配偶、打破冰层或是作为一种感觉器官。\n",
      "\n",
      "独角鲸通常体长在4到5米之间，体重可达1600公斤。它们喜欢成群结队，通常10到20只独角鲸会一起活动。除了长牙，它们还有着圆润的身体和灰色斑点的皮肤。\n",
      "\n",
      "独角鲸主要以鱼类、甲壳纲动物和软体动物为食。它们能够潜水到很深的水域，以寻找食物。由于栖息地的寒冷，独角鲸有着厚重的脂肪层以保温。\n",
      "\n",
      "这些独特的生物对气候变化非常敏感，因为冰层的变化直接影响到它们的生存环境。随着北极冰层的融化，科学家们对独角鲸的未来表示担忧。\n",
      "\n",
      "希望这些信息对你有帮助！如果还有其他问题，请随时问我。\n"
     ]
    }
   ],
   "source": [
    "# Invoke, using message filtering\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745",
   "metadata": {
    "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745"
   },
   "source": [
    "图状态中仍然保存着全部消息。\n",
    "\n",
    "不过，从 LangSmith 的追踪可以看到：模型实际调用时只使用了最后一条消息。\n",
    "\n",
    "https://smith.langchain.com/public/75aca3ce-ef19-4b92-94be-0178c7a660d9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c",
   "metadata": {
    "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c"
   },
   "source": [
    "## 裁剪消息（Trim Messages）\n",
    "\n",
    "另一种方式是按令牌数对历史进行“裁剪”（trim）：即通过 [`trim_messages`](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens) 限制用于本次回复的历史消息总令牌数。\n",
    "\n",
    "- 过滤（filter）：只是事后挑选一部分消息传给模型；\n",
    "- 裁剪（trim）：从头/尾等策略出发，严格限制可用的历史上下文所占令牌数。\n",
    "\n",
    "下面演示如何使用 `trim_messages`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
    "outputId": "01d5e043-6313-4da4-e557-db2851691a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图可视化：\n",
      "❌ Pyppeteer 渲染失败: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "📝 图结构（Mermaid 文本格式）：\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tchat_model(chat_model)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> chat_model;\n",
      "\tchat_model --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "🔗 图结构信息：\n",
      "节点: ['__start__', 'chat_model', '__end__']\n",
      "边: [Edge(source='__start__', target='chat_model', data=None, conditional=False), Edge(source='chat_model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "💡 手动渲染说明：\n",
      "1. 复制上面的 Mermaid 文本\n",
      "2. 访问 https://mermaid.live/\n",
      "3. 粘贴文本到编辑器中查看图形\n",
      "4. 或者使用支持 Mermaid 的 Markdown 编辑器\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# 节点：对“历史消息”按令牌数进行裁剪，再调用模型\n",
    "# - max_tokens：历史消息允许占用的最大 token 数\n",
    "# - strategy：裁剪策略（\"last\" 表示优先保留靠后的消息）\n",
    "# - token_counter：用于估算 token 数的模型或计数器\n",
    "# - allow_partial：为 False 时，不会截断单条消息的一部分（保证每条消息的完整性）\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False,\n",
    "        )\n",
    "    return {\"messages\": [llm.invoke(messages)]}\n",
    "\n",
    "# 构建图\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# 展示图结构\n",
    "# 图可视化\n",
    "print(\"图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24df63ac-da29-4874-b3df-7e390e97cc8a",
   "metadata": {
    "id": "24df63ac-da29-4874-b3df-7e390e97cc8a"
   },
   "outputs": [],
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"Tell me where Orcas live!\", name=\"Lance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
    "outputId": "82e30268-08c3-4ad0-9812-15a56502a810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me where Orcas live!', additional_kwargs={}, response_metadata={}, name='Lance')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of trimming messages\n",
    "trim_messages(\n",
    "            messages,\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
    "outputId": "a3aa43b7-7b84-44b0-c27f-971c6e1df9ac"
   },
   "outputs": [],
   "source": [
    "# Invoke, using message trimming in the chat_model_node\n",
    "messages_out_trim = graph.invoke({'messages': messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3db67-380e-46b5-9a6a-20100ba52008",
   "metadata": {
    "id": "38b3db67-380e-46b5-9a6a-20100ba52008"
   },
   "source": [
    "最后，我们再通过 LangSmith 追踪看看本次模型调用的实际入参情况：\n",
    "\n",
    "![image-20250930110917504](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301109900.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
