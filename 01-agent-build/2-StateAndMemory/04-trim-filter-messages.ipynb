{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "### ğŸ”§ ç¯å¢ƒé…ç½®å’Œæ£€æŸ¥\n",
    "\n",
    "#### æ¦‚è¿°\n",
    "\n",
    "æœ¬æ•™ç¨‹éœ€è¦ç‰¹å®šçš„ç¯å¢ƒé…ç½®ä»¥ç¡®ä¿æœ€ä½³å­¦ä¹ ä½“éªŒã€‚ä»¥ä¸‹é…ç½®å°†å¸®åŠ©æ‚¨ï¼š\n",
    "\n",
    "- ä½¿ç”¨ç»Ÿä¸€çš„condaç¯å¢ƒï¼šæ¿€æ´»ç»Ÿä¸€çš„å­¦ä¹ ç¯å¢ƒ\n",
    "- é€šè¿‡å›½å†…é•œåƒæºå¿«é€Ÿå®‰è£…ä¾èµ–ï¼šé…ç½®pipä½¿ç”¨æ¸…åé•œåƒæº\n",
    "- åŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼šè®¾ç½®HuggingFaceé•œåƒä»£ç†\n",
    "- æ£€æŸ¥ç³»ç»Ÿé…ç½®ï¼šæ£€æŸ¥ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®\n",
    "\n",
    "#### é…ç½®\n",
    "\n",
    "- **æ‰€éœ€ç¯å¢ƒåŠå…¶ä¾èµ–å·²ç»éƒ¨ç½²å¥½**\n",
    "- åœ¨`Notebook`å³ä¸Šè§’é€‰æ‹©`jupyterå†…æ ¸`ä¸º`python(flyai_agent_in_action)`ï¼Œå³å¯æ‰§è¡Œä¸‹æ–¹ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda ç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š (ä»…é’ˆå¯¹å½“å‰ Bash å­è¿›ç¨‹) ==\n",
      "=========================================\n",
      "âœ… å½“å‰å•å…ƒæ ¼å·²æˆåŠŸæ¿€æ´»åˆ° flyai_agent_in_action ç¯å¢ƒã€‚\n",
      "âœ… æ­£åœ¨ä½¿ç”¨çš„ç¯å¢ƒè·¯å¾„: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "ğŸ’¡ æç¤º: åç»­çš„ Python å•å…ƒæ ¼å°†ä½¿ç”¨ Notebook å½“å‰é€‰æ‹©çš„ Jupyter å†…æ ¸ã€‚\n",
      "   å¦‚æœéœ€è¦åç»­å•å…ƒæ ¼ä¹Ÿä½¿ç”¨æ­¤ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:\n",
      "   1. æ£€æŸ¥ Notebook å³ä¸Šè§’æ˜¯å¦å·²é€‰æ‹© 'python(flyai_agent_in_action)'ã€‚\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. æ¿€æ´» conda ç¯å¢ƒ (ä»…å¯¹å½“å‰å•å…ƒæ ¼æœ‰æ•ˆ)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda ç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š (ä»…é’ˆå¯¹å½“å‰ Bash å­è¿›ç¨‹) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. æ£€æŸ¥å½“å‰æ¿€æ´»çš„ç¯å¢ƒ\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"âœ… å½“å‰å•å…ƒæ ¼å·²æˆåŠŸæ¿€æ´»åˆ° flyai_agent_in_action ç¯å¢ƒã€‚\"\n",
    "    echo \"âœ… æ­£åœ¨ä½¿ç”¨çš„ç¯å¢ƒè·¯å¾„: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"ğŸ’¡ æç¤º: åç»­çš„ Python å•å…ƒæ ¼å°†ä½¿ç”¨ Notebook å½“å‰é€‰æ‹©çš„ Jupyter å†…æ ¸ã€‚\"\n",
    "    echo \"   å¦‚æœéœ€è¦åç»­å•å…ƒæ ¼ä¹Ÿä½¿ç”¨æ­¤ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:\"\n",
    "    echo \"   1. æ£€æŸ¥ Notebook å³ä¸Šè§’æ˜¯å¦å·²é€‰æ‹© 'python(flyai_agent_in_action)'ã€‚\"\n",
    "else\n",
    "    echo \"âŒ æ¿€æ´»å¤±è´¥æˆ–ç¯å¢ƒåç§°ä¸åŒ¹é…ã€‚å½“å‰ç¯å¢ƒ: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸ ä¸¥é‡æç¤º: å»ºè®®å°† Notebook çš„ Jupyter **å†…æ ¸ (Kernel)** åˆ‡æ¢ä¸º 'python(flyai_agent_in_action)'ã€‚\"\n",
    "    echo \"   (é€šå¸¸ä½äº Notebook å³ä¸Šè§’æˆ– 'å†…æ ¸' èœå•ä¸­)\"\n",
    "    echo \"\"\n",
    "    echo \"ğŸ“š å¤‡ç”¨æ–¹æ³• (ä¸æ¨è): å¦‚æœæ— æ³•åˆ‡æ¢å†…æ ¸ï¼Œåˆ™å¿…é¡»åœ¨**æ¯ä¸ª**ä»£ç å•å…ƒæ ¼çš„å¤´éƒ¨é‡å¤ä»¥ä¸‹å‘½ä»¤:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# å¿…é¡»åœ¨æ¯ä¸ªå•å…ƒæ ¼éƒ½æ‰§è¡Œ\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. è®¾ç½®pip ä¸ºæ¸…åæº\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. è®¾ç½®HuggingFaceä»£ç†\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# éªŒè¯ï¼šä½¿ç”¨shellå‘½ä»¤æ£€æŸ¥\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### ç¯å¢ƒä¿¡æ¯\n",
      "| é¡¹ç›®         | ä¿¡æ¯                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| æ“ä½œç³»ç»Ÿ     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU ä¿¡æ¯     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| å†…å­˜ä¿¡æ¯     | 2015.36 GB (Available: 1867.88 GB)                                    |\n",
      "| GPU ä¿¡æ¯     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA ä¿¡æ¯    | 12.6                                                                  |\n",
      "| Python ç‰ˆæœ¬  | 3.12.11                                                               |\n",
      "| Conda ç‰ˆæœ¬   | conda 25.7.0                                                          |\n",
      "| ç‰©ç†ç£ç›˜ç©ºé—´ | Total: 2014.78 GB, Used: 651.70 GB, Free: 1260.66 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
    "#\n",
    "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
    "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
    "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
    "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
    "#\n",
    "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©æ‚¨ï¼š\n",
    "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
    "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
    "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "\n",
    "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
    "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
    "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
    "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
    "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
    "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
    "        try:\n",
    "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
    "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "\n",
    "\n",
    "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    if platform.system() == \"Windows\":\n",
    "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
    "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
    "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
    "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
    "\n",
    "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
    "        else:\n",
    "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "\n",
    "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
    "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
    "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_python_version():\n",
    "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
    "\n",
    "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
    "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
    "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
    "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
    "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
    "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
    "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
    "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
    "                     # å°è¯•è·å– codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
    "                     except:\n",
    "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
    "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
    "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
    "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
    "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
    "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "\n",
    "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "env_data = {\n",
    "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
    "        \"æ“ä½œç³»ç»Ÿ\",\n",
    "        \"CPU ä¿¡æ¯\",\n",
    "        \"å†…å­˜ä¿¡æ¯\",\n",
    "        \"GPU ä¿¡æ¯\",\n",
    "        \"CUDA ä¿¡æ¯\",\n",
    "        \"Python ç‰ˆæœ¬\",\n",
    "        \"Conda ç‰ˆæœ¬\",\n",
    "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
    "    ],\n",
    "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# æ‰“å°è¡¨æ ¼\n",
    "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
    "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ea2f9-03ff-4647-b782-46867ebed04e",
   "metadata": {
    "id": "c52ea2f9-03ff-4647-b782-46867ebed04e"
   },
   "source": [
    "# è¿‡æ»¤ä¸è£å‰ªå¯¹è¯æ¶ˆæ¯ï¼ˆFiltering and Trimming Messagesï¼‰\n",
    "\n",
    "## å›é¡¾\n",
    "\n",
    "åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»è¿›ä¸€æ­¥ç†è§£äº†ä»¥ä¸‹å†…å®¹ï¼š\n",
    "\n",
    "- å¦‚ä½•è‡ªå®šä¹‰å›¾çŠ¶æ€ï¼ˆGraph Stateï¼‰çš„æ•°æ®æ¨¡å¼ï¼ˆSchemaï¼‰\n",
    "- å¦‚ä½•å®šä¹‰è‡ªå®šä¹‰çš„çŠ¶æ€å½’çº¦å™¨å™¨ï¼ˆState Reducersï¼‰\n",
    "- å¦‚ä½•åœ¨ä¸€ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨å¤šç§å›¾çŠ¶æ€ Schema\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åœ¨ LangGraph ä¸­æŠŠè¿™äº›æ¦‚å¿µç”¨äºå®é™…æ¨¡å‹è°ƒç”¨ã€‚\n",
    "\n",
    "åœ¨åç»­ç« èŠ‚é‡Œï¼Œæˆ‘ä»¬ä¼šé€æ­¥æ„å»ºä¸€ä¸ªæ”¯æŒé•¿æœŸè®°å¿†çš„èŠå¤©æœºå™¨äººã€‚\n",
    "\n",
    "**å› ä¸ºæœºå™¨äººä¾èµ–æ¶ˆæ¯æ¥å¯¹è¯ï¼Œæˆ‘ä»¬å…ˆä»‹ç»åœ¨å›¾çŠ¶æ€ä¸­å¤„ç†æ¶ˆæ¯çš„å‡ ç§é«˜çº§æ–¹å¼ï¼šè¿‡æ»¤ï¼ˆFilterï¼‰ä¸è£å‰ªï¼ˆTrimï¼‰ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda",
   "metadata": {
    "id": "d5197aba-5d46-421b-ae3b-4e3034edcfda"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install --quiet -U langchain_core langgraph langchain_openai\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "768dc606-d5f2-468d-96ea-910b264e0f8a",
    "outputId": "86b1e4e0-4df3-42df-cdd2-529a5ca6a231"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "OPENAI_BASE_URL:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "# å°å·¥å…·ï¼šè‹¥ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼Œåˆ™åœ¨è¿è¡Œæ—¶æç¤ºæ‰‹åŠ¨è¾“å…¥å¹¶å†™å…¥åˆ°è¿›ç¨‹ç¯å¢ƒå˜é‡\n",
    "# ç”Ÿäº§ç¯å¢ƒå»ºè®®é€šè¿‡æ›´å®‰å…¨çš„æ–¹å¼ï¼ˆå¦‚ .envã€å¯†é’¥ç®¡ç†æœåŠ¡ã€CI/CD æ³¨å…¥ï¼‰è®¾ç½®\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# OpenAI API å¯†é’¥ï¼ˆç”¨äº ChatOpenAI è°ƒç”¨ï¼‰\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# è®¾ç½® OpenAI APIä»£ç†åœ°å€ (ä¾‹å¦‚ï¼šhttps://api.apiyi.com/v1ï¼‰\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864",
   "metadata": {
    "id": "8b64d8d3-e4ac-4961-bdc0-688825eb5864"
   },
   "source": [
    "æˆ‘ä»¬å°†ä½¿ç”¨ [LangSmith](https://docs.smith.langchain.com/) è¿›è¡Œ[è°ƒç”¨è¿½è¸ªï¼ˆtracingï¼‰](https://docs.smith.langchain.com/concepts/tracing)ã€‚\n",
    "\n",
    "æœ¬ç¬”è®°ä¼šå°†è¿½è¸ªæ•°æ®è®°å½•åˆ°é¡¹ç›® `langchain-academy` ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd020c79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd020c79",
    "outputId": "ae7dec59-b178-4045-fd29-de21c5547aaf"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "# LangSmith ç”¨äºé“¾è·¯è¿½è¸ªä¸å¯è§†åŒ–åˆ†æ\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# æŒ‡å®šè¿½è¸ªé¡¹ç›®åï¼Œä¾¿äºåŒºåˆ†ä¸åŒå®éªŒ/ç¯å¢ƒ\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7",
   "metadata": {
    "id": "72f3fc90-58b6-4f7f-897e-dddf6ae532c7"
   },
   "source": [
    "## å°†æ¶ˆæ¯ä½œä¸ºçŠ¶æ€ï¼ˆMessages as Stateï¼‰\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆå®šä¹‰ä¸€äº›å¯¹è¯æ¶ˆæ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf11a463-e27a-4a05-b41d-64882e38edca",
    "outputId": "1444c716-76be-4eea-c649-029780eaa50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# å®šä¹‰ä¸€ç»„å¯¹è¯æ¶ˆæ¯ï¼ˆå…ˆæ˜¯ AIï¼Œå†æ˜¯äººç±»ï¼‰ï¼Œç”¨äºåç»­ç¤ºä¾‹\n",
    "messages = [AIMessage(f\"ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\", name=\"Bot\")]\n",
    "messages.append(HumanMessage(f\"æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\", name=\"Lance\"))\n",
    "# ä»¥æ›´æ˜“è¯»çš„æ ¼å¼æ‰“å°æ¶ˆæ¯ï¼Œä¾¿äºè°ƒè¯•ä¸ç†è§£\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c",
   "metadata": {
    "id": "b814adcb-6bf9-4b75-be11-e59f933fbd0c"
   },
   "source": [
    "å›é¡¾ä¸€ä¸‹ï¼šæˆ‘ä»¬å¯ä»¥ç›´æ¥æŠŠè¿™ç»„æ¶ˆæ¯ä¼ ç»™èŠå¤©æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4712e288-e622-48a2-ad3f-a52f65f3ab08",
    "outputId": "23c82818-51f1-414b-ea15-a7965cbde0f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='é™¤äº†é²¸é±¼ä¹‹å¤–ï¼Œæµ·æ´‹å“ºä¹³åŠ¨ç‰©è¿˜æœ‰æµ·è±šã€æµ·è±¹ã€æµ·ç‹®ã€æµ·è±¡ã€å„’è‰®å’Œæ±ŸçŒªã€‚\\n\\n1. **æµ·è±š**ï¼šå®ƒä»¬æ˜¯é«˜åº¦æ™ºèƒ½çš„ç”Ÿç‰©ï¼Œå±äºé½¿é²¸äºšç›®ï¼Œè®¸å¤šç§ç±»æœ‰ç€å¤æ‚çš„ç¤¾ä¼šç»“æ„ã€‚\\n\\n2. **æµ·è±¹**ï¼šå±äºé³è¶³ç±»ï¼Œå¸¸è§äºæµ·å²¸å’Œå¸†èˆ¹ä¸Šã€‚å®ƒä»¬é€šè¿‡æ°´ä¸­çš„é³è¿›è¡Œæ¸¸æ³³ã€‚\\n\\n3. **æµ·ç‹®**ï¼šä¹Ÿå±äºé³è¶³ç±»ï¼ŒåŒºåˆ«åœ¨äºå®ƒä»¬å…·æœ‰å¤–è€³å£³ï¼Œå¹¶èƒ½åˆ©ç”¨é³åœ¨é™†åœ°ä¸Šè¡Œèµ°ã€‚\\n\\n4. **æµ·è±¡**ï¼šä»¥å·¨å¤§çš„ç ç‰™å’Œå¥å£®çš„ä½“å‹é—»åï¼Œå®ƒä»¬ä¸»è¦ä¾é é¥®é£Ÿä¸­çš„è½¯ä½“åŠ¨ç‰©ã€‚\\n\\n5. **å„’è‰®**ï¼šä¸€ç§è‰é£Ÿæ€§æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œä»¥æµ·è‰ä¸ºé£Ÿï¼Œæœ‰æ—¶æ˜“è¢«è¯¯è®¤ä¸ºæ˜¯â€œç¾äººé±¼â€ã€‚\\n\\n6. **æ±ŸçŒª**ï¼šå³æ·¡æ°´è±šç±»ï¼Œå®ƒä»¬ç”Ÿæ´»åœ¨æ·¡æ°´åŒºåŸŸï¼Œå¹¶ä»¥å…¶ç‹¬ç‰¹çš„ç”Ÿæ€è§’è‰²é—»åã€‚\\n\\næ¯ç§æµ·æ´‹å“ºä¹³åŠ¨ç‰©éƒ½æœ‰ç‹¬ç‰¹çš„è¡Œä¸ºã€ç”Ÿæ€è§’è‰²å’Œè¿›åŒ–ç‰¹å¾ã€‚äº†è§£è¿™äº›ç”Ÿç‰©ä¸ä»…èƒ½å¤Ÿå¯¹å®ƒä»¬çš„ä¿æŠ¤æä¾›å¸®åŠ©ï¼Œäº¦èƒ½ä¸°å¯Œå¯¹æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿçš„æ•´ä½“è®¤è¯†ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 290, 'prompt_tokens': 45, 'total_tokens': 335, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_8458c98457', 'id': 'chatcmpl-CLKxqFJmzs6p4f3rmg3wNrFUYDMzR', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--415fc185-35d1-4672-915f-f57c28fd387a-0', usage_metadata={'input_tokens': 45, 'output_tokens': 290, 'total_tokens': 335, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# åˆå§‹åŒ–èŠå¤©æ¨¡å‹ï¼›gpt-4o ä»…ä¸ºç¤ºä¾‹ï¼Œå¯æ ¹æ®è´¦å·æƒé™ä¸æˆæœ¬é€‰æ‹©å…¶ä»–æ¨¡å‹\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# ç›´æ¥å°†å®Œæ•´æ¶ˆæ¯å†å²ä¼ å…¥æ¨¡å‹ï¼ˆæ³¨æ„ï¼šé•¿å¯¹è¯ä¼šæ¨é«˜ token ä¸å»¶è¿Ÿï¼‰\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec",
   "metadata": {
    "id": "fbd1dab8-0af8-4621-8264-ce65065f76ec"
   },
   "source": [
    "æˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ªä½¿ç”¨ `MessagesState` çš„ç®€å•å›¾ï¼ˆGraphï¼‰é‡Œè¿è¡ŒèŠå¤©æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "bbd8c39c-633b-4176-9cc6-8318e42bb5dd",
    "outputId": "b5fb0b94-b85f-4e4c-c4ea-48ab0afd87e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›¾å¯è§†åŒ–ï¼š\n",
      "âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "ğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tchat_model(chat_model)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> chat_model;\n",
      "\tchat_model --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\n",
      "èŠ‚ç‚¹: ['__start__', 'chat_model', '__end__']\n",
      "è¾¹: [Edge(source='__start__', target='chat_model', data=None, conditional=False), Edge(source='chat_model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "ğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\n",
      "1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\n",
      "2. è®¿é—® https://mermaid.live/\n",
      "3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\n",
      "4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå›¾èŠ‚ç‚¹ï¼šæ¥æ”¶çŠ¶æ€ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œç›´æ¥è°ƒç”¨èŠå¤©æ¨¡å‹\n",
    "# æ³¨æ„ï¼šæ­¤å¤„æœªåšä»»ä½•è¿‡æ»¤/è£å‰ªï¼Œé•¿å¯¹è¯ä¼šä¸æ–­å¢é•¿\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": llm.invoke(state[\"messages\"])}\n",
    "\n",
    "# æ„å»ºæœ€ç®€å›¾ï¼šSTART -> chat_model -> END\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# å±•ç¤ºå›¾ç»“æ„\n",
    "# å›¾å¯è§†åŒ–\n",
    "print(\"å›¾å¯è§†åŒ–ï¼š\")\n",
    "\n",
    "# æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ Pyppeteer æœ¬åœ°æ¸²æŸ“ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    # å¯è§†åŒ–ï¼šé€šè¿‡ Mermaid æ¸²æŸ“å›¾ç»“æ„\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"âœ… å›¾æ¸²æŸ“æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: {e}\")\n",
    "    \n",
    "    # æ–¹æ¡ˆ2ï¼šæ˜¾ç¤º Mermaid æ–‡æœ¬æ ¼å¼\n",
    "    print(\"\\nğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ–¹æ¡ˆ3ï¼šæ˜¾ç¤ºå›¾çš„èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯\n",
    "    print(\"\\nğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\")\n",
    "    print(\"èŠ‚ç‚¹:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"è¾¹:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # æ–¹æ¡ˆ4ï¼šæä¾›æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜\n",
    "    print(\"\\nğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\")\n",
    "    print(\"1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\")\n",
    "    print(\"2. è®¿é—® https://mermaid.live/\")\n",
    "    print(\"3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\")\n",
    "    print(\"4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a5a3e4a-ccfd-4d14-81f1-f0de6e11a1e4",
    "outputId": "988612ad-e9b8-410e-8541-53f1ad26ea06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "é™¤äº†é²¸é±¼ä¹‹å¤–ï¼Œè¿˜æœ‰å‡ ä¸ªä¸»è¦çš„æµ·æ´‹å“ºä¹³åŠ¨ç‰©ç±»åˆ«å€¼å¾—å…³æ³¨ï¼š\n",
      "\n",
      "1. **æµ·è±š**ï¼šæµ·è±šæ˜¯é²¸ç›®ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œä¸é²¸é±¼å…³ç³»å¯†åˆ‡ã€‚å®ƒä»¬ä»¥æ™ºæ…§å’Œç¤¾ä¼šè¡Œä¸ºè€Œé—»åã€‚\n",
      "\n",
      "2. **æµ·ç‹®**ï¼šæµ·ç‹®å±äºé³è¶³åŠ¨ç‰©ï¼Œé€šå¸¸è¾ƒä¸ºæ´»è·ƒï¼Œå¹¶ä¸”å–œæ¬¢åœ¨æµ·å²¸çº¿é™„è¿‘æ´»åŠ¨ã€‚\n",
      "\n",
      "3. **æµ·è±¹**ï¼šä¸æµ·ç‹®ç±»ä¼¼ï¼Œæµ·è±¹ä¹Ÿæ˜¯é³è¶³åŠ¨ç‰©ã€‚å®ƒä»¬é€šå¸¸åœ¨æ›´å†·çš„æ°´åŸŸä¸­è¢«å‘ç°ï¼Œå¹¶ä»¥å…¶æµçº¿å‹çš„èº«ä½“è‘—ç§°ã€‚\n",
      "\n",
      "4. **æµ·è±¡**ï¼šæµ·è±¡ä»¥å…¶å·¨å¤§çš„ä½“å‹å’Œæ˜¾è‘—çš„é•¿ç‰™è€Œé—»åï¼Œä¸»è¦æ –æ¯åœ¨åŒ—æå’ŒäºšåŒ—æåœ°åŒºã€‚\n",
      "\n",
      "5. **å„’è‰®å’Œæµ·ç‰›**ï¼šä¹Ÿè¢«ç§°ä¸ºç¾äººé±¼æˆ–æµ·ç‰›ï¼Œå„’è‰®å’Œæµ·ç‰›æ˜¯é£Ÿè‰åŠ¨ç‰©ï¼Œå¸¸åœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦æ°´åŸŸè¢«å‘ç°ã€‚\n",
      "\n",
      "è¿™äº›åŠ¨ç‰©éƒ½æœ‰ç‹¬ç‰¹çš„é€‚åº”æ€§å’Œè¡Œä¸ºæ¨¡å¼ï¼Œä½¿å®ƒä»¬åœ¨æµ·æ´‹ç¯å¢ƒä¸­æˆåŠŸç”Ÿå­˜ã€‚å¦‚æœä½ å¯¹æµ·æ´‹å“ºä¹³åŠ¨ç‰©çš„ç”Ÿæ€ã€ä¿æŠ¤å’Œè¡Œä¸ºæ„Ÿå…´è¶£ï¼Œè¿™äº›éƒ½æ˜¯å¾ˆå¥½çš„ç ”ç©¶å¯¹è±¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7",
   "metadata": {
    "id": "34c33e63-1ef4-412d-bb10-6a1b9e5b35a7"
   },
   "source": [
    "## å½’çº¦å™¨ï¼ˆReducerï¼‰\n",
    "\n",
    "åœ¨å¤„ç†æ¶ˆæ¯æ—¶ï¼Œä¸€ä¸ªå®é™…æŒ‘æˆ˜æ˜¯ç®¡ç†â€œé•¿å¯¹è¯â€ã€‚\n",
    "\n",
    "å¦‚æœä¸åŠ æ§åˆ¶ï¼Œéšç€å¯¹è¯å†å²ä¸æ–­å¢é•¿ï¼Œæ¨¡å‹æ¯æ¬¡éƒ½ä¼šæ¥æ”¶è¶Šæ¥è¶Šé•¿çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯¼è‡´ä»¤ç‰Œï¼ˆtokenï¼‰æ¶ˆè€—å¢å¤§ã€å»¶è¿Ÿå‡é«˜ã€‚\n",
    "\n",
    "ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æœ‰å‡ ç§å¸¸ç”¨æ–¹æ³•ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œå›é¡¾ä¹‹å‰çš„æŠ€å·§ï¼šä½¿ç”¨ `RemoveMessage` é…åˆ `add_messages` å½’çº¦å™¨æ¥åˆ é™¤ä¸éœ€è¦çš„å†å²æ¶ˆæ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "222c6bc5-bb0e-4a43-80f5-c8ec38d99f3a",
    "outputId": "4cd3968a-cff6-41f8-8d19-7ef79c3f815b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "# è¿‡æ»¤èŠ‚ç‚¹ï¼šä»…ä¿ç•™æœ€è¿‘ 2 æ¡æ¶ˆæ¯ï¼Œåˆ é™¤æ›´æ—©çš„å†å²\n",
    "# ä½¿ç”¨ RemoveMessage + add_messages å½’çº¦å™¨çš„ç»„åˆè¯­ä¹‰\n",
    "\n",
    "def filter_messages(state: MessagesState):\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"messages\": delete_messages}\n",
    "\n",
    "# è°ƒç”¨èŠ‚ç‚¹ï¼šå°†è¿‡æ»¤åçš„æ¶ˆæ¯ä¼ ç»™æ¨¡å‹\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# æ„å»ºå›¾ï¼šSTART -> filter -> chat_model -> END\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"filter\", filter_messages)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"filter\")\n",
    "builder.add_edge(\"filter\", \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95a7c2cc-54ce-43e7-9a90-abf37827d709",
    "outputId": "a6b1c515-165c-4128-d1d5-5d0c4c79953b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "é™¤äº†é²¸é±¼ä¹‹å¤–ï¼Œæµ·æ´‹å“ºä¹³åŠ¨ç‰©è¿˜æœ‰å¾ˆå¤šç§ç±»ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¸»è¦çš„ç±»åˆ«å’Œä¾‹å­ï¼š\n",
      "\n",
      "1. **æµ·è±š**ï¼šæµ·è±šæ˜¯éå¸¸èªæ˜çš„æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå±äºé½¿é²¸ç±»ã€‚å¸¸è§çš„æœ‰ç“¶é¼»æµ·è±šå’Œæ™®é€šæµ·è±šã€‚\n",
      "\n",
      "2. **é¼ æµ·è±š**ï¼šé¼ æµ·è±šç±»ä¼¼äºæµ·è±šï¼Œä½†é€šå¸¸ä½“å‹è¾ƒå°ï¼Œä¸”èº«ä½“æ›´ä¸ºåœ†æ¶¦ã€‚å¸¸è§çš„ç§ç±»åŒ…æ‹¬æ¸¯æ¹¾é¼ æµ·è±šã€‚\n",
      "\n",
      "3. **æµ·è±¹**ï¼šæµ·è±¹æ˜¯é³è„šç±»åŠ¨ç‰©ä¹‹ä¸€ï¼Œé€‚åº”æ°´é™†ä¸¤æ –ç”Ÿæ´»ã€‚å¸¸è§çš„æœ‰æ–‘æµ·è±¹ã€ç°æµ·è±¹å’Œè±¡æµ·è±¹ã€‚\n",
      "\n",
      "4. **æµ·ç‹®å’Œæµ·ç‹—**ï¼šè™½ç„¶åå­—ä¸­æœ‰â€œç‹®â€å’Œâ€œç‹—â€ï¼Œä½†å®ƒä»¬éƒ½æ˜¯é³è„šç±»åŠ¨ç‰©ã€‚æµ·ç‹®çš„ç‰¹ç‚¹æ˜¯æœ‰å¤–è€³å»“è€Œæµ·è±¹æ²¡æœ‰ï¼Œå¸¸è§çš„æœ‰åŠ å·æµ·ç‹®å’Œå—ç¾æµ·ç‹®ã€‚\n",
      "\n",
      "5. **æµ·è±¡**ï¼šæµ·è±¡æ˜¯å¤§å‹é³è„šç±»åŠ¨ç‰©ï¼Œæ‹¥æœ‰é•¿é•¿çš„ç ç‰™å’Œå«ä½œèƒ¡é¡»çš„è§¦é¡»ï¼Œæ˜¯åŒ—æåœ°åŒºçš„ä»£è¡¨æ€§ç‰©ç§ã€‚\n",
      "\n",
      "6. **å„’è‰®å’Œç¾äººé±¼ï¼ˆæµ·ç‰›ï¼‰**ï¼šè¿™äº›æ˜¯æ¸©æš–æ°´åŸŸçš„é£Ÿè‰åŠ¨ç‰©ã€‚å„’è‰®å’Œç¾äººé±¼åœ¨ç”Ÿæ€ä¸Šéå¸¸é‡è¦ï¼Œå¸®åŠ©ä¿æŒæµ·è‰åºŠçš„å¥åº·ã€‚\n",
      "\n",
      "7. **æåœ°ç†Šï¼ˆåŒ—æç†Šï¼‰**ï¼šè™½ç„¶åŒ—æç†Šä¸»è¦åœ¨é™†åœ°æˆ–å†°é¢ä¸Šæ´»åŠ¨ï¼Œå®ƒä»¬è¢«è®¤ä¸ºæ˜¯æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå› ä¸ºå¤§éƒ¨åˆ†ç”Ÿæ´»ä¾èµ–äºæµ·æ´‹ç¯å¢ƒã€‚\n",
      "\n",
      "è¿™äº›åŠ¨ç‰©å„æœ‰ç‹¬ç‰¹çš„é€‚åº”æ€§å’Œç”Ÿæ€è§’è‰²ï¼Œæ˜¯æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚äº†è§£æ¯ä¸€ç§ç±»çš„ç”Ÿæ´»ä¹ æ€§å’Œä¿æŠ¤æªæ–½ä¼šå¯¹ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©éå¸¸æœ‰å¸®åŠ©ã€‚\n"
     ]
    }
   ],
   "source": [
    "# Message list with a preamble\n",
    "messages = [AIMessage(\"ä½ å¥½ã€‚\", name=\"Bot\", id=\"1\")]\n",
    "messages.append(HumanMessage(\"ä½ å¥½ã€‚\", name=\"Lance\", id=\"2\"))\n",
    "messages.append(AIMessage(\"ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\", name=\"Bot\", id=\"3\"))\n",
    "messages.append(HumanMessage(\"æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\", name=\"Lance\", id=\"4\"))\n",
    "\n",
    "# Invoke\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d",
   "metadata": {
    "id": "f506457d-014b-4fee-a684-e5edfb4b8f0d"
   },
   "source": [
    "## è¿‡æ»¤æ¶ˆæ¯ï¼ˆFiltering Messagesï¼‰\n",
    "\n",
    "å¦‚æœä½ ä¸éœ€è¦æˆ–ä¸æƒ³ä¿®æ”¹å›¾çŠ¶æ€æœ¬èº«ï¼Œå¯ä»¥åªå¯¹â€œä¼ å…¥èŠå¤©æ¨¡å‹çš„æ¶ˆæ¯åˆ—è¡¨â€è¿›è¡Œè¿‡æ»¤ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œä»…æŠŠæœ€åä¸€æ¡æ¶ˆæ¯ä¼ ç»™æ¨¡å‹ï¼š`llm.invoke(messages[-1:])`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "22d0b904-7cd6-486b-8948-105bee3d4683",
    "outputId": "f05f73bc-05e6-47a5-b65e-0a06071aa7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›¾å¯è§†åŒ–ï¼š\n",
      "âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "ğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tchat_model(chat_model)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> chat_model;\n",
      "\tchat_model --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\n",
      "èŠ‚ç‚¹: ['__start__', 'chat_model', '__end__']\n",
      "è¾¹: [Edge(source='__start__', target='chat_model', data=None, conditional=False), Edge(source='chat_model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "ğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\n",
      "1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\n",
      "2. è®¿é—® https://mermaid.live/\n",
      "3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\n",
      "4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\n"
     ]
    }
   ],
   "source": [
    "# ä»…ä¼ å…¥â€œæœ€åä¸€æ¡æ¶ˆæ¯â€ç»™æ¨¡å‹ï¼Œé¿å…æŠŠæ•´æ®µå†å²éƒ½é€å…¥æ¨¡å‹\n",
    "# æ³¨æ„ï¼šè¿™ç§æ–¹å¼é€‚ç”¨äºä¸Šä¸‹æ–‡ä¾èµ–è¾ƒå¼±çš„é—®ç­”ï¼›è‹¥å¼ºä¾èµ–ä¸Šä¸‹æ–‡ï¼Œè¯·è€ƒè™‘è£å‰ªï¼ˆtrimï¼‰è€Œéç®€å•è¿‡æ»¤\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"][-1:])]}\n",
    "\n",
    "# æ„å»ºå›¾\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# å±•ç¤ºå›¾ç»“æ„\n",
    "# å›¾å¯è§†åŒ–\n",
    "print(\"å›¾å¯è§†åŒ–ï¼š\")\n",
    "\n",
    "# æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ Pyppeteer æœ¬åœ°æ¸²æŸ“ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    # å¯è§†åŒ–ï¼šé€šè¿‡ Mermaid æ¸²æŸ“å›¾ç»“æ„\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"âœ… å›¾æ¸²æŸ“æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: {e}\")\n",
    "    \n",
    "    # æ–¹æ¡ˆ2ï¼šæ˜¾ç¤º Mermaid æ–‡æœ¬æ ¼å¼\n",
    "    print(\"\\nğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ–¹æ¡ˆ3ï¼šæ˜¾ç¤ºå›¾çš„èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯\n",
    "    print(\"\\nğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\")\n",
    "    print(\"èŠ‚ç‚¹:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"è¾¹:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # æ–¹æ¡ˆ4ï¼šæä¾›æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜\n",
    "    print(\"\\nğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\")\n",
    "    print(\"1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\")\n",
    "    print(\"2. è®¿é—® https://mermaid.live/\")\n",
    "    print(\"3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\")\n",
    "    print(\"4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5",
   "metadata": {
    "id": "6f58c6fc-532f-418d-b70a-cfcb3307daf5"
   },
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠŠç°æœ‰æ¶ˆæ¯åˆ—è¡¨ç»§ç»­æ‰©å±•ï¼šå…ˆåŠ å…¥ä¸Šé¢çš„ LLM å›å¤ï¼Œå†è¿½åŠ ä¸€ä¸ªè¿½é—®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16956015-1dbe-4108-89b5-4209b68b51ca",
   "metadata": {
    "id": "16956015-1dbe-4108-89b5-4209b68b51ca"
   },
   "outputs": [],
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"å†å‘Šè¯‰æˆ‘ä¸€äº›å…³äºç‹¬è§’é²¸çš„äº‹å§ï¼\", name=\"Lance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85563415-c085-46a8-a4ac-155df798c54e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85563415-c085-46a8-a4ac-155df798c54e",
    "outputId": "cc792e3f-a61c-4047-8954-74b6620fd095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ å¥½ã€‚\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "ä½ å¥½ã€‚\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "é™¤äº†é²¸é±¼ä¹‹å¤–ï¼Œæµ·æ´‹å“ºä¹³åŠ¨ç‰©è¿˜æœ‰å¾ˆå¤šç§ç±»ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¸»è¦çš„ç±»åˆ«å’Œä¾‹å­ï¼š\n",
      "\n",
      "1. **æµ·è±š**ï¼šæµ·è±šæ˜¯éå¸¸èªæ˜çš„æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå±äºé½¿é²¸ç±»ã€‚å¸¸è§çš„æœ‰ç“¶é¼»æµ·è±šå’Œæ™®é€šæµ·è±šã€‚\n",
      "\n",
      "2. **é¼ æµ·è±š**ï¼šé¼ æµ·è±šç±»ä¼¼äºæµ·è±šï¼Œä½†é€šå¸¸ä½“å‹è¾ƒå°ï¼Œä¸”èº«ä½“æ›´ä¸ºåœ†æ¶¦ã€‚å¸¸è§çš„ç§ç±»åŒ…æ‹¬æ¸¯æ¹¾é¼ æµ·è±šã€‚\n",
      "\n",
      "3. **æµ·è±¹**ï¼šæµ·è±¹æ˜¯é³è„šç±»åŠ¨ç‰©ä¹‹ä¸€ï¼Œé€‚åº”æ°´é™†ä¸¤æ –ç”Ÿæ´»ã€‚å¸¸è§çš„æœ‰æ–‘æµ·è±¹ã€ç°æµ·è±¹å’Œè±¡æµ·è±¹ã€‚\n",
      "\n",
      "4. **æµ·ç‹®å’Œæµ·ç‹—**ï¼šè™½ç„¶åå­—ä¸­æœ‰â€œç‹®â€å’Œâ€œç‹—â€ï¼Œä½†å®ƒä»¬éƒ½æ˜¯é³è„šç±»åŠ¨ç‰©ã€‚æµ·ç‹®çš„ç‰¹ç‚¹æ˜¯æœ‰å¤–è€³å»“è€Œæµ·è±¹æ²¡æœ‰ï¼Œå¸¸è§çš„æœ‰åŠ å·æµ·ç‹®å’Œå—ç¾æµ·ç‹®ã€‚\n",
      "\n",
      "5. **æµ·è±¡**ï¼šæµ·è±¡æ˜¯å¤§å‹é³è„šç±»åŠ¨ç‰©ï¼Œæ‹¥æœ‰é•¿é•¿çš„ç ç‰™å’Œå«ä½œèƒ¡é¡»çš„è§¦é¡»ï¼Œæ˜¯åŒ—æåœ°åŒºçš„ä»£è¡¨æ€§ç‰©ç§ã€‚\n",
      "\n",
      "6. **å„’è‰®å’Œç¾äººé±¼ï¼ˆæµ·ç‰›ï¼‰**ï¼šè¿™äº›æ˜¯æ¸©æš–æ°´åŸŸçš„é£Ÿè‰åŠ¨ç‰©ã€‚å„’è‰®å’Œç¾äººé±¼åœ¨ç”Ÿæ€ä¸Šéå¸¸é‡è¦ï¼Œå¸®åŠ©ä¿æŒæµ·è‰åºŠçš„å¥åº·ã€‚\n",
      "\n",
      "7. **æåœ°ç†Šï¼ˆåŒ—æç†Šï¼‰**ï¼šè™½ç„¶åŒ—æç†Šä¸»è¦åœ¨é™†åœ°æˆ–å†°é¢ä¸Šæ´»åŠ¨ï¼Œå®ƒä»¬è¢«è®¤ä¸ºæ˜¯æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå› ä¸ºå¤§éƒ¨åˆ†ç”Ÿæ´»ä¾èµ–äºæµ·æ´‹ç¯å¢ƒã€‚\n",
      "\n",
      "è¿™äº›åŠ¨ç‰©å„æœ‰ç‹¬ç‰¹çš„é€‚åº”æ€§å’Œç”Ÿæ€è§’è‰²ï¼Œæ˜¯æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚äº†è§£æ¯ä¸€ç§ç±»çš„ç”Ÿæ´»ä¹ æ€§å’Œä¿æŠ¤æªæ–½ä¼šå¯¹ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©éå¸¸æœ‰å¸®åŠ©ã€‚\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "å†å‘Šè¯‰æˆ‘ä¸€äº›å…³äºç‹¬è§’é²¸çš„äº‹å§ï¼\n"
     ]
    }
   ],
   "source": [
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23349705-a059-47b5-9760-d8f64e687393",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23349705-a059-47b5-9760-d8f64e687393",
    "outputId": "a54ab476-d742-47c8-86ca-3d5c35deaaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ å¥½ã€‚\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "ä½ å¥½ã€‚\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Bot\n",
      "\n",
      "ä½ ä¹‹å‰è¯´ä½ åœ¨ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå¯¹å—ï¼Ÿ\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "æ˜¯çš„ï¼Œæˆ‘çŸ¥é“é²¸é±¼ã€‚ä½†æˆ‘è¿˜åº”è¯¥äº†è§£å“ªäº›å…¶ä»–çš„ï¼Ÿ\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "é™¤äº†é²¸é±¼ä¹‹å¤–ï¼Œæµ·æ´‹å“ºä¹³åŠ¨ç‰©è¿˜æœ‰å¾ˆå¤šç§ç±»ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¸»è¦çš„ç±»åˆ«å’Œä¾‹å­ï¼š\n",
      "\n",
      "1. **æµ·è±š**ï¼šæµ·è±šæ˜¯éå¸¸èªæ˜çš„æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå±äºé½¿é²¸ç±»ã€‚å¸¸è§çš„æœ‰ç“¶é¼»æµ·è±šå’Œæ™®é€šæµ·è±šã€‚\n",
      "\n",
      "2. **é¼ æµ·è±š**ï¼šé¼ æµ·è±šç±»ä¼¼äºæµ·è±šï¼Œä½†é€šå¸¸ä½“å‹è¾ƒå°ï¼Œä¸”èº«ä½“æ›´ä¸ºåœ†æ¶¦ã€‚å¸¸è§çš„ç§ç±»åŒ…æ‹¬æ¸¯æ¹¾é¼ æµ·è±šã€‚\n",
      "\n",
      "3. **æµ·è±¹**ï¼šæµ·è±¹æ˜¯é³è„šç±»åŠ¨ç‰©ä¹‹ä¸€ï¼Œé€‚åº”æ°´é™†ä¸¤æ –ç”Ÿæ´»ã€‚å¸¸è§çš„æœ‰æ–‘æµ·è±¹ã€ç°æµ·è±¹å’Œè±¡æµ·è±¹ã€‚\n",
      "\n",
      "4. **æµ·ç‹®å’Œæµ·ç‹—**ï¼šè™½ç„¶åå­—ä¸­æœ‰â€œç‹®â€å’Œâ€œç‹—â€ï¼Œä½†å®ƒä»¬éƒ½æ˜¯é³è„šç±»åŠ¨ç‰©ã€‚æµ·ç‹®çš„ç‰¹ç‚¹æ˜¯æœ‰å¤–è€³å»“è€Œæµ·è±¹æ²¡æœ‰ï¼Œå¸¸è§çš„æœ‰åŠ å·æµ·ç‹®å’Œå—ç¾æµ·ç‹®ã€‚\n",
      "\n",
      "5. **æµ·è±¡**ï¼šæµ·è±¡æ˜¯å¤§å‹é³è„šç±»åŠ¨ç‰©ï¼Œæ‹¥æœ‰é•¿é•¿çš„ç ç‰™å’Œå«ä½œèƒ¡é¡»çš„è§¦é¡»ï¼Œæ˜¯åŒ—æåœ°åŒºçš„ä»£è¡¨æ€§ç‰©ç§ã€‚\n",
      "\n",
      "6. **å„’è‰®å’Œç¾äººé±¼ï¼ˆæµ·ç‰›ï¼‰**ï¼šè¿™äº›æ˜¯æ¸©æš–æ°´åŸŸçš„é£Ÿè‰åŠ¨ç‰©ã€‚å„’è‰®å’Œç¾äººé±¼åœ¨ç”Ÿæ€ä¸Šéå¸¸é‡è¦ï¼Œå¸®åŠ©ä¿æŒæµ·è‰åºŠçš„å¥åº·ã€‚\n",
      "\n",
      "7. **æåœ°ç†Šï¼ˆåŒ—æç†Šï¼‰**ï¼šè™½ç„¶åŒ—æç†Šä¸»è¦åœ¨é™†åœ°æˆ–å†°é¢ä¸Šæ´»åŠ¨ï¼Œå®ƒä»¬è¢«è®¤ä¸ºæ˜¯æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œå› ä¸ºå¤§éƒ¨åˆ†ç”Ÿæ´»ä¾èµ–äºæµ·æ´‹ç¯å¢ƒã€‚\n",
      "\n",
      "è¿™äº›åŠ¨ç‰©å„æœ‰ç‹¬ç‰¹çš„é€‚åº”æ€§å’Œç”Ÿæ€è§’è‰²ï¼Œæ˜¯æµ·æ´‹ç”Ÿæ€ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚äº†è§£æ¯ä¸€ç§ç±»çš„ç”Ÿæ´»ä¹ æ€§å’Œä¿æŠ¤æªæ–½ä¼šå¯¹ç ”ç©¶æµ·æ´‹å“ºä¹³åŠ¨ç‰©éå¸¸æœ‰å¸®åŠ©ã€‚\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Lance\n",
      "\n",
      "å†å‘Šè¯‰æˆ‘ä¸€äº›å…³äºç‹¬è§’é²¸çš„äº‹å§ï¼\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "å½“ç„¶å¯ä»¥ï¼ç‹¬è§’é²¸æ˜¯ä¸€ç§æµ·æ´‹å“ºä¹³åŠ¨ç‰©ï¼Œç”Ÿæ´»åœ¨åŒ—æé™„è¿‘çš„å¯’å†·æµ·åŸŸã€‚å®ƒä»¬ä»¥å…¶ç‹¬ç‰¹çš„èºæ—‹å½¢é•¿ç‰™è€Œé—»åï¼Œè¿™æ ¹é•¿ç‰™å…¶å®æ˜¯é›„æ€§ç‹¬è§’é²¸å·¦ä¾§ç‰™é½¿çš„å»¶ä¼¸ï¼Œå¯ä»¥é•¿è¾¾3ç±³ã€‚è™½ç„¶é•¿ç‰™çš„ç¡®åˆ‡åŠŸèƒ½å°šæœªå®Œå…¨ç†è§£ï¼Œä½†ç§‘å­¦å®¶è®¤ä¸ºå®ƒå¯èƒ½ç”¨äºäº‰å¤ºé…å¶ã€æ‰“ç ´å†°å±‚æˆ–æ˜¯ä½œä¸ºä¸€ç§æ„Ÿè§‰å™¨å®˜ã€‚\n",
      "\n",
      "ç‹¬è§’é²¸é€šå¸¸ä½“é•¿åœ¨4åˆ°5ç±³ä¹‹é—´ï¼Œä½“é‡å¯è¾¾1600å…¬æ–¤ã€‚å®ƒä»¬å–œæ¬¢æˆç¾¤ç»“é˜Ÿï¼Œé€šå¸¸10åˆ°20åªç‹¬è§’é²¸ä¼šä¸€èµ·æ´»åŠ¨ã€‚é™¤äº†é•¿ç‰™ï¼Œå®ƒä»¬è¿˜æœ‰ç€åœ†æ¶¦çš„èº«ä½“å’Œç°è‰²æ–‘ç‚¹çš„çš®è‚¤ã€‚\n",
      "\n",
      "ç‹¬è§’é²¸ä¸»è¦ä»¥é±¼ç±»ã€ç”²å£³çº²åŠ¨ç‰©å’Œè½¯ä½“åŠ¨ç‰©ä¸ºé£Ÿã€‚å®ƒä»¬èƒ½å¤Ÿæ½œæ°´åˆ°å¾ˆæ·±çš„æ°´åŸŸï¼Œä»¥å¯»æ‰¾é£Ÿç‰©ã€‚ç”±äºæ –æ¯åœ°çš„å¯’å†·ï¼Œç‹¬è§’é²¸æœ‰ç€åšé‡çš„è„‚è‚ªå±‚ä»¥ä¿æ¸©ã€‚\n",
      "\n",
      "è¿™äº›ç‹¬ç‰¹çš„ç”Ÿç‰©å¯¹æ°”å€™å˜åŒ–éå¸¸æ•æ„Ÿï¼Œå› ä¸ºå†°å±‚çš„å˜åŒ–ç›´æ¥å½±å“åˆ°å®ƒä»¬çš„ç”Ÿå­˜ç¯å¢ƒã€‚éšç€åŒ—æå†°å±‚çš„èåŒ–ï¼Œç§‘å­¦å®¶ä»¬å¯¹ç‹¬è§’é²¸çš„æœªæ¥è¡¨ç¤ºæ‹…å¿§ã€‚\n",
      "\n",
      "å¸Œæœ›è¿™äº›ä¿¡æ¯å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œè¯·éšæ—¶é—®æˆ‘ã€‚\n"
     ]
    }
   ],
   "source": [
    "# Invoke, using message filtering\n",
    "output = graph.invoke({'messages': messages})\n",
    "for m in output['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745",
   "metadata": {
    "id": "42e1d8d2-e297-4d78-b54c-d12b3c866745"
   },
   "source": [
    "å›¾çŠ¶æ€ä¸­ä»ç„¶ä¿å­˜ç€å…¨éƒ¨æ¶ˆæ¯ã€‚\n",
    "\n",
    "ä¸è¿‡ï¼Œä» LangSmith çš„è¿½è¸ªå¯ä»¥çœ‹åˆ°ï¼šæ¨¡å‹å®é™…è°ƒç”¨æ—¶åªä½¿ç”¨äº†æœ€åä¸€æ¡æ¶ˆæ¯ã€‚\n",
    "\n",
    "https://smith.langchain.com/public/75aca3ce-ef19-4b92-94be-0178c7a660d9/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c",
   "metadata": {
    "id": "fc40d930-3c1f-47fe-8d2a-ce174873353c"
   },
   "source": [
    "## è£å‰ªæ¶ˆæ¯ï¼ˆTrim Messagesï¼‰\n",
    "\n",
    "å¦ä¸€ç§æ–¹å¼æ˜¯æŒ‰ä»¤ç‰Œæ•°å¯¹å†å²è¿›è¡Œâ€œè£å‰ªâ€ï¼ˆtrimï¼‰ï¼šå³é€šè¿‡ [`trim_messages`](https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens) é™åˆ¶ç”¨äºæœ¬æ¬¡å›å¤çš„å†å²æ¶ˆæ¯æ€»ä»¤ç‰Œæ•°ã€‚\n",
    "\n",
    "- è¿‡æ»¤ï¼ˆfilterï¼‰ï¼šåªæ˜¯äº‹åæŒ‘é€‰ä¸€éƒ¨åˆ†æ¶ˆæ¯ä¼ ç»™æ¨¡å‹ï¼›\n",
    "- è£å‰ªï¼ˆtrimï¼‰ï¼šä»å¤´/å°¾ç­‰ç­–ç•¥å‡ºå‘ï¼Œä¸¥æ ¼é™åˆ¶å¯ç”¨çš„å†å²ä¸Šä¸‹æ–‡æ‰€å ä»¤ç‰Œæ•°ã€‚\n",
    "\n",
    "ä¸‹é¢æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ `trim_messages`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "2ff99b81-cf03-4cc2-b44f-44829a73e1fd",
    "outputId": "01d5e043-6313-4da4-e557-db2851691a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›¾å¯è§†åŒ–ï¼š\n",
      "âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "ğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tchat_model(chat_model)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> chat_model;\n",
      "\tchat_model --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\n",
      "èŠ‚ç‚¹: ['__start__', 'chat_model', '__end__']\n",
      "è¾¹: [Edge(source='__start__', target='chat_model', data=None, conditional=False), Edge(source='chat_model', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "ğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\n",
      "1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\n",
      "2. è®¿é—® https://mermaid.live/\n",
      "3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\n",
      "4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "# èŠ‚ç‚¹ï¼šå¯¹â€œå†å²æ¶ˆæ¯â€æŒ‰ä»¤ç‰Œæ•°è¿›è¡Œè£å‰ªï¼Œå†è°ƒç”¨æ¨¡å‹\n",
    "# - max_tokensï¼šå†å²æ¶ˆæ¯å…è®¸å ç”¨çš„æœ€å¤§ token æ•°\n",
    "# - strategyï¼šè£å‰ªç­–ç•¥ï¼ˆ\"last\" è¡¨ç¤ºä¼˜å…ˆä¿ç•™é åçš„æ¶ˆæ¯ï¼‰\n",
    "# - token_counterï¼šç”¨äºä¼°ç®— token æ•°çš„æ¨¡å‹æˆ–è®¡æ•°å™¨\n",
    "# - allow_partialï¼šä¸º False æ—¶ï¼Œä¸ä¼šæˆªæ–­å•æ¡æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†ï¼ˆä¿è¯æ¯æ¡æ¶ˆæ¯çš„å®Œæ•´æ€§ï¼‰\n",
    "\n",
    "def chat_model_node(state: MessagesState):\n",
    "    messages = trim_messages(\n",
    "            state[\"messages\"],\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False,\n",
    "        )\n",
    "    return {\"messages\": [llm.invoke(messages)]}\n",
    "\n",
    "# æ„å»ºå›¾\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"chat_model\", chat_model_node)\n",
    "builder.add_edge(START, \"chat_model\")\n",
    "builder.add_edge(\"chat_model\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# å±•ç¤ºå›¾ç»“æ„\n",
    "# å›¾å¯è§†åŒ–\n",
    "print(\"å›¾å¯è§†åŒ–ï¼š\")\n",
    "\n",
    "# æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ Pyppeteer æœ¬åœ°æ¸²æŸ“ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    # å¯è§†åŒ–ï¼šé€šè¿‡ Mermaid æ¸²æŸ“å›¾ç»“æ„\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"âœ… å›¾æ¸²æŸ“æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: {e}\")\n",
    "    \n",
    "    # æ–¹æ¡ˆ2ï¼šæ˜¾ç¤º Mermaid æ–‡æœ¬æ ¼å¼\n",
    "    print(\"\\nğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ–¹æ¡ˆ3ï¼šæ˜¾ç¤ºå›¾çš„èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯\n",
    "    print(\"\\nğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\")\n",
    "    print(\"èŠ‚ç‚¹:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"è¾¹:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # æ–¹æ¡ˆ4ï¼šæä¾›æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜\n",
    "    print(\"\\nğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\")\n",
    "    print(\"1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\")\n",
    "    print(\"2. è®¿é—® https://mermaid.live/\")\n",
    "    print(\"3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\")\n",
    "    print(\"4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24df63ac-da29-4874-b3df-7e390e97cc8a",
   "metadata": {
    "id": "24df63ac-da29-4874-b3df-7e390e97cc8a"
   },
   "outputs": [],
   "source": [
    "messages.append(output['messages'][-1])\n",
    "messages.append(HumanMessage(f\"Tell me where Orcas live!\", name=\"Lance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d9d8971-c75c-43ca-a209-eb1d07b2ead0",
    "outputId": "82e30268-08c3-4ad0-9812-15a56502a810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me where Orcas live!', additional_kwargs={}, response_metadata={}, name='Lance')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of trimming messages\n",
    "trim_messages(\n",
    "            messages,\n",
    "            max_tokens=100,\n",
    "            strategy=\"last\",\n",
    "            token_counter=ChatOpenAI(model=\"gpt-4o\"),\n",
    "            allow_partial=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed70a269-a869-4fa0-a1df-29736a432c51",
    "outputId": "a3aa43b7-7b84-44b0-c27f-971c6e1df9ac"
   },
   "outputs": [],
   "source": [
    "# Invoke, using message trimming in the chat_model_node\n",
    "messages_out_trim = graph.invoke({'messages': messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b3db67-380e-46b5-9a6a-20100ba52008",
   "metadata": {
    "id": "38b3db67-380e-46b5-9a6a-20100ba52008"
   },
   "source": [
    "æœ€åï¼Œæˆ‘ä»¬å†é€šè¿‡ LangSmith è¿½è¸ªçœ‹çœ‹æœ¬æ¬¡æ¨¡å‹è°ƒç”¨çš„å®é™…å…¥å‚æƒ…å†µï¼š\n",
    "\n",
    "![image-20250930110917504](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301109900.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
