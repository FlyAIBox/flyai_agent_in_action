{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1867.21 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 788.88 GB, Free: 1123.48 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651ead9-5504-45ee-938d-f91ac78dddd1",
   "metadata": {
    "id": "b651ead9-5504-45ee-938d-f91ac78dddd1"
   },
   "source": [
    "# 带消息摘要功能的聊天机器人\n",
    "\n",
    "## 回顾\n",
    "\n",
    "我们已经学习了如何自定义图状态模式（graph state schema）和状态归约器（reducer）。\n",
    "\n",
    "我们也展示了多种在图状态中修剪或过滤消息的方法。\n",
    "\n",
    "## 目标\n",
    "\n",
    "现在，让我们更进一步！\n",
    "\n",
    "不仅仅是修剪或过滤消息，我们将展示**如何使用大语言模型（LLM）来生成对话的持续摘要**。\n",
    "\n",
    "这使我们能够保留完整对话的压缩表示，而不是简单地通过修剪或过滤来删除它们。\n",
    "\n",
    "我们将把这种摘要功能整合到一个简单的聊天机器人中。\n",
    "\n",
    "并且我们将为这个聊天机器人配备记忆功能，支持长时间运行的对话，而不会产生高昂的token成本或延迟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d",
   "metadata": {
    "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的依赖包\n",
    "# 使用 %%capture 来隐藏安装过程的输出信息\n",
    "# %%capture --no-stderr\n",
    "# %pip install --quiet -U langchain_core langgraph langchain_openai\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09201a62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09201a62",
    "outputId": "181fdbf3-2fa5-42a5-d4de-ec9ab8e0a9ee"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "    如果环境变量不存在，则提示用户输入\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd",
   "metadata": {
    "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[追踪](https://docs.smith.langchain.com/concepts/tracing)。\n",
    "\n",
    "LangSmith 是 LangChain 提供的调试和监控工具，可以帮助我们：\n",
    "- 追踪 LLM 调用链\n",
    "- 监控性能和成本\n",
    "- 调试对话流程\n",
    "- 分析模型行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "464856d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "464856d4",
    "outputId": "02083c1b-58d2-4ecf-9bfd-d176c5ed864a"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# 设置 LangSmith 相关环境变量\n",
    "# LangSmith 用于追踪和调试 LangChain 应用\n",
    "_set_env(\"LANGSMITH_API_KEY\")  # LangSmith API 密钥\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # 启用追踪功能\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\"  # 设置项目名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392",
   "metadata": {
    "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化 OpenAI 聊天模型\n",
    "# gpt-4o: 使用 GPT-4 Omni 模型，这是 OpenAI 最新的多模态模型\n",
    "# temperature=0: 设置温度为 0，使模型输出更加确定性和一致\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b",
   "metadata": {
    "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b"
   },
   "source": [
    "我们将使用 `MessagesState`，就像之前一样。\n",
    "\n",
    "除了内置的 `messages` 键之外，我们现在还将包含一个自定义键（`summary`）。\n",
    "\n",
    "**状态设计说明：**\n",
    "- `messages`: 存储对话历史消息列表\n",
    "- `summary`: 存储对话的摘要信息，用于压缩长对话历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "948e60f0-5c76-4235-b40e-cf523205d40e",
   "metadata": {
    "id": "948e60f0-5c76-4235-b40e-cf523205d40e"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"\n",
    "    自定义状态类，继承自 MessagesState\n",
    "\n",
    "    继承的功能：\n",
    "    - messages: 消息列表，用于存储对话历史\n",
    "\n",
    "    新增功能：\n",
    "    - summary: 字符串类型，用于存储对话摘要\n",
    "    \"\"\"\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855ea31-5cc1-4277-a189-0b72459f67ec",
   "metadata": {
    "id": "6855ea31-5cc1-4277-a189-0b72459f67ec"
   },
   "source": [
    "我们将定义一个节点来调用我们的 LLM，如果存在摘要，则将其融入到提示中。\n",
    "\n",
    "**节点功能说明：**\n",
    "- 检查状态中是否存在摘要\n",
    "- 如果存在摘要，将其作为系统消息添加到对话中\n",
    "- 调用 LLM 生成回复\n",
    "- 返回 AI 的回复消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b",
   "metadata": {
    "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "def call_model(state: State):\n",
    "    \"\"\"\n",
    "    调用 LLM 模型生成回复的节点函数\n",
    "\n",
    "    参数:\n",
    "        state (State): 当前图状态，包含消息和摘要信息\n",
    "\n",
    "    返回:\n",
    "        dict: 包含 AI 回复消息的字典\n",
    "\n",
    "    功能说明:\n",
    "        1. 检查状态中是否存在对话摘要\n",
    "        2. 如果存在摘要，将其作为系统消息添加到对话上下文中\n",
    "        3. 调用 LLM 模型生成回复\n",
    "        4. 返回包含 AI 回复的状态更新\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取摘要信息，如果不存在则返回空字符串\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # 如果存在摘要，将其融入到提示中\n",
    "    if summary:\n",
    "        # 创建包含摘要的系统消息\n",
    "        # 这有助于 LLM 理解之前的对话上下文\n",
    "        # system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        system_message = f\"此前对话的摘要：{summary}\"\n",
    "\n",
    "        # 将系统消息添加到消息列表的开头\n",
    "        # 系统消息通常用于提供上下文和指令\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "\n",
    "    else:\n",
    "        # 如果没有摘要，直接使用原始消息\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    # 调用 LLM 模型生成回复\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 返回包含新消息的状态更新\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450",
   "metadata": {
    "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450"
   },
   "source": [
    "我们将定义一个节点来生成摘要。\n",
    "\n",
    "**摘要节点功能：**\n",
    "- 分析当前对话历史\n",
    "- 使用 LLM 生成对话摘要\n",
    "- 清理旧消息，只保留最近的几条消息\n",
    "- 更新状态中的摘要信息\n",
    "\n",
    "**注意：** 这里我们将使用 `RemoveMessage` 在生成摘要后过滤我们的状态，这样可以：\n",
    "- 减少内存占用\n",
    "- 降低后续调用的 token 成本\n",
    "- 保持对话的连续性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e",
   "metadata": {
    "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e"
   },
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \"\"\"\n",
    "    生成对话摘要的节点函数\n",
    "\n",
    "    参数:\n",
    "        state (State): 当前图状态，包含消息和摘要信息\n",
    "\n",
    "    返回:\n",
    "        dict: 包含新摘要和消息删除指令的字典\n",
    "\n",
    "    功能说明:\n",
    "        1. 检查是否已存在摘要\n",
    "        2. 根据情况创建不同的摘要提示\n",
    "        3. 调用 LLM 生成或更新摘要\n",
    "        4. 删除旧消息，只保留最近的 2 条消息\n",
    "        5. 更新状态中的摘要信息\n",
    "    \"\"\"\n",
    "\n",
    "    # 首先获取任何现有的摘要\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    if summary:\n",
    "      # 如果摘要已存在，则扩展现有摘要\n",
    "      summary_message = (\n",
    "          f\"目前为止的对话摘要：{summary}\\n\\n\"\n",
    "          \"请结合上方的新消息，扩展现有摘要：\"\n",
    "      )\n",
    "\n",
    "    else:\n",
    "        # 如果没有现有摘要，则创建新摘要\n",
    "        summary_message = \"请对上方的对话创建摘要：\"\n",
    "\n",
    "    # 将摘要提示添加到消息历史中\n",
    "    # 这样 LLM 可以基于完整的对话历史生成摘要\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "\n",
    "    # 调用 LLM 生成摘要\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 删除除最近 2 条消息外的所有消息\n",
    "    # 这样可以减少内存占用和 token 成本\n",
    "    # RemoveMessage 用于标记消息为删除状态\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "\n",
    "    # 返回新的摘要和消息删除指令\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982993e-f4be-4ff7-9a38-886f75398b3d",
   "metadata": {
    "id": "f982993e-f4be-4ff7-9a38-886f75398b3d"
   },
   "source": [
    "我们将添加一个条件边来决定是否基于对话长度生成摘要。\n",
    "\n",
    "**条件边的作用：**\n",
    "- 监控对话中的消息数量\n",
    "- 当消息数量超过阈值时，触发摘要生成\n",
    "- 否则直接结束对话\n",
    "- 这样可以平衡对话质量和性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b507665d-7f5d-442a-b498-218c94c5dd8b",
   "metadata": {
    "id": "b507665d-7f5d-442a-b498-218c94c5dd8b"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from typing_extensions import Literal\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"\n",
    "    决定是否继续对话或生成摘要的条件函数\n",
    "\n",
    "    参数:\n",
    "        state (State): 当前图状态\n",
    "\n",
    "    返回:\n",
    "        Literal: 下一个要执行的节点名称或 END\n",
    "\n",
    "    功能说明:\n",
    "        根据对话中的消息数量决定下一步操作：\n",
    "        - 如果消息数量 > 6，则生成摘要\n",
    "        - 否则直接结束对话\n",
    "\n",
    "    设计理念:\n",
    "        这个阈值可以根据实际需求调整：\n",
    "        - 较小的阈值：更频繁的摘要，但可能丢失细节\n",
    "        - 较大的阈值：保留更多上下文，但可能增加成本\n",
    "    \"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 如果消息数量超过 6 条，则生成对话摘要\n",
    "    # 这个阈值可以根据实际需求调整\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "\n",
    "    # 否则直接结束对话\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd",
   "metadata": {
    "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd"
   },
   "source": [
    "## 添加记忆功能\n",
    "\n",
    "回顾一下，[状态在单次图执行中是瞬态的](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220)。\n",
    "\n",
    "这限制了我们在中断情况下进行多轮对话的能力。\n",
    "\n",
    "正如在模块 1 末尾介绍的那样，我们可以使用[持久化](https://langchain-ai.github.io/langgraph/how-tos/persistence/)来解决这个问题！\n",
    "\n",
    "**LangGraph 持久化机制：**\n",
    "- LangGraph 可以使用检查点器（checkpointer）在每一步后自动保存图状态\n",
    "- 这个内置的持久化层为我们提供了记忆功能\n",
    "- 允许 LangGraph 从最后一次状态更新处继续执行\n",
    "\n",
    "**MemorySaver 介绍：**\n",
    "- 我们之前展示的最容易使用的检查点器之一是 `MemorySaver`\n",
    "- 它是一个用于图状态的内存键值存储\n",
    "- 我们只需要在编译图时添加检查点器，图就具有了记忆功能！\n",
    "\n",
    "**记忆功能的好处：**\n",
    "- 支持长时间运行的对话\n",
    "- 可以在中断后恢复对话\n",
    "- 保持对话的连续性和上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
    "outputId": "47e6692c-8686-4f20-939d-ad595e76a491"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# 定义一个新的图工作流\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 添加节点\n",
    "# conversation: 处理用户输入并生成回复的节点\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "# summarize_conversation: 生成对话摘要的节点\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# 设置图的连接关系\n",
    "# 设置入口点为 conversation 节点\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "# 添加条件边：根据消息数量决定是否生成摘要\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "# 摘要生成完成后直接结束\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# 编译图并添加记忆功能\n",
    "memory = MemorySaver()  # 创建内存检查点器\n",
    "graph = workflow.compile(checkpointer=memory)  # 编译图并启用持久化\n",
    "\n",
    "# 显示图的流程图\n",
    "# 这有助于理解整个对话流程\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9",
   "metadata": {
    "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9"
   },
   "source": [
    "## 线程（Threads）\n",
    "\n",
    "检查点器在每一步都将状态保存为检查点。\n",
    "\n",
    "这些保存的检查点可以分组到一个对话的`线程`中。\n",
    "\n",
    "**线程概念类比：**\n",
    "- 想象 Slack 作为类比：不同的频道承载不同的对话\n",
    "- 线程就像 Slack 频道，捕获分组的状态集合（例如对话）\n",
    "\n",
    "**线程的作用：**\n",
    "- 隔离不同的对话会话\n",
    "- 允许同时进行多个独立的对话\n",
    "- 每个线程维护自己的状态和记忆\n",
    "\n",
    "**配置线程：**\n",
    "- 使用 `configurable` 参数设置线程 ID\n",
    "- 不同的线程 ID 对应不同的对话会话\n",
    "- 这样可以在同一个图中管理多个独立的对话\n",
    "\n",
    "![状态图](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbadf3b379c2ee621adfd1_chatbot-summarization1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
    "outputId": "184c1386-bed2-453e-a56a-0a82841e1200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 第一轮对话 ===\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，Lance！很高兴见到你。有什么我可以帮助你的吗？\n",
      "\n",
      "=== 第二轮对话 ===\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你告诉我你叫Lance。有什么我可以帮助你的吗？\n",
      "\n",
      "=== 第三轮对话 ===\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yo-Yo Ma是一位非常杰出的大提琴家，他的演奏深受许多人喜爱。他的音乐作品涵盖了广泛的风格和时期，从巴洛克到现代音乐都有涉猎。你有特别喜欢的Yo-Yo Ma的专辑或作品吗？\n"
     ]
    }
   ],
   "source": [
    "# 创建一个对话线程\n",
    "# 线程 ID 用于区分不同的对话会话\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 开始对话 - 第一轮\n",
    "print(\"=== 第一轮对话 ===\")\n",
    "input_message = HumanMessage(content=\"你好！我是Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 继续对话 - 第二轮\n",
    "print(\"\\n=== 第二轮对话 ===\")\n",
    "input_message = HumanMessage(content=\"我叫什么名字？\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 继续对话 - 第三轮\n",
    "print(\"\\n=== 第三轮对话 ===\")\n",
    "input_message = HumanMessage(content=\"我喜欢YoYoMA的古典音乐\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 注意：由于我们使用了相同的 config（相同的线程 ID），\n",
    "# 聊天机器人可以记住之前的对话内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2",
   "metadata": {
    "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2"
   },
   "source": [
    "现在，我们还没有生成状态摘要，因为我们仍然有 ≤ 6 条消息。\n",
    "\n",
    "这是在 `should_continue` 函数中设置的。\n",
    "\n",
    "```python\n",
    "# 如果消息数量超过 6 条，则生成对话摘要\n",
    "if len(messages) > 6:\n",
    "    return \"summarize_conversation\"\n",
    "```\n",
    "\n",
    "**当前状态分析：**\n",
    "- 目前有 6 条消息（3 轮对话，每轮 2 条消息）\n",
    "- 还没有达到触发摘要的阈值（> 6）\n",
    "- 因此对话直接结束，没有生成摘要\n",
    "\n",
    "**线程的作用：**\n",
    "- 由于我们使用了线程，可以继续对话\n",
    "- 每次调用都会从上次的状态继续\n",
    "- 这展示了记忆功能的重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
    "outputId": "138d7ee8-9f26-4e81-d71d-ff29296ac79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前摘要: ''\n",
      "说明: 由于消息数量 ≤ 6，还没有生成摘要\n"
     ]
    }
   ],
   "source": [
    "# 检查当前线程的状态摘要\n",
    "# 由于消息数量还没有超过阈值，摘要应该为空\n",
    "current_summary = graph.get_state(config).values.get(\"summary\", \"\")\n",
    "print(f\"当前摘要: '{current_summary}'\")\n",
    "print(\"说明: 由于消息数量 ≤ 6，还没有生成摘要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a93e9-f716-4980-8edf-94115017d865",
   "metadata": {
    "id": "068a93e9-f716-4980-8edf-94115017d865"
   },
   "source": [
    "带有线程 ID 的 `config` 允许我们从之前记录的状态继续！\n",
    "\n",
    "**线程记忆功能演示：**\n",
    "- 每次调用 `graph.invoke()` 时都使用相同的 `config`\n",
    "- 这确保了对话的连续性和上下文保持\n",
    "- 聊天机器人能够记住之前的所有对话内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
    "outputId": "d48baf28-45b8-491a-8dc4-458982a5f08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 第四轮对话（触发摘要） ===\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "孙颖莎是一位非常优秀的乒乓球运动员，她在国际比赛中取得了许多优异的成绩。不过，乒乓球世界排名会根据选手在各项赛事中的表现进行动态调整，因此她是否是当前的世界第一需要查看最新的国际乒联（ITTF）排名。无论如何，她的实力和表现都得到了广泛的认可。你喜欢她的比赛风格吗？\n"
     ]
    }
   ],
   "source": [
    "# 继续对话 - 第四轮（这将触发摘要生成）\n",
    "print(\"=== 第四轮对话（触发摘要） ===\")\n",
    "input_message = (HumanMessage(content=\"我喜欢孙颖莎，她不是乒乓球女单世界第一吗？\"))\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 注意：这一轮对话后，消息数量将超过 6 条\n",
    "# 系统会自动生成对话摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
    "outputId": "851714ce-7f06-468b-e438-0c6a86ddb51a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 生成的对话摘要 ===\n",
      "摘要内容: 在这段对话中，用户Lance介绍了自己，并表达了对Yo-Yo Ma的古典音乐和乒乓球运动员孙颖莎的喜爱。用户询问孙颖莎是否是乒乓球女单世界第一，助理解释了排名的动态性，并认可了孙颖莎的实力和表现。\n",
      "\n",
      "=== 摘要功能说明 ===\n",
      "1. 系统自动分析了整个对话历史\n",
      "2. 生成了包含关键信息的摘要\n",
      "3. 旧消息被清理，只保留最近的 2 条\n",
      "4. 摘要将在后续对话中提供上下文\n"
     ]
    }
   ],
   "source": [
    "# 检查摘要生成后的状态\n",
    "# 现在应该有了对话摘要\n",
    "final_summary = graph.get_state(config).values.get(\"summary\", \"\")\n",
    "print(\"=== 生成的对话摘要 ===\")\n",
    "print(f\"摘要内容: {final_summary}\")\n",
    "print(\"\\n=== 摘要功能说明 ===\")\n",
    "print(\"1. 系统自动分析了整个对话历史\")\n",
    "print(\"2. 生成了包含关键信息的摘要\")\n",
    "print(\"3. 旧消息被清理，只保留最近的 2 条\")\n",
    "print(\"4. 摘要将在后续对话中提供上下文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e",
   "metadata": {
    "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e"
   },
   "source": [
    "## LangSmith 追踪\n",
    "\n",
    "让我们查看追踪信息！\n",
    "\n",
    "**LangSmith 追踪功能：**\n",
    "- 可以查看完整的对话流程\n",
    "- 监控每个节点的执行情况\n",
    "- 分析 LLM 调用的性能和成本\n",
    "- 调试对话逻辑和状态变化\n",
    "\n",
    "**如何查看追踪：**\n",
    "1. 访问 LangSmith 网站\n",
    "2. 登录您的账户\n",
    "3. 查看 \"FlyAIBox\" 项目\n",
    "4. 找到最新的追踪记录\n",
    "5. 分析对话流程和性能指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e1501",
   "metadata": {
    "id": "c62e1501"
   },
   "source": [
    "## 系统架构总结\n",
    "\n",
    "### 核心组件\n",
    "\n",
    "1. **状态管理 (State)**\n",
    "   - `MessagesState`: 基础消息状态\n",
    "   - `summary`: 自定义摘要字段\n",
    "   - 支持消息的增删改查\n",
    "\n",
    "2. **节点功能 (Nodes)**\n",
    "   - `call_model`: 调用 LLM 生成回复\n",
    "   - `summarize_conversation`: 生成对话摘要\n",
    "   - `should_continue`: 条件判断逻辑\n",
    "\n",
    "3. **记忆系统 (Memory)**\n",
    "   - `MemorySaver`: 内存检查点器\n",
    "   - 线程隔离: 支持多对话并行\n",
    "   - 状态持久化: 支持中断恢复\n",
    "\n",
    "### 工作流程\n",
    "\n",
    "```\n",
    "用户输入 → call_model → should_continue →\n",
    "    ↓                    ↓\n",
    "   结束               summarize_conversation → 结束\n",
    "```\n",
    "\n",
    "### 关键特性\n",
    "\n",
    "- **智能摘要**: 自动压缩长对话历史\n",
    "- **记忆保持**: 支持长时间对话\n",
    "- **成本优化**: 减少 token 使用量\n",
    "- **状态管理**: 灵活的状态更新机制\n",
    "\n",
    "### 适用场景\n",
    "\n",
    "- 客服聊天机器人\n",
    "- 个人助手应用\n",
    "- 教育对话系统\n",
    "- 技术支持工具\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39bd23a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39bd23a4",
    "outputId": "9e47ee8e-577a-4569-d5c6-24ef80e9a9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 创建新对话线程演示 ===\n",
      "新线程中的对话：\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好！欢迎你！如果你有任何问题或需要帮助，请随时告诉我。有什么我可以为你做的吗？\n",
      "\n",
      "=== 线程隔离说明 ===\n",
      "1. 线程 1 和线程 2 是完全独立的对话\n",
      "2. 每个线程维护自己的状态和记忆\n",
      "3. 可以在同一个图中同时管理多个对话\n",
      "4. 这类似于 Slack 中的不同频道\n"
     ]
    }
   ],
   "source": [
    "# 演示如何创建新的对话线程\n",
    "print(\"=== 创建新对话线程演示 ===\")\n",
    "\n",
    "# 创建新的线程配置\n",
    "new_config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# 在新线程中开始对话\n",
    "print(\"新线程中的对话：\")\n",
    "input_message = HumanMessage(content=\"你好，我是新用户\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, new_config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "print(\"\\n=== 线程隔离说明 ===\")\n",
    "print(\"1. 线程 1 和线程 2 是完全独立的对话\")\n",
    "print(\"2. 每个线程维护自己的状态和记忆\")\n",
    "print(\"3. 可以在同一个图中同时管理多个对话\")\n",
    "print(\"4. 这类似于 Slack 中的不同频道\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
