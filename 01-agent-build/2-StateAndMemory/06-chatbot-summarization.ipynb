{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "### 🔧 环境配置和检查\n\n#### 概述\n\n本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n\n- 使用统一的conda环境：激活统一的学习环境\n- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n- 加速模型下载：设置HuggingFace镜像代理\n- 检查系统配置：检查硬件和软件配置\n\n#### 配置\n\n- **所需环境及其依赖已经部署好**\n- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n\n# 1. 激活 conda 环境 (仅对当前单元格有效)\neval \"$(conda shell.bash hook)\"\nconda activate flyai_agent_in_action\n\necho \"=========================================\"\necho \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\necho \"=========================================\"\n\n# 2. 检查当前激活的环境\nCURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n\nif [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n    echo \"\"\n    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\nelse\n    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n    echo \"\"\n    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n    echo \"\"\n    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n    echo \"\"\n    echo \"%%script bash\"\n    echo \"# 必须在每个单元格都执行\"\n    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n    echo \"conda activate flyai_agent_in_action\"\nfi\n\necho \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/langchain-academy/blob/fly101/module-2/chatbot-summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651ead9-5504-45ee-938d-f91ac78dddd1",
   "metadata": {
    "id": "b651ead9-5504-45ee-938d-f91ac78dddd1"
   },
   "source": [
    "# 带消息摘要功能的聊天机器人\n",
    "\n",
    "## 回顾\n",
    "\n",
    "我们已经学习了如何自定义图状态模式（graph state schema）和状态归约器（reducer）。\n",
    "\n",
    "我们也展示了多种在图状态中修剪或过滤消息的方法。\n",
    "\n",
    "## 目标\n",
    "\n",
    "现在，让我们更进一步！\n",
    "\n",
    "不仅仅是修剪或过滤消息，我们将展示如何使用大语言模型（LLM）来生成对话的持续摘要。\n",
    "\n",
    "这使我们能够保留完整对话的压缩表示，而不是简单地通过修剪或过滤来删除它们。\n",
    "\n",
    "我们将把这种摘要功能整合到一个简单的聊天机器人中。\n",
    "\n",
    "并且我们将为这个聊天机器人配备记忆功能，支持长时间运行的对话，而不会产生高昂的token成本或延迟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d",
   "metadata": {
    "id": "000a6daa-92ad-4e57-a060-d1c81176eb0d"
   },
   "outputs": [],
   "source": [
    "# 安装必要的依赖包\n",
    "# 使用 %%capture 来隐藏安装过程的输出信息\n",
    "%%capture --no-stderr\n",
    "# %pip install --quiet -U langchain_core langgraph langchain_openai\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09201a62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09201a62",
    "outputId": "181fdbf3-2fa5-42a5-d4de-ec9ab8e0a9ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "    如果环境变量不存在，则提示用户输入\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd",
   "metadata": {
    "id": "dfddfce9-3a9b-4b35-a76d-28265515aabd"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[追踪](https://docs.smith.langchain.com/concepts/tracing)。\n",
    "\n",
    "LangSmith 是 LangChain 提供的调试和监控工具，可以帮助我们：\n",
    "- 追踪 LLM 调用链\n",
    "- 监控性能和成本\n",
    "- 调试对话流程\n",
    "- 分析模型行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464856d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "464856d4",
    "outputId": "02083c1b-58d2-4ecf-9bfd-d176c5ed864a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY: ··········\n"
     ]
    }
   ],
   "source": [
    "# 设置 LangSmith 相关环境变量\n",
    "# LangSmith 用于追踪和调试 LangChain 应用\n",
    "_set_env(\"LANGSMITH_API_KEY\")  # LangSmith API 密钥\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # 启用追踪功能\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\"  # 设置项目名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392",
   "metadata": {
    "id": "537ade30-6a0e-4b6b-8bcd-ce90790b6392"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化 OpenAI 聊天模型\n",
    "# gpt-4o: 使用 GPT-4 Omni 模型，这是 OpenAI 最新的多模态模型\n",
    "# temperature=0: 设置温度为 0，使模型输出更加确定性和一致\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b",
   "metadata": {
    "id": "db3afac3-8b7a-45db-a3c1-7e4125c1bc8b"
   },
   "source": [
    "我们将使用 `MessagesState`，就像之前一样。\n",
    "\n",
    "除了内置的 `messages` 键之外，我们现在还将包含一个自定义键（`summary`）。\n",
    "\n",
    "**状态设计说明：**\n",
    "- `messages`: 存储对话历史消息列表\n",
    "- `summary`: 存储对话的摘要信息，用于压缩长对话历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948e60f0-5c76-4235-b40e-cf523205d40e",
   "metadata": {
    "id": "948e60f0-5c76-4235-b40e-cf523205d40e"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"\n",
    "    自定义状态类，继承自 MessagesState\n",
    "\n",
    "    继承的功能：\n",
    "    - messages: 消息列表，用于存储对话历史\n",
    "\n",
    "    新增功能：\n",
    "    - summary: 字符串类型，用于存储对话摘要\n",
    "    \"\"\"\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855ea31-5cc1-4277-a189-0b72459f67ec",
   "metadata": {
    "id": "6855ea31-5cc1-4277-a189-0b72459f67ec"
   },
   "source": [
    "我们将定义一个节点来调用我们的 LLM，如果存在摘要，则将其融入到提示中。\n",
    "\n",
    "**节点功能说明：**\n",
    "- 检查状态中是否存在摘要\n",
    "- 如果存在摘要，将其作为系统消息添加到对话中\n",
    "- 调用 LLM 生成回复\n",
    "- 返回 AI 的回复消息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b",
   "metadata": {
    "id": "c3f7d19b-afe0-4381-9b1a-0a832b162e7b"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "\n",
    "def call_model(state: State):\n",
    "    \"\"\"\n",
    "    调用 LLM 模型生成回复的节点函数\n",
    "\n",
    "    参数:\n",
    "        state (State): 当前图状态，包含消息和摘要信息\n",
    "\n",
    "    返回:\n",
    "        dict: 包含 AI 回复消息的字典\n",
    "\n",
    "    功能说明:\n",
    "        1. 检查状态中是否存在对话摘要\n",
    "        2. 如果存在摘要，将其作为系统消息添加到对话上下文中\n",
    "        3. 调用 LLM 模型生成回复\n",
    "        4. 返回包含 AI 回复的状态更新\n",
    "    \"\"\"\n",
    "\n",
    "    # 获取摘要信息，如果不存在则返回空字符串\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # 如果存在摘要，将其融入到提示中\n",
    "    if summary:\n",
    "        # 创建包含摘要的系统消息\n",
    "        # 这有助于 LLM 理解之前的对话上下文\n",
    "        # system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        system_message = f\"此前对话的摘要：{summary}\"\n",
    "\n",
    "        # 将系统消息添加到消息列表的开头\n",
    "        # 系统消息通常用于提供上下文和指令\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "\n",
    "    else:\n",
    "        # 如果没有摘要，直接使用原始消息\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    # 调用 LLM 模型生成回复\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 返回包含新消息的状态更新\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450",
   "metadata": {
    "id": "6882042c-b42d-4d52-a6a7-6ec8efa72450"
   },
   "source": [
    "我们将定义一个节点来生成摘要。\n",
    "\n",
    "**摘要节点功能：**\n",
    "- 分析当前对话历史\n",
    "- 使用 LLM 生成对话摘要\n",
    "- 清理旧消息，只保留最近的几条消息\n",
    "- 更新状态中的摘要信息\n",
    "\n",
    "**注意：** 这里我们将使用 `RemoveMessage` 在生成摘要后过滤我们的状态，这样可以：\n",
    "- 减少内存占用\n",
    "- 降低后续调用的 token 成本\n",
    "- 保持对话的连续性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e",
   "metadata": {
    "id": "78c7aa59-3760-4e76-93f1-bc713e3ec39e"
   },
   "outputs": [],
   "source": [
    "def summarize_conversation(state: State):\n",
    "    \"\"\"\n",
    "    生成对话摘要的节点函数\n",
    "\n",
    "    参数:\n",
    "        state (State): 当前图状态，包含消息和摘要信息\n",
    "\n",
    "    返回:\n",
    "        dict: 包含新摘要和消息删除指令的字典\n",
    "\n",
    "    功能说明:\n",
    "        1. 检查是否已存在摘要\n",
    "        2. 根据情况创建不同的摘要提示\n",
    "        3. 调用 LLM 生成或更新摘要\n",
    "        4. 删除旧消息，只保留最近的 2 条消息\n",
    "        5. 更新状态中的摘要信息\n",
    "    \"\"\"\n",
    "\n",
    "    # 首先获取任何现有的摘要\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # 创建摘要提示\n",
    "    # if summary:\n",
    "    #     # 如果摘要已存在，则扩展现有摘要\n",
    "    #     summary_message = (\n",
    "    #         f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "    #         \"Extend the summary by taking into account the new messages above:\"\n",
    "    #     )\n",
    "\n",
    "    # else:\n",
    "    #     # 如果没有现有摘要，则创建新摘要\n",
    "    #     summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    if summary:\n",
    "      # 如果摘要已存在，则扩展现有摘要\n",
    "      summary_message = (\n",
    "          f\"目前为止的对话摘要：{summary}\\n\\n\"\n",
    "          \"请结合上方的新消息，扩展现有摘要：\"\n",
    "      )\n",
    "\n",
    "    else:\n",
    "        # 如果没有现有摘要，则创建新摘要\n",
    "        summary_message = \"请对上方的对话创建摘要：\"\n",
    "\n",
    "    # 将摘要提示添加到消息历史中\n",
    "    # 这样 LLM 可以基于完整的对话历史生成摘要\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "\n",
    "    # 调用 LLM 生成摘要\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 删除除最近 2 条消息外的所有消息\n",
    "    # 这样可以减少内存占用和 token 成本\n",
    "    # RemoveMessage 用于标记消息为删除状态\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "\n",
    "    # 返回新的摘要和消息删除指令\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f982993e-f4be-4ff7-9a38-886f75398b3d",
   "metadata": {
    "id": "f982993e-f4be-4ff7-9a38-886f75398b3d"
   },
   "source": [
    "我们将添加一个条件边来决定是否基于对话长度生成摘要。\n",
    "\n",
    "**条件边的作用：**\n",
    "- 监控对话中的消息数量\n",
    "- 当消息数量超过阈值时，触发摘要生成\n",
    "- 否则直接结束对话\n",
    "- 这样可以平衡对话质量和性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b507665d-7f5d-442a-b498-218c94c5dd8b",
   "metadata": {
    "id": "b507665d-7f5d-442a-b498-218c94c5dd8b"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from typing_extensions import Literal\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"summarize_conversation\", END]:\n",
    "    \"\"\"\n",
    "    决定是否继续对话或生成摘要的条件函数\n",
    "\n",
    "    参数:\n",
    "        state (State): 当前图状态\n",
    "\n",
    "    返回:\n",
    "        Literal: 下一个要执行的节点名称或 END\n",
    "\n",
    "    功能说明:\n",
    "        根据对话中的消息数量决定下一步操作：\n",
    "        - 如果消息数量 > 6，则生成摘要\n",
    "        - 否则直接结束对话\n",
    "\n",
    "    设计理念:\n",
    "        这个阈值可以根据实际需求调整：\n",
    "        - 较小的阈值：更频繁的摘要，但可能丢失细节\n",
    "        - 较大的阈值：保留更多上下文，但可能增加成本\n",
    "    \"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 如果消息数量超过 6 条，则生成对话摘要\n",
    "    # 这个阈值可以根据实际需求调整\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "\n",
    "    # 否则直接结束对话\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd",
   "metadata": {
    "id": "5a838f4c-7067-4f7f-a4c4-6654e11214cd"
   },
   "source": [
    "## 添加记忆功能\n",
    "\n",
    "回顾一下，[状态在单次图执行中是瞬态的](https://github.com/langchain-ai/langgraph/discussions/352#discussioncomment-9291220)。\n",
    "\n",
    "这限制了我们在中断情况下进行多轮对话的能力。\n",
    "\n",
    "正如在模块 1 末尾介绍的那样，我们可以使用[持久化](https://langchain-ai.github.io/langgraph/how-tos/persistence/)来解决这个问题！\n",
    "\n",
    "**LangGraph 持久化机制：**\n",
    "- LangGraph 可以使用检查点器（checkpointer）在每一步后自动保存图状态\n",
    "- 这个内置的持久化层为我们提供了记忆功能\n",
    "- 允许 LangGraph 从最后一次状态更新处继续执行\n",
    "\n",
    "**MemorySaver 介绍：**\n",
    "- 我们之前展示的最容易使用的检查点器之一是 `MemorySaver`\n",
    "- 它是一个用于图状态的内存键值存储\n",
    "- 我们只需要在编译图时添加检查点器，图就具有了记忆功能！\n",
    "\n",
    "**记忆功能的好处：**\n",
    "- 支持长时间运行的对话\n",
    "- 可以在中断后恢复对话\n",
    "- 保持对话的连续性和上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "1d57516d-f9f1-4d3c-a84a-7277b5ce6df6",
    "outputId": "47e6692c-8686-4f20-939d-ad595e76a491"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "# 定义一个新的图工作流\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 添加节点\n",
    "# conversation: 处理用户输入并生成回复的节点\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "# summarize_conversation: 生成对话摘要的节点\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# 设置图的连接关系\n",
    "# 设置入口点为 conversation 节点\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "# 添加条件边：根据消息数量决定是否生成摘要\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "# 摘要生成完成后直接结束\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# 编译图并添加记忆功能\n",
    "memory = MemorySaver()  # 创建内存检查点器\n",
    "graph = workflow.compile(checkpointer=memory)  # 编译图并启用持久化\n",
    "\n",
    "# 显示图的流程图\n",
    "# 这有助于理解整个对话流程\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9",
   "metadata": {
    "id": "d0bd5d23-ac3b-4496-a049-9a9f97d2feb9"
   },
   "source": [
    "## 线程（Threads）\n",
    "\n",
    "检查点器在每一步都将状态保存为检查点。\n",
    "\n",
    "这些保存的检查点可以分组到一个对话的`线程`中。\n",
    "\n",
    "**线程概念类比：**\n",
    "- 想象 Slack 作为类比：不同的频道承载不同的对话\n",
    "- 线程就像 Slack 频道，捕获分组的状态集合（例如对话）\n",
    "\n",
    "**线程的作用：**\n",
    "- 隔离不同的对话会话\n",
    "- 允许同时进行多个独立的对话\n",
    "- 每个线程维护自己的状态和记忆\n",
    "\n",
    "**配置线程：**\n",
    "- 使用 `configurable` 参数设置线程 ID\n",
    "- 不同的线程 ID 对应不同的对话会话\n",
    "- 这样可以在同一个图中管理多个独立的对话\n",
    "\n",
    "![状态图](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbadf3b379c2ee621adfd1_chatbot-summarization1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2566c93b-13e6-4a53-bc0f-b00fff691d30",
    "outputId": "184c1386-bed2-453e-a56a-0a82841e1200"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to multipart ingest runs: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=0f6681b8-8a9c-48e7-adc4-568f01165179; trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=1144f291-f0c8-4dde-867a-126b834c03ff; trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=45bfc5ab-ce15-48e4-9471-725df12f20e3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 第一轮对话 ===\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，Lance！很高兴见到你。有什么我可以帮助你的吗？\n",
      "\n",
      "=== 第二轮对话 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=45bfc5ab-ce15-48e4-9471-725df12f20e3; trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=4befc9ad-0283-4843-9401-4186fa78fa89; trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=4befc9ad-0283-4843-9401-4186fa78fa89; trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=1144f291-f0c8-4dde-867a-126b834c03ff; trace=0f6681b8-8a9c-48e7-adc4-568f01165179,id=0f6681b8-8a9c-48e7-adc4-568f01165179; trace=799e270f-1684-47b6-beb5-7760b8445836,id=799e270f-1684-47b6-beb5-7760b8445836; trace=799e270f-1684-47b6-beb5-7760b8445836,id=189bba46-2290-4f0f-8864-248672114216; trace=799e270f-1684-47b6-beb5-7760b8445836,id=664d0deb-dbb4-43f2-9a6a-f30e5e582b9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你告诉我你叫Lance。有什么我可以帮助你的吗？\n",
      "\n",
      "=== 第三轮对话 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=799e270f-1684-47b6-beb5-7760b8445836,id=664d0deb-dbb4-43f2-9a6a-f30e5e582b9e; trace=799e270f-1684-47b6-beb5-7760b8445836,id=9dcc090f-7c9e-4e38-a40b-54545a6f10f0; trace=799e270f-1684-47b6-beb5-7760b8445836,id=9dcc090f-7c9e-4e38-a40b-54545a6f10f0; trace=799e270f-1684-47b6-beb5-7760b8445836,id=189bba46-2290-4f0f-8864-248672114216; trace=799e270f-1684-47b6-beb5-7760b8445836,id=799e270f-1684-47b6-beb5-7760b8445836; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=4d599692-dd6e-496a-9dbd-85eb57dc928d; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=d51a7315-216e-4cfa-aa8b-87c699aa4788; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=711bff83-f8da-4ac7-9f2a-6208d258907a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yo-Yo Ma是一位非常杰出的古典大提琴家，他的音乐作品深受许多人喜爱。他以其丰富的情感表达和精湛的技艺而闻名。你有特别喜欢他的哪一部作品或专辑吗？\n"
     ]
    }
   ],
   "source": [
    "# 创建一个对话线程\n",
    "# 线程 ID 用于区分不同的对话会话\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 开始对话 - 第一轮\n",
    "print(\"=== 第一轮对话 ===\")\n",
    "input_message = HumanMessage(content=\"你好！我是Lance\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 继续对话 - 第二轮\n",
    "print(\"\\n=== 第二轮对话 ===\")\n",
    "input_message = HumanMessage(content=\"我叫什么名字？\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 继续对话 - 第三轮\n",
    "print(\"\\n=== 第三轮对话 ===\")\n",
    "input_message = HumanMessage(content=\"我喜欢YoYoMA的古典音乐\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 注意：由于我们使用了相同的 config（相同的线程 ID），\n",
    "# 聊天机器人可以记住之前的对话内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2",
   "metadata": {
    "id": "531e5b63-5e8b-486e-baa0-a45521e2fbc2"
   },
   "source": [
    "现在，我们还没有生成状态摘要，因为我们仍然有 ≤ 6 条消息。\n",
    "\n",
    "这是在 `should_continue` 函数中设置的。\n",
    "\n",
    "```python\n",
    "# 如果消息数量超过 6 条，则生成对话摘要\n",
    "if len(messages) > 6:\n",
    "    return \"summarize_conversation\"\n",
    "```\n",
    "\n",
    "**当前状态分析：**\n",
    "- 目前有 6 条消息（3 轮对话，每轮 2 条消息）\n",
    "- 还没有达到触发摘要的阈值（> 6）\n",
    "- 因此对话直接结束，没有生成摘要\n",
    "\n",
    "**线程的作用：**\n",
    "- 由于我们使用了线程，可以继续对话\n",
    "- 每次调用都会从上次的状态继续\n",
    "- 这展示了记忆功能的重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91b82aaa-17f9-49e2-9528-f4b22e23ebcb",
    "outputId": "138d7ee8-9f26-4e81-d71d-ff29296ac79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前摘要: '在这段对话中，用户自我介绍为Lance，并表示喜欢Yo-Yo Ma的古典音乐和乒乓球运动员孙颖莎。用户提到孙颖莎是否是乒乓球女单世界第一，并对她的表现表示认可。'\n",
      "说明: 由于消息数量 ≤ 6，还没有生成摘要\n"
     ]
    }
   ],
   "source": [
    "# 检查当前线程的状态摘要\n",
    "# 由于消息数量还没有超过阈值，摘要应该为空\n",
    "current_summary = graph.get_state(config).values.get(\"summary\", \"\")\n",
    "print(f\"当前摘要: '{current_summary}'\")\n",
    "print(\"说明: 由于消息数量 ≤ 6，还没有生成摘要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a93e9-f716-4980-8edf-94115017d865",
   "metadata": {
    "id": "068a93e9-f716-4980-8edf-94115017d865"
   },
   "source": [
    "带有线程 ID 的 `config` 允许我们从之前记录的状态继续！\n",
    "\n",
    "**线程记忆功能演示：**\n",
    "- 每次调用 `graph.invoke()` 时都使用相同的 `config`\n",
    "- 这确保了对话的连续性和上下文保持\n",
    "- 聊天机器人能够记住之前的所有对话内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24b34f0f-62ef-4008-8e96-480cbe92ea3e",
    "outputId": "d48baf28-45b8-491a-8dc4-458982a5f08a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 第四轮对话（触发摘要） ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=711bff83-f8da-4ac7-9f2a-6208d258907a; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=13b12a6c-3b7a-4ec5-b717-2a09cdb4824a; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=13b12a6c-3b7a-4ec5-b717-2a09cdb4824a; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=d51a7315-216e-4cfa-aa8b-87c699aa4788; trace=4d599692-dd6e-496a-9dbd-85eb57dc928d,id=4d599692-dd6e-496a-9dbd-85eb57dc928d; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=43c4901b-e921-483a-8945-6afa9f7a9881; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=7a86b03e-bb64-4cfc-bc9b-a822eb3debe0; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=79feaaa0-295e-49df-a148-c059954740c9\n",
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=79feaaa0-295e-49df-a148-c059954740c9; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=51c36359-94d2-4d65-9f66-7c16206574c5; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=51c36359-94d2-4d65-9f66-7c16206574c5; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=7a86b03e-bb64-4cfc-bc9b-a822eb3debe0; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=752d19e3-4fd7-4554-8691-03cae7b7246e; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=4bc42940-0d84-4791-8584-a41ccc88c3ee\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "孙颖莎是一位非常优秀的乒乓球运动员，她在国际比赛中取得了许多优异的成绩。不过，乒乓球世界排名会根据选手在各项赛事中的表现进行动态调整，因此她是否是当前的世界第一需要查看最新的排名更新。无论如何，她的实力和表现都得到了广泛的认可。你喜欢她的比赛风格吗？\n"
     ]
    }
   ],
   "source": [
    "# 继续对话 - 第四轮（这将触发摘要生成）\n",
    "print(\"=== 第四轮对话（触发摘要） ===\")\n",
    "input_message = (HumanMessage(content=\"我喜欢孙颖莎，她不是乒乓球女单世界第一吗？\"))\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# 注意：这一轮对话后，消息数量将超过 6 条\n",
    "# 系统会自动生成对话摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22f1b35f-e4bb-47f6-87b1-d84d8aed9aa9",
    "outputId": "851714ce-7f06-468b-e438-0c6a86ddb51a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 生成的对话摘要 ===\n",
      "摘要内容: 在这段对话中，用户自我介绍为Lance，并表示喜欢Yo-Yo Ma的古典音乐和乒乓球运动员孙颖莎。用户提到孙颖莎是否是乒乓球女单世界第一，并对她的表现表示认可。\n",
      "\n",
      "=== 摘要功能说明 ===\n",
      "1. 系统自动分析了整个对话历史\n",
      "2. 生成了包含关键信息的摘要\n",
      "3. 旧消息被清理，只保留最近的 2 条\n",
      "4. 摘要将在后续对话中提供上下文\n"
     ]
    }
   ],
   "source": [
    "# 检查摘要生成后的状态\n",
    "# 现在应该有了对话摘要\n",
    "final_summary = graph.get_state(config).values.get(\"summary\", \"\")\n",
    "print(\"=== 生成的对话摘要 ===\")\n",
    "print(f\"摘要内容: {final_summary}\")\n",
    "print(\"\\n=== 摘要功能说明 ===\")\n",
    "print(\"1. 系统自动分析了整个对话历史\")\n",
    "print(\"2. 生成了包含关键信息的摘要\")\n",
    "print(\"3. 旧消息被清理，只保留最近的 2 条\")\n",
    "print(\"4. 摘要将在后续对话中提供上下文\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e",
   "metadata": {
    "id": "ad7cc0ab-905a-4037-b7cb-69db5b89591e"
   },
   "source": [
    "## LangSmith 追踪\n",
    "\n",
    "让我们查看追踪信息！\n",
    "\n",
    "**LangSmith 追踪功能：**\n",
    "- 可以查看完整的对话流程\n",
    "- 监控每个节点的执行情况\n",
    "- 分析 LLM 调用的性能和成本\n",
    "- 调试对话逻辑和状态变化\n",
    "\n",
    "**如何查看追踪：**\n",
    "1. 访问 LangSmith 网站\n",
    "2. 登录您的账户\n",
    "3. 查看 \"FlyAIBox\" 项目\n",
    "4. 找到最新的追踪记录\n",
    "5. 分析对话流程和性能指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62e1501",
   "metadata": {
    "id": "c62e1501"
   },
   "source": [
    "## 系统架构总结\n",
    "\n",
    "### 核心组件\n",
    "\n",
    "1. **状态管理 (State)**\n",
    "   - `MessagesState`: 基础消息状态\n",
    "   - `summary`: 自定义摘要字段\n",
    "   - 支持消息的增删改查\n",
    "\n",
    "2. **节点功能 (Nodes)**\n",
    "   - `call_model`: 调用 LLM 生成回复\n",
    "   - `summarize_conversation`: 生成对话摘要\n",
    "   - `should_continue`: 条件判断逻辑\n",
    "\n",
    "3. **记忆系统 (Memory)**\n",
    "   - `MemorySaver`: 内存检查点器\n",
    "   - 线程隔离: 支持多对话并行\n",
    "   - 状态持久化: 支持中断恢复\n",
    "\n",
    "### 工作流程\n",
    "\n",
    "```\n",
    "用户输入 → call_model → should_continue →\n",
    "    ↓                    ↓\n",
    "   结束               summarize_conversation → 结束\n",
    "```\n",
    "\n",
    "### 关键特性\n",
    "\n",
    "- **智能摘要**: 自动压缩长对话历史\n",
    "- **记忆保持**: 支持长时间对话\n",
    "- **成本优化**: 减少 token 使用量\n",
    "- **状态管理**: 灵活的状态更新机制\n",
    "\n",
    "### 适用场景\n",
    "\n",
    "- 客服聊天机器人\n",
    "- 个人助手应用\n",
    "- 教育对话系统\n",
    "- 技术支持工具\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bd23a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39bd23a4",
    "outputId": "9e47ee8e-577a-4569-d5c6-24ef80e9a9b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 创建新对话线程演示 ===\n",
      "新线程中的对话：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=4bc42940-0d84-4791-8584-a41ccc88c3ee; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=752d19e3-4fd7-4554-8691-03cae7b7246e; trace=43c4901b-e921-483a-8945-6afa9f7a9881,id=43c4901b-e921-483a-8945-6afa9f7a9881; trace=f9568fda-d71b-4c4b-9c05-7932e606a54a,id=f9568fda-d71b-4c4b-9c05-7932e606a54a; trace=f9568fda-d71b-4c4b-9c05-7932e606a54a,id=61b00404-5712-44eb-bdcf-0e5ad14b1e6b; trace=f9568fda-d71b-4c4b-9c05-7932e606a54a,id=26d4170c-5b6f-4796-b365-35979b9fa13f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好！欢迎你！如果你有任何问题或者需要帮助，请随时告诉我。有什么我可以为你做的吗？\n",
      "\n",
      "=== 线程隔离说明 ===\n",
      "1. 线程 1 和线程 2 是完全独立的对话\n",
      "2. 每个线程维护自己的状态和记忆\n",
      "3. 可以在同一个图中同时管理多个对话\n",
      "4. 这类似于 Slack 中的不同频道\n"
     ]
    }
   ],
   "source": [
    "# 演示如何创建新的对话线程\n",
    "print(\"=== 创建新对话线程演示 ===\")\n",
    "\n",
    "# 创建新的线程配置\n",
    "new_config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# 在新线程中开始对话\n",
    "print(\"新线程中的对话：\")\n",
    "input_message = HumanMessage(content=\"你好，我是新用户\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, new_config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "\n",
    "print(\"\\n=== 线程隔离说明 ===\")\n",
    "print(\"1. 线程 1 和线程 2 是完全独立的对话\")\n",
    "print(\"2. 每个线程维护自己的状态和记忆\")\n",
    "print(\"3. 可以在同一个图中同时管理多个对话\")\n",
    "print(\"4. 这类似于 Slack 中的不同频道\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}