{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/langchain-academy/blob/fly101/module-4/map-reduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36737349-c949-4d64-9aa3-3767cbd02ad1",
   "metadata": {
    "id": "36737349-c949-4d64-9aa3-3767cbd02ad1"
   },
   "source": [
    "# Map-Reduce（映射-归约）\n",
    "\n",
    "## 回顾（Review）\n",
    "\n",
    "我们正在逐步构建一个多智能体（multi-agent）研究助理，它会把本课程中的各个模块串联起来。\n",
    "\n",
    "为构建这个多智能体助理，我们已经介绍了一些 LangGraph 的“可控性”主题。\n",
    "\n",
    "上一节我们学习了并行化（parallelization）和子图（sub-graphs）。\n",
    "\n",
    "## 目标（Goals）\n",
    "\n",
    "本节我们将学习 [Map-Reduce](https://langchain-ai.github.io/langgraph/how-tos/map-reduce/) 模式。\n",
    "---\n",
    "LangGraph 中的 **Map-Reduce** 是一种常见且强大的编程模式，用于处理并行任务。它在 LangGraph 的上下文中通常指的是创建一个能够并行执行多个子任务的节点，然后将这些子任务的结果收集（或“合并”）起来，以产生一个最终结果。\n",
    "\n",
    "这个模式在 LangGraph 中没有一个专门的、内置的“Map-Reduce”节点类型。相反，它是通过构建一个特定的图结构来实现的，这个图通常包含：\n",
    "\n",
    "1.  **Map 阶段（并行处理）**：一个或多个节点被设计为接收一个输入列表，并为列表中的每个元素启动一个独立的、并行的执行流（即子图或子任务）。\n",
    "2.  **Reduce 阶段（结果合并）**：一个**Join**节点或一个自定义的聚合节点，其作用是等待所有并行的执行流完成，然后将它们各自的输出收集起来，进行合并、汇总或总结。\n",
    "\n",
    "### 为什么在 LangGraph 中使用 Map-Reduce？\n",
    "\n",
    "  * **提高效率**：当你有多个独立但需要执行相同或相似任务的数据块时，Map-Reduce 模式能让你同时处理它们，而不是按顺序执行，从而大大减少总处理时间。例如，你可以同时对 100 个文档进行摘要提取，而不是一个一个来。\n",
    "  * **处理复杂任务**：这个模式非常适合那些可以将大问题分解为多个小问题（Map），然后将小问题的答案组合成最终答案（Reduce）的场景。\n",
    "  * **灵活性**：你可以使用 LangGraph 的**Join**节点（这是一个内置功能，用来等待所有传入的边都到达后才执行）来轻松实现 Reduce 阶段。你也可以编写一个自定义的函数来作为 Reduce 节点，以实现任何你需要的聚合逻辑，比如求和、平均、拼接字符串等。\n",
    "\n",
    "-----\n",
    "\n",
    "### 一个简单的 Map-Reduce 示例\n",
    "\n",
    "假设你有一个任务：对一个包含多个 URL 的列表进行内容抓取和摘要。\n",
    "\n",
    "**图的结构可能如下：**\n",
    "\n",
    "```\n",
    "                [Start]\n",
    "                   |\n",
    "             (输入URL列表)\n",
    "                   |\n",
    "                   ▼\n",
    "       +-----------------------+\n",
    "       |   [并行处理节点]      |  (Map 阶段)\n",
    "       | (为每个URL启动一个子任务) |\n",
    "       +-----------------------+\n",
    "          /     |     \\\n",
    "         /      |      \\\n",
    "       ▼        ▼        ▼\n",
    "[提取URL_1]  [提取URL_2] ...  [提取URL_N]  (每个任务都并行执行)\n",
    "         \\      |      /\n",
    "          \\     |     /\n",
    "       +-----------------------+\n",
    "       |   [Join 节点]         |  (Reduce 阶段)\n",
    "       | (等待所有任务完成)      |\n",
    "       +-----------------------+\n",
    "                   |\n",
    "                   ▼\n",
    "       +-----------------------+\n",
    "       |   [聚合节点]          |  (将所有摘要合并)\n",
    "       +-----------------------+\n",
    "                   |\n",
    "                   ▼\n",
    "                 [End]\n",
    "```\n",
    "\n",
    "在这个例子中：\n",
    "\n",
    "1.  **Map**：你有一个并行处理节点，它接收 URL 列表。它为每个 URL 分别启动一个子图执行，每个子图负责抓取一个网页并生成摘要。\n",
    "2.  **Reduce**：一个 `Join` 节点会等待所有并行子图执行完毕。一旦所有子图都返回了结果，这个 `Join` 节点就会将这些结果传递给下一个节点。\n",
    "3.  **最终处理**：一个最后的节点接收所有返回的摘要，并将它们拼接成一个完整的文档或报告。\n",
    "\n",
    "虽然 LangGraph 没有一个叫作 `Map-Reduce` 的命令，但它提供的工具（如 `Join` 节点和创建并行子图的能力）使实现这种模式变得非常直接和高效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24e95c8",
   "metadata": {
    "id": "f24e95c8"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install -U langchain_openai langgraph\n",
    "%pip install --quiet langgraph==0.6.7 langchain_openai==0.3.32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff57cbf7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff57cbf7",
    "outputId": "0798a212-1480-4803-91da-c256bc8ecddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd868a",
   "metadata": {
    "id": "cbcd868a"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[链路追踪（tracing）](https://docs.smith.langchain.com/concepts/tracing)。这可以帮助初学者观察每一步调用与中间结果，便于调试与理解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdc647f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fdc647f",
    "outputId": "2e65c5b6-6a7a-4c52-c0a3-672adb1864d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY: ··········\n"
     ]
    }
   ],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe9b9f-4375-4bca-8e32-7d57cb861469",
   "metadata": {
    "id": "2bbe9b9f-4375-4bca-8e32-7d57cb861469"
   },
   "source": [
    "## 问题（Problem）\n",
    "\n",
    "Map-Reduce 是一种高效的任务拆解与并行处理模式，通常分为两个阶段：\n",
    "\n",
    "1) `Map`（映射）：把一个大任务拆分为多个可并行处理的小任务。\n",
    "\n",
    "2) `Reduce`（归约）：汇总所有并行子任务的结果，并得到最终输出。\n",
    "\n",
    "我们来设计一个简单系统来演示这个思想，做两件事：\n",
    "\n",
    "1) `Map`：围绕某个主题生成一组笑话（每个子主题各出一个）。\n",
    "\n",
    "2) `Reduce`：从生成的笑话中挑选出最好的一条。\n",
    "\n",
    "我们将使用大语言模型（LLM）来完成“生成候选（Map）”与“选择最佳（Reduce）”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994cf903-1ed6-4ae2-b32a-7891a2808f81",
   "metadata": {
    "id": "994cf903-1ed6-4ae2-b32a-7891a2808f81"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 将使用的提示词（Prompts）：\n",
    "# - 生成与主题相关的 3 个子主题\n",
    "subjects_prompt = \"\"\"生成与这个总体主题相关的3个子主题列表: {topic}.\"\"\"\n",
    "# - 针对某个子主题生成一个笑话\n",
    "joke_prompt = \"\"\"生成一个的笑话，关于 {subject}\"\"\"\n",
    "# - 在一组笑话中选择最佳的一条，并返回其索引（从 0 开始计数）\n",
    "best_joke_prompt = \"\"\"以下是一些关于{topic}的笑话。请选择最好笑的一个！返回最佳笑话的ID，第一个笑话的ID从0开始。笑话: \\n\\n  {jokes}\"\"\"\n",
    "\n",
    "# LLM（使用确定性温度）\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b883cc-3469-4e96-b1a4-deadf7bf3ce5",
   "metadata": {
    "id": "f3b883cc-3469-4e96-b1a4-deadf7bf3ce5"
   },
   "source": [
    "## 状态（State）\n",
    "\n",
    "### 并行生成笑话\n",
    "\n",
    "我们先定义图（graph）的入口节点，它将会：\n",
    "\n",
    "- 接收用户输入的主题（topic）\n",
    "- 基于主题生成一组“子主题”（用于分别写笑话）\n",
    "- 把每个子主题发送到笑话生成节点（并行执行）\n",
    "\n",
    "我们的有向状态中包含一个 `jokes` 键，用来累积（append）并行生成的笑话列表。注意这里我们为 `jokes` 配置了“列表合并（list reduce）”的归约器，用于把多个并行结果合并到一个列表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099218ca-ee78-4291-95a1-87ee61382e3b",
   "metadata": {
    "id": "099218ca-ee78-4291-95a1-87ee61382e3b"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Subjects 类定义了从总体主题生成子主题的结构。\n",
    "# 它继承自 BaseModel，用于数据验证和结构化输出。\n",
    "class Subjects(BaseModel):\n",
    "    # subjects 字段是一个字符串列表，用于存储生成的子主题。\n",
    "    subjects: list[str]\n",
    "\n",
    "# BestJoke 类定义了从笑话列表中选择最佳笑话的结构。\n",
    "# 它继承自 BaseModel，用于数据验证和结构化输出。\n",
    "class BestJoke(BaseModel):\n",
    "    # id 字段是一个整数，表示在笑话列表中的索引（从 0 开始）。\n",
    "    id: int\n",
    "\n",
    "# OverallState 类定义了整个 Map-Reduce 图的总体状态。\n",
    "# 它继承自 TypedDict，用于提供类型提示。\n",
    "class OverallState(TypedDict):\n",
    "    # topic 字段是一个字符串，存储用户输入的总体主题。\n",
    "    topic: str\n",
    "    # subjects 字段是一个列表，存储从总体主题生成的子主题。\n",
    "    subjects: list\n",
    "    # jokes 字段是一个列表，使用 Annotated 结合 operator.add\n",
    "    # 定义了如何合并并行生成的笑话。operator.add 表示将新的笑话添加到列表中。\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    # best_selected_joke 字段是一个字符串，存储最终选出的最佳笑话。\n",
    "    best_selected_joke: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7176d1c-4a88-4b0f-a960-ee04a45279bd",
   "metadata": {
    "id": "c7176d1c-4a88-4b0f-a960-ee04a45279bd"
   },
   "source": [
    "为笑话生成子主题（subjects）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45010efd-ad31-4daa-b77e-aaec79ef0309",
   "metadata": {
    "id": "45010efd-ad31-4daa-b77e-aaec79ef0309"
   },
   "outputs": [],
   "source": [
    "def generate_topics(state: OverallState):\n",
    "    # 根据总体主题格式化提示词\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    # 使用带有结构化输出的模型调用LLM，期望返回Subjects类的结构\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    # 从LLM的响应中提取子主题列表，并作为状态更新返回\n",
    "    return {\"subjects\": response.subjects}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5296bb0-c163-4e5c-8181-1e305b37442a",
   "metadata": {
    "id": "e5296bb0-c163-4e5c-8181-1e305b37442a"
   },
   "source": [
    "关键点在这里：我们使用 [Send](https://langchain-ai.github.io/langgraph/concepts/low_level/#send) 为“每一个子主题”分别创建一次笑话生成任务。\n",
    "\n",
    "这非常实用！无论有多少子主题，都会自动并行地触发笑话生成。\n",
    "\n",
    "- `generate_joke`：图中的节点名称\n",
    "- `{\"subject\": s}`：发送给该节点的“局部状态”\n",
    "\n",
    "注意：`Send` 允许我们向目标节点传入“它自己的局部状态”，不需要与全局的 `OverallState` 一一对应。\n",
    "\n",
    "在这里，`generate_joke` 使用的是自己的内部状态结构（只需要 `subject`），我们通过 `Send` 来填充并传递它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc83e575-11f6-41a9-990a-adb571bcda06",
   "metadata": {
    "id": "bc83e575-11f6-41a9-990a-adb571bcda06"
   },
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    # 遍历状态中的每个子主题（subjects）\n",
    "    # 为每个子主题创建一个 Send 对象\n",
    "    # Send 对象指示图将执行 \"generate_joke\" 节点\n",
    "    # 并将一个包含当前子主题的局部状态 {\"subject\": s} 传递给它\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847192d-d358-411e-90c0-f06be0738717",
   "metadata": {
    "id": "9847192d-d358-411e-90c0-f06be0738717"
   },
   "source": [
    "### 笑话生成（Map）\n",
    "\n",
    "现在我们来定义一个“生成笑话”的节点 `generate_joke`。\n",
    "\n",
    "该节点会把生成结果写回全局状态的 `jokes` 列表。\n",
    "\n",
    "由于 `jokes` 使用了“列表合并”的归约器（reducer），多个并行生成的结果会自动合并为一个列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcddc567-73d3-4fb3-bfc5-1bea538f2aab",
   "metadata": {
    "id": "bcddc567-73d3-4fb3-bfc5-1bea538f2aab"
   },
   "outputs": [],
   "source": [
    "# JokeState 类定义了生成笑话节点（Map 阶段）的输入状态结构。\n",
    "# 它继承自 TypedDict，用于提供类型提示。\n",
    "class JokeState(TypedDict):\n",
    "    # subject 字段是一个字符串，存储当前需要生成笑话的子主题。\n",
    "    subject: str\n",
    "\n",
    "# Joke 类定义了生成笑话节点（Map 阶段）的输出结构。\n",
    "# 它继承自 BaseModel，用于数据验证和结构化输出。\n",
    "class Joke(BaseModel):\n",
    "    # joke 字段是一个字符串，存储生成的笑话文本。\n",
    "    joke: str\n",
    "\n",
    "# generate_joke 函数是 Map 阶段的核心节点，负责根据子主题生成笑话。\n",
    "def generate_joke(state: JokeState):\n",
    "    # 根据输入的子主题格式化笑话提示词。\n",
    "    prompt = joke_prompt.format(subject=state[\"subject\"])\n",
    "    # 使用带有结构化输出的模型调用LLM，期望返回 Joke 类的结构。\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    # 返回一个字典，其中 'jokes' 键对应一个包含生成的笑话的列表。\n",
    "    # 注意这里返回的是一个列表，因为 OverallState 的 jokes 字段配置了列表合并，\n",
    "    # 这样多个并行生成的笑话会自动合并到一个列表中。\n",
    "    return {\"jokes\": [response.joke]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02960657-d174-4076-99a8-b3f9eea015f4",
   "metadata": {
    "id": "02960657-d174-4076-99a8-b3f9eea015f4"
   },
   "source": [
    "### 最佳笑话选择（Reduce）\n",
    "\n",
    "接下来添加一个“选择最佳笑话”的节点，根据所有候选笑话，挑出得分最高或最合理的一条。这里我们直接让 LLM 返回最佳笑话的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d672870-75e3-4307-bda0-c41a86cbbaff",
   "metadata": {
    "id": "8d672870-75e3-4307-bda0-c41a86cbbaff"
   },
   "outputs": [],
   "source": [
    "def best_joke(state: OverallState):\n",
    "    # 将状态中累积的笑话列表用双换行符连接成一个字符串，方便作为整体输入给LLM。\n",
    "    jokes = \"\\n\\n\".join(state[\"jokes\"])\n",
    "    # 根据总体主题和所有笑话格式化选择最佳笑话的提示词。\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "    # 使用带有结构化输出的模型调用LLM，期望返回 BestJoke 类的结构，其中包含最佳笑话的索引。\n",
    "    response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    # 根据LLM返回的最佳笑话索引，从原始笑话列表中提取出最佳笑话的文本，并作为状态更新返回。\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][response.id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cd12e-5bff-426e-97f4-c774df998cfb",
   "metadata": {
    "id": "837cd12e-5bff-426e-97f4-c774df998cfb"
   },
   "source": [
    "## 编译并可视化（Compile）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ae6be4b-144e-483c-88ad-ce86d6477a0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "2ae6be4b-144e-483c-88ad-ce86d6477a0d",
    "outputId": "7383bd27-d04d-4c83-e66d-dcc2761ebc11"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "# 构建图：把上述各个节点与边组合成完整图\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_topics\", generate_topics)\n",
    "# 并行生成笑话的节点\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "# 选择最佳笑话的节点\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "# 起始 -> 生成子主题\n",
    "graph.add_edge(START, \"generate_topics\")\n",
    "# 条件边：为每个子主题发送到 generate_joke，并行执行\n",
    "graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\n",
    "# Map -> Reduce\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "# 结束\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "\n",
    "# 编译图\n",
    "app = graph.compile()\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e21dc7c9-0add-4125-be76-af701adb874a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e21dc7c9-0add-4125-be76-af701adb874a",
    "outputId": "8b519011-e78f-47b6-8c99-eb0a8b13c7b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=5aa13876-fa33-4532-9bde-ab0fee1d29b5; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=87ea50be-7278-4960-9cfe-608e86690a80; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=f25bbe34-6644-4788-b503-c094f704f678; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=259bca8a-ad2e-4b40-8fc0-4d11b5a1cf47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_topics': {'subjects': ['小花猫的生活习性', '小花猫的健康与护理', '小花猫的品种与特征']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=259bca8a-ad2e-4b40-8fc0-4d11b5a1cf47; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=b75f9619-4d11-43f1-942a-f85f0faf7796; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=b75f9619-4d11-43f1-942a-f85f0faf7796; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=f25bbe34-6644-4788-b503-c094f704f678; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=7e091c13-2e23-40de-80ac-87f7f7929555; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=7e091c13-2e23-40de-80ac-87f7f7929555; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=87ea50be-7278-4960-9cfe-608e86690a80; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=542bd8ad-ee0b-4309-8dba-f1fb904f0c81; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=5a3a261a-9e3e-49cc-a9f5-0900f6e7f303; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=d6110bc9-cb0f-429b-b5fb-064c758ae892; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=9e93b2f8-746f-45a0-b054-520bbf0e325e; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=b43ef517-acf7-4e90-8d41-3dd29883b1a2; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=c4b7adff-c718-4c97-8b41-b4cb400200e7; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=d38f5513-e143-42cb-8484-461cff8f34e5; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=7eb5d454-ec24-40e5-a9f7-283648fb24a9; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=16046e75-0c9c-4fd4-86a8-948b9d9aa115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_joke': {'jokes': ['为什么小花猫总是拒绝去看兽医？\\n\\n因为它听说医生会给它开\"猫\"药！😸']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=c4b7adff-c718-4c97-8b41-b4cb400200e7; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=90b03047-ca68-4abe-908f-c2675ba6ae87; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=90b03047-ca68-4abe-908f-c2675ba6ae87; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=b43ef517-acf7-4e90-8d41-3dd29883b1a2; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=9e93b2f8-746f-45a0-b054-520bbf0e325e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_joke': {'jokes': ['**小花猫的生活习性笑话**\\n\\n一天，小花猫和它的朋友小黑猫在阳光下懒洋洋地躺着。\\n\\n小黑猫问：“小花猫，你为什么总是喜欢在阳光下打盹呢？”\\n\\n小花猫懒懒地回答：“因为我在做光合作用啊！”\\n\\n小黑猫惊讶地说：“光合作用？你是植物吗？”\\n\\n小花猫得意地笑了：“当然不是，但我在吸收阳光，储存能量，晚上就能在家里跑酷了！”\\n\\n小黑猫无奈地摇摇头：“看来你真是个‘光’明正大的懒猫！”\\n\\n小花猫眯着眼睛，继续享受着阳光，心里想着：\"谁说猫不能有点植物的梦想呢？\" 😸🌞']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=d6110bc9-cb0f-429b-b5fb-064c758ae892; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=32454117-603d-4c03-a864-9cee04de7ebf; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=32454117-603d-4c03-a864-9cee04de7ebf; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=5a3a261a-9e3e-49cc-a9f5-0900f6e7f303; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=542bd8ad-ee0b-4309-8dba-f1fb904f0c81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_joke': {'jokes': ['**喵笑段子：高贵的喵种与毛色之谜🐱**\\n\\n在小镇的喵升级大会上，各种猫咪代表都前来展示各自的优越品种和特点。\\n\\n小花猫走上台，自豪地说：“我是波斯猫家族的后裔，毛发柔顺如丝，气质高贵至极。”\\n\\n白猫站在旁边，摇着头，满脸疑惑：“真的？你看上去颜色还挺丰富呢。”\\n\\n小花猫神秘一笑：“我只不过是披着一身花毛的波斯猫罢了。用彩色来增加神秘感，让人猜我到底是哪一种高贵的猫。”\\n\\n灰猫听后，神秘兮兮地附和：“原来如此，我也是波斯猫披着灰毛，只是为了在夜里更好藏身，顺便给家里的沙发来点惊喜脱毛艺术。”\\n\\n白猫睁大了眼睛：“那我是不是也披了毛色，才显得如此纯白？”\\n\\n小花猫笑着回答：“不，你是披着勇气和单纯的幸福，纯白如雪，让人忍不住想拍拍。”\\n\\n此刻，一旁的黑猫抱怨道：“所以，我的黑色是披着夜影催眠术？为什么大家都以为我在静悄悄打算些什么？”\\n\\n小花猫和白猫一同点头：“也许是，你在酝酿一场梦中探险呢！”\\n\\n整个大会现场，猫咪们一起笑成了一团，互相拍打着毛发，享受着自嘲和各自身上的特点。原来，最重要的不是披了什么毛，而是毛下那颗跳动的小小心脏，满载着机智和快乐。😺🤣']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langsmith.client:Failed to send compressed multipart ingest: langsmith.utils.LangSmithRateLimitError: Rate limit exceeded for https://api.smith.langchain.com/runs/multipart. HTTPError('429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Too many requests: tenant exceeded usage limits: user-defined usage limit monthly_traces of 10 exceeded. Check LangSmith usage configuration settings\"}\\n')trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=16046e75-0c9c-4fd4-86a8-948b9d9aa115; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=381b6641-f36b-4da5-902a-b8ee9f051f21; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=381b6641-f36b-4da5-902a-b8ee9f051f21; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=7eb5d454-ec24-40e5-a9f7-283648fb24a9; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=d38f5513-e143-42cb-8484-461cff8f34e5; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=ab2d1e02-5dab-4a75-903c-6a8f33c6f99d; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=6e8b3b40-fe83-4c2e-bb70-bb6f0f1c0533; trace=5aa13876-fa33-4532-9bde-ab0fee1d29b5,id=4d9e8376-8f37-45aa-abf6-01628469d855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'best_joke': {'best_selected_joke': '**小花猫的生活习性笑话**\\n\\n一天，小花猫和它的朋友小黑猫在阳光下懒洋洋地躺着。\\n\\n小黑猫问：“小花猫，你为什么总是喜欢在阳光下打盹呢？”\\n\\n小花猫懒懒地回答：“因为我在做光合作用啊！”\\n\\n小黑猫惊讶地说：“光合作用？你是植物吗？”\\n\\n小花猫得意地笑了：“当然不是，但我在吸收阳光，储存能量，晚上就能在家里跑酷了！”\\n\\n小黑猫无奈地摇摇头：“看来你真是个‘光’明正大的懒猫！”\\n\\n小花猫眯着眼睛，继续享受着阳光，心里想着：\"谁说猫不能有点植物的梦想呢？\" 😸🌞'}}\n"
     ]
    }
   ],
   "source": [
    "# 调用图：以输入主题触发生成与选择流程\n",
    "for s in app.stream({\"topic\": \"小花猫\"}):\n",
    "    print(s)  # 逐步输出每一步状态，便于初学者观察执行顺序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96517e-77ab-46e2-95e2-79168c044e9c",
   "metadata": {
    "id": "2a96517e-77ab-46e2-95e2-79168c044e9c"
   },
   "source": [
    "## Studio（可视化与本地开发）\n",
    "\n",
    "**⚠️ 说明**\n",
    "\n",
    "自课程视频录制后，Studio 已更新支持“本地运行并在浏览器打开”。这现在是推荐方式（而不是使用视频中演示的桌面应用）。\n",
    "\n",
    "- 本地开发服务器说明见文档：[LangGraph Studio 本地开发服务](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server)\n",
    "- 如何运行见文档：[运行本地 Studio](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server)\n",
    "\n",
    "在本模块的 `/studio` 目录中，执行：\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "你将看到类似输出：\n",
    "```\n",
    "- 🚀 API: http://127.0.0.1:2024\n",
    "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "用浏览器打开 Studio UI：`https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`。\n",
    "\n",
    "我们将把上面的图在 Studio 中加载，对应到 `module-4/studio/langgraph.json` 中配置的 `module-4/studio/map_reduce.py`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5e45-9a4c-43b4-8393-9298b3dcda53",
   "metadata": {
    "id": "741a5e45-9a4c-43b4-8393-9298b3dcda53"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
