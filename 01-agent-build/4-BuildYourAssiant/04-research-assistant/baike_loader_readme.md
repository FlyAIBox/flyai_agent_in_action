# ç™¾åº¦ç™¾ç§‘æ–‡æ¡£åŠ è½½å™¨ä½¿ç”¨è¯´æ˜

## æ¦‚è¿°

`BaiduBaikeLoader` æ˜¯ä¸€ä¸ªä¸“é—¨ä¸º LangChain æ¡†æ¶è®¾è®¡çš„ç™¾åº¦ç™¾ç§‘å†…å®¹åŠ è½½å™¨ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°ä»ç™¾åº¦ç™¾ç§‘è·å–æƒå¨çš„ä¸­æ–‡çŸ¥è¯†å†…å®¹ã€‚è¿™ä¸ªåŠ è½½å™¨ç‰¹åˆ«é€‚ç”¨äºæ„å»ºä¸­æ–‡çŸ¥è¯†åº“ã€é—®ç­”ç³»ç»Ÿä»¥åŠç ”ç©¶åŠ©æ‰‹å·¥å…·ã€‚

## ä¸»è¦ç‰¹æ€§

### ğŸ¯ æ™ºèƒ½å†…å®¹æå–
- **ä¼˜å…ˆæå–æ‘˜è¦**ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶æå–ç™¾åº¦ç™¾ç§‘æ¡ç›®çš„æ‘˜è¦éƒ¨åˆ†ï¼Œé€šå¸¸åŒ…å«æœ€é‡è¦çš„ä¿¡æ¯
- **å›é€€åˆ°æ­£æ–‡**ï¼šå¦‚æœæ‘˜è¦ä¸å­˜åœ¨ï¼Œåˆ™æå–æ­£æ–‡æ®µè½å†…å®¹
- **åŒé‡è§£ææ¨¡å¼**ï¼šæ”¯æŒ BeautifulSoup ç²¾ç¡®è§£æå’Œæ­£åˆ™è¡¨è¾¾å¼å¤‡ç”¨æ–¹æ¡ˆ

### ğŸ”„ å®¹é”™æœºåˆ¶
- **ç›´æ¥è®¿é—®ä¼˜å…ˆ**ï¼šé¦–å…ˆå°è¯•ç›´æ¥è®¿é—®æ¡ç›®é¡µé¢ï¼ˆå¦‚ï¼š`https://baike.baidu.com/item/äººå·¥æ™ºèƒ½`ï¼‰
- **æœç´¢å›é€€**ï¼šå¦‚æœç›´æ¥è®¿é—®å¤±è´¥ï¼Œè‡ªåŠ¨å›é€€åˆ°æœç´¢é¡µé¢å¹¶é€‰æ‹©ç›¸å…³æ¡ç›®
- **å¼‚å¸¸å¤„ç†**ï¼šå•ä¸ªé“¾æ¥å¤±è´¥ä¸å½±å“å…¶ä»–é“¾æ¥çš„å¤„ç†

### âš™ï¸ çµæ´»é…ç½®
- **å†…å®¹é•¿åº¦é™åˆ¶**ï¼šæ”¯æŒè®¾ç½®æœ€å¤§å­—ç¬¦æ•°ï¼Œé¿å…å•æ¬¡åŠ è½½è¿‡å¤šå†…å®¹
- **æ–‡æ¡£æ•°é‡æ§åˆ¶**ï¼šå¯è®¾ç½®æœ€å¤§åŠ è½½çš„æ¡ç›®é¡µé¢æ•°é‡
- **è¶…æ—¶è®¾ç½®**ï¼šå¯è‡ªå®šä¹‰ HTTP è¯·æ±‚è¶…æ—¶æ—¶é—´
- **è‡ªå®šä¹‰è¯·æ±‚å¤´**ï¼šæ”¯æŒæ·»åŠ è‡ªå®šä¹‰ HTTP è¯·æ±‚å¤´

## å®‰è£…ä¾èµ–

```bash
# å¿…éœ€ä¾èµ–
pip install requests langchain-core

# å¯é€‰ä¾èµ–ï¼ˆæ¨èï¼Œç”¨äºæ›´å¥½çš„å†…å®¹è§£æï¼‰
pip install beautifulsoup4
```

## åŸºç¡€ç”¨æ³•

### ç®€å•ä½¿ç”¨

```python
from baike_loader import BaiduBaikeLoader

# åˆ›å»ºåŠ è½½å™¨å®ä¾‹
loader = BaiduBaikeLoader("äººå·¥æ™ºèƒ½")

# è·å–æ–‡æ¡£
documents = list(loader.lazy_load())

# æŸ¥çœ‹ç»“æœ
for doc in documents:
    print(f"æ¥æº: {doc.metadata['source']}")
    print(f"å†…å®¹: {doc.page_content[:200]}...")
    print("-" * 50)
```

### é«˜çº§é…ç½®

```python
# åˆ›å»ºé«˜çº§é…ç½®çš„åŠ è½½å™¨
loader = BaiduBaikeLoader(
    query="æœºå™¨å­¦ä¹ ",           # æœç´¢æŸ¥è¯¢è¯
    load_max_docs=3,          # æœ€å¤šåŠ è½½3ä¸ªæ–‡æ¡£
    doc_content_chars_max=5000,  # æ¯ä¸ªæ–‡æ¡£æœ€å¤š5000å­—ç¬¦
    timeout=15,               # è¯·æ±‚è¶…æ—¶15ç§’
    headers={                 # è‡ªå®šä¹‰è¯·æ±‚å¤´
        "User-Agent": "MyBot/1.0"
    }
)

# æ‡’åŠ è½½æ–¹å¼è·å–æ–‡æ¡£
for doc in loader.lazy_load():
    print(f"æ ‡é¢˜: {doc.metadata.get('title', 'æœªçŸ¥')}")
    print(f"æ¥æº: {doc.metadata['source']}")
    print(f"å†…å®¹é•¿åº¦: {len(doc.page_content)} å­—ç¬¦")
    print(f"å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...")
    print("=" * 60)
```

## åœ¨ LangChain ä¸­ä½¿ç”¨

### ä¸å‘é‡æ•°æ®åº“ç»“åˆ

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from baike_loader import BaiduBaikeLoader

# åˆ›å»ºåµŒå…¥æ¨¡å‹
embeddings = OpenAIEmbeddings()

# åŠ è½½ç™¾åº¦ç™¾ç§‘å†…å®¹
loader = BaiduBaikeLoader("æ·±åº¦å­¦ä¹ ", load_max_docs=5)
documents = list(loader.lazy_load())

# åˆ›å»ºå‘é‡æ•°æ®åº“
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings
)

# è¿›è¡Œç›¸ä¼¼æ€§æœç´¢
results = vectorstore.similarity_search("ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ", k=3)
for result in results:
    print(result.page_content[:200])
```

### ä¸æ£€ç´¢å™¨ç»“åˆ

```python
from langchain.retrievers import VectorStoreRetriever
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# åˆ›å»ºæ£€ç´¢å™¨
retriever = VectorStoreRetriever(vectorstore=vectorstore)

# åˆ›å»ºé—®ç­”é“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    chain_type="stuff",
    retriever=retriever
)

# è¿›è¡Œé—®ç­”
question = "æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ"
answer = qa_chain.run(question)
print(f"é—®é¢˜: {question}")
print(f"ç­”æ¡ˆ: {answer}")
```

## å‚æ•°è¯¦è§£

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `query` | str | å¿…éœ€ | æœç´¢æŸ¥è¯¢è¯æˆ–å®ä½“åç§° |
| `load_max_docs` | int | 2 | æœ€å¤§åŠ è½½çš„æ¡ç›®é¡µé¢æ•°é‡ |
| `doc_content_chars_max` | Optional[int] | None | æ–‡æ¡£å†…å®¹æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ |
| `timeout` | int | 12 | HTTP è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |
| `headers` | Optional[dict] | None | è‡ªå®šä¹‰ HTTP è¯·æ±‚å¤´ |

## ä½¿ç”¨åœºæ™¯

### 1. çŸ¥è¯†åº“æ„å»º
```python
# æ‰¹é‡æ„å»ºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†åº“
topics = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†"]
all_documents = []

for topic in topics:
    loader = BaiduBaikeLoader(topic, load_max_docs=3)
    documents = list(loader.lazy_load())
    all_documents.extend(documents)

# åˆ›å»ºçŸ¥è¯†åº“
vectorstore = Chroma.from_documents(all_documents, embeddings)
```

### 2. å®æ—¶é—®ç­”ç³»ç»Ÿ
```python
# åŠ¨æ€è·å–æœ€æ–°ä¿¡æ¯
def get_latest_info(query: str):
    loader = BaiduBaikeLoader(query, load_max_docs=2, doc_content_chars_max=3000)
    documents = list(loader.lazy_load())
    return documents

# åœ¨é—®ç­”ç³»ç»Ÿä¸­ä½¿ç”¨
user_question = "ä»€ä¹ˆæ˜¯ChatGPTï¼Ÿ"
relevant_docs = get_latest_info(user_question)
# å°†æ–‡æ¡£ä¼ é€’ç»™LLMè¿›è¡Œå›ç­”
```

### 3. ç ”ç©¶åŠ©æ‰‹å·¥å…·
```python
# ä¸ºç ”ç©¶åŠ©æ‰‹æä¾›æƒå¨ä¿¡æ¯æº
def research_topic(topic: str, max_docs: int = 5):
    loader = BaiduBaikeLoader(
        query=topic,
        load_max_docs=max_docs,
        doc_content_chars_max=8000
    )
    return list(loader.lazy_load())

# ä½¿ç”¨ç¤ºä¾‹
research_data = research_topic("é‡å­è®¡ç®—", max_docs=3)
for doc in research_data:
    print(f"æ¥æº: {doc.metadata['source']}")
    print(f"å†…å®¹æ‘˜è¦: {doc.page_content[:300]}...")
```

## æ³¨æ„äº‹é¡¹

### 1. ç½‘ç»œè¯·æ±‚
- ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œèƒ½å¤Ÿè®¿é—®ç™¾åº¦ç™¾ç§‘
- å»ºè®®è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
- å¤§é‡è¯·æ±‚æ—¶æ³¨æ„æ§åˆ¶é¢‘ç‡ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆå‹åŠ›

### 2. å†…å®¹è´¨é‡
- ç™¾åº¦ç™¾ç§‘å†…å®¹è´¨é‡å‚å·®ä¸é½ï¼Œå»ºè®®å¯¹è·å–çš„å†…å®¹è¿›è¡ŒéªŒè¯
- æŸäº›æ¡ç›®å¯èƒ½åŒ…å«è¿‡æ—¶ä¿¡æ¯ï¼Œéœ€è¦ç»“åˆå…¶ä»–ä¿¡æ¯æº
- å»ºè®®è®¾ç½®å­—ç¬¦æ•°é™åˆ¶ï¼Œé¿å…åŠ è½½è¿‡é•¿çš„å†…å®¹

### 3. ä¾èµ–ç®¡ç†
- BeautifulSoup ä¸ºå¯é€‰ä¾èµ–ï¼Œä½†å¼ºçƒˆæ¨èå®‰è£…ä»¥è·å¾—æ›´å¥½çš„è§£ææ•ˆæœ
- éœ€è¦ LangChain Core 0.3.x ç‰ˆæœ¬ï¼ˆä¸é¡¹ç›®ä¾èµ–ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰

### 4. é”™è¯¯å¤„ç†
```python
try:
    loader = BaiduBaikeLoader("ä¸å­˜åœ¨çš„æ¡ç›®")
    documents = list(loader.lazy_load())
    if not documents:
        print("æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
except Exception as e:
    print(f"åŠ è½½å¤±è´¥: {e}")
```

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆæœ‰æ—¶å€™è·å–ä¸åˆ°å†…å®¹ï¼Ÿ
A: å¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼š
- ç½‘ç»œè¿æ¥é—®é¢˜
- æŸ¥è¯¢è¯åœ¨ç™¾åº¦ç™¾ç§‘ä¸­ä¸å­˜åœ¨
- è¯·æ±‚è¢«æœåŠ¡å™¨æ‹’ç»ï¼ˆå¯ä»¥å°è¯•æ·»åŠ æ›´å¤šè¯·æ±‚å¤´ï¼‰
- é¡µé¢ç»“æ„å‘ç”Ÿå˜åŒ–ï¼ˆéœ€è¦æ›´æ–°é€‰æ‹©å™¨ï¼‰

### Q: å¦‚ä½•æé«˜å†…å®¹æå–çš„å‡†ç¡®æ€§ï¼Ÿ
A: å»ºè®®ï¼š
- å®‰è£… BeautifulSoup ä¾èµ–
- ä½¿ç”¨æ›´å…·ä½“çš„æŸ¥è¯¢è¯
- è®¾ç½®åˆç†çš„å­—ç¬¦æ•°é™åˆ¶
- æ£€æŸ¥è¿”å›çš„å…ƒæ•°æ®ä¸­çš„ source å­—æ®µ

### Q: å¯ä»¥ç”¨äºå•†ä¸šé¡¹ç›®å—ï¼Ÿ
A: è¯·éµå®ˆç™¾åº¦ç™¾ç§‘çš„ä½¿ç”¨æ¡æ¬¾å’Œ robots.txt è§„åˆ™ï¼Œå»ºè®®ï¼š
- æ§åˆ¶è¯·æ±‚é¢‘ç‡
- ä¸è¦ç”¨äºå¤§è§„æ¨¡çˆ¬å–
- å°Šé‡ç½‘ç«™çš„æœåŠ¡æ¡æ¬¾

## æ›´æ–°æ—¥å¿—

- **v1.0.0**: åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºç¡€çš„ç™¾åº¦ç™¾ç§‘å†…å®¹åŠ è½½åŠŸèƒ½
- æ”¯æŒç›´æ¥è®¿é—®å’Œæœç´¢å›é€€ä¸¤ç§æ¨¡å¼
- æ”¯æŒå†…å®¹é•¿åº¦é™åˆ¶å’Œæ–‡æ¡£æ•°é‡æ§åˆ¶
- é€‚é… LangChain Core 0.3.x ç‰ˆæœ¬
