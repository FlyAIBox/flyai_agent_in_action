{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1696.30 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 651.77 GB, Free: 1260.59 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b"
   },
   "source": [
    "# 研究助理（Research Assistant）\n",
    "\n",
    "## 回顾\n",
    "\n",
    "本节将用到 LangGraph 的几个核心主题：\n",
    "\n",
    "- 记忆（Memory）\n",
    "- 人机协同（Human-in-the-loop）\n",
    "- 可控性（Controllability）\n",
    "\n",
    "现在，我们把这些理念整合起来，构建一个非常常见且实用的 AI 应用：研究自动化。\n",
    "\n",
    "传统研究通常由分析师承担，工作繁琐、耗时。AI 在这方面有很大助力空间。\n",
    "\n",
    "但研究环节需要强定制化：直接使用大模型原始输出，往往不适合真实的决策流程。\n",
    "\n",
    "基于 AI 的定制化「研究与报告生成」工作流，是一个可行且有前景的方向（参考：[Reports over RAG](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag)）。\n",
    "\n",
    "## 目标\n",
    "\n",
    "构建一个围绕聊天模型的「轻量级多智能体」系统，用于定制化研究流程。\n",
    "\n",
    "`数据源选择（Source Selection）`\n",
    "- 用户可以自由选择任意研究输入源。\n",
    "\n",
    "`规划（Planning）`\n",
    "- 用户给出主题，系统生成一支 AI 分析师团队，每位分析师负责一个子主题。\n",
    "- 在研究开始前，通过「人机协同」来微调/确认这些子主题。\n",
    "\n",
    "`大模型使用（LLM Utilization）`\n",
    "- 每位分析师会基于所选数据源，与专家型 AI 进行深入「访谈」。\n",
    "- 访谈为多轮对话，旨在抽取更具体、更有洞见的信息（类似于 [STORM](https://arxiv.org/abs/2402.14207) 论文中的做法）。\n",
    "- 这些访谈将使用带有内部状态的 `子图（sub-graphs）` 来承载与追踪。\n",
    "\n",
    "`研究过程（Research Process）`\n",
    "- 专家会并行收集信息以回答分析师的问题（`parallel`）。\n",
    "- 所有访谈将通过 `map-reduce` 同步进行与汇总。\n",
    "\n",
    "`输出格式（Output Format）`\n",
    "- 将每次访谈中得到的洞见进行综合，产出最终报告。\n",
    "- 报告使用可定制提示词，便于灵活调整输出格式。\n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
   "metadata": {
    "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# 安装项目所需的Python包\n",
    "# 使用 %%capture --no-stderr 来隐藏安装过程中的输出信息，保持notebook整洁\n",
    "# %pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia\n",
    "%pip install --quiet langgraph==0.6.7 langchain_openai==0.3.32 langchain_community==0.3.29 langchain_core==0.3.75 tavily-python==0.7.12 wikipedia==1.4.0\n",
    "\n",
    "# 包说明：\n",
    "# - langgraph: LangGraph框架，用于构建多智能体工作流\n",
    "# - langchain_openai: LangChain的OpenAI集成，用于调用GPT模型\n",
    "# - langchain_community: LangChain社区工具集，包含各种第三方集成\n",
    "# - langchain_core: LangChain核心组件，提供基础功能\n",
    "# - tavily-python: Tavily搜索API客户端，用于网络搜索\n",
    "# - wikipedia: 维基百科API客户端，用于获取维基百科内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {
    "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914"
   },
   "source": [
    "## 环境准备（Setup）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
    "outputId": "d651b4af-0c90-457a-f4c7-33c8d2b91688"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 环境变量名称\n",
    "\n",
    "    功能:\n",
    "        - 检查环境变量是否已存在\n",
    "        - 如果不存在，则通过getpass安全地获取用户输入\n",
    "        - 将用户输入设置为环境变量值\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的，用于身份验证\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "# 用于配置API请求的基础URL，支持使用代理服务\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {
    "id": "afe9ff57-0826-4669-b88b-4d0501a509f5"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化OpenAI聊天模型\n",
    "# 使用GPT-4o模型，这是OpenAI最新的多模态大语言模型\n",
    "# temperature=0 确保输出结果具有确定性和一致性，适合需要稳定输出的场景\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "id": "3419257b-2c6b-4d68-ae38-4a266cc02982"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[链路追踪（tracing）](https://docs.smith.langchain.com/concepts/tracing)，便于调试与分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
    "outputId": "9ef35fad-1514-41ed-e436-bdf87699f20e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "# 设置LangSmith追踪配置\n",
    "# LangSmith是LangChain的官方监控和调试平台\n",
    "_set_env(\"LANGSMITH_API_KEY\")  # 设置LangSmith API密钥\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # 启用链路追踪功能\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\" # 设置项目名称，用于组织追踪数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea",
   "metadata": {
    "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea"
   },
   "source": [
    "## 分析师：人机协同（Human-In-The-Loop）\n",
    "\n",
    "通过人机协同的方式生成并审核分析师角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e",
   "metadata": {
    "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"\n",
    "    分析师数据模型\n",
    "\n",
    "    用于定义每个AI分析师的基本信息和角色特征\n",
    "    每个分析师代表一个特定的研究视角和专长领域\n",
    "    \"\"\"\n",
    "    affiliation: str = Field(\n",
    "        description=\"分析师的主要隶属机构或组织\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"分析师姓名\"\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"分析师在研究主题中的具体角色定位\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"分析师的关注焦点、关切点和动机的详细描述\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        \"\"\"\n",
    "        生成分析师人设描述\n",
    "\n",
    "        返回:\n",
    "            str: 格式化的分析师人设信息，用于后续的AI对话中\n",
    "        \"\"\"\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    \"\"\"\n",
    "    分析师集合数据模型\n",
    "\n",
    "    用于存储和管理多个分析师的信息\n",
    "    支持结构化输出，确保AI生成的分析师信息格式正确\n",
    "    \"\"\"\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"包含所有分析师角色和隶属机构的综合列表\",\n",
    "    )\n",
    "\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    \"\"\"\n",
    "    分析师生成状态管理\n",
    "\n",
    "    用于在LangGraph工作流中管理分析师生成过程的状态信息\n",
    "    \"\"\"\n",
    "    topic: str  # 研究主题\n",
    "    max_analysts: int  # 分析师数量上限\n",
    "    human_analyst_feedback: str  # 人类反馈信息，用于人机协同调整\n",
    "    analysts: List[Analyst]  # 生成的分析师列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
    "outputId": "a7d4bbfb-b9e1-4c81-d490-f8f9214425ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图可视化：\n",
      "❌ Pyppeteer 渲染失败: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "📝 图结构（Mermaid 文本格式）：\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tcreate_analysts(create_analysts)\n",
      "\thuman_feedback(human_feedback<hr/><small><em>__interrupt = before</em></small>)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> create_analysts;\n",
      "\tcreate_analysts --> human_feedback;\n",
      "\thuman_feedback -.-> __end__;\n",
      "\thuman_feedback -.-> create_analysts;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "🔗 图结构信息：\n",
      "节点: ['__start__', 'create_analysts', 'human_feedback', '__end__']\n",
      "边: [Edge(source='__start__', target='create_analysts', data=None, conditional=False), Edge(source='create_analysts', target='human_feedback', data=None, conditional=False), Edge(source='human_feedback', target='__end__', data=None, conditional=True), Edge(source='human_feedback', target='create_analysts', data=None, conditional=True)]\n",
      "\n",
      "💡 手动渲染说明：\n",
      "1. 复制上面的 Mermaid 文本\n",
      "2. 访问 https://mermaid.live/\n",
      "3. 粘贴文本到编辑器中查看图形\n",
      "4. 或者使用支持 Mermaid 的 Markdown 编辑器\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# 分析师生成指令模板\n",
    "# 这个提示词指导AI如何根据研究主题创建合适的分析师团队\n",
    "analyst_instructions=\"\"\"你需要创建一组 AI 分析师人设。请严格遵循以下指引：\n",
    "\n",
    "1. 先审阅研究主题：\n",
    "{topic}\n",
    "\n",
    "2. 查看（可选的）编辑反馈，它将指导分析师的人设创建：\n",
    "\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. 基于上述文档与/或反馈，识别最值得关注的主题。\n",
    "\n",
    "4. 选出前 {max_analysts} 个主题。\n",
    "\n",
    "5. 为每个主题分配一位分析师。\"\"\"\n",
    "\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    创建分析师人设的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 根据研究主题和人类反馈生成分析师团队\n",
    "        2. 使用结构化输出确保生成的分析师信息格式正确\n",
    "        3. 将生成的分析师信息存储到状态中\n",
    "\n",
    "    参数:\n",
    "        state: 包含研究主题、分析师数量限制和人类反馈的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含生成的分析师列表的字典\n",
    "    \"\"\"\n",
    "    # 从状态中提取必要信息\n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
    "\n",
    "    # 配置结构化输出，确保返回Perspectives格式的数据\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # 构建系统消息，包含研究主题、反馈和数量限制\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成分析师集合\n",
    "    analysts = structured_llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=\"生成分析师集合。\")\n",
    "    ])\n",
    "\n",
    "    # 将分析师列表写入状态，供后续节点使用\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    人机协同中断点节点\n",
    "\n",
    "    功能:\n",
    "        - 作为工作流的中断点，允许人类审查和修改生成的分析师\n",
    "        - 这是一个空操作节点，主要用于流程控制\n",
    "        - 人类可以在此节点提供反馈，系统会根据反馈重新生成分析师\n",
    "\n",
    "    参数:\n",
    "        state: 当前状态对象\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    条件路由函数：决定工作流的下一步执行\n",
    "\n",
    "    功能:\n",
    "        - 检查是否有人类反馈\n",
    "        - 如果有反馈，重新生成分析师\n",
    "        - 如果没有反馈，结束流程\n",
    "\n",
    "    参数:\n",
    "        state: 当前状态对象\n",
    "\n",
    "    返回:\n",
    "        str: 下一个要执行的节点名称\n",
    "    \"\"\"\n",
    "    # 检查是否有人类反馈\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"  # 有反馈，重新生成分析师\n",
    "\n",
    "    # 没有反馈，结束流程\n",
    "    return END\n",
    "\n",
    "# 构建LangGraph工作流\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# 添加节点到工作流\n",
    "builder.add_node(\"create_analysts\", create_analysts)  # 分析师生成节点\n",
    "builder.add_node(\"human_feedback\", human_feedback)    # 人类反馈节点\n",
    "\n",
    "# 添加边连接节点\n",
    "builder.add_edge(START, \"create_analysts\")  # 开始 -> 生成分析师\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")  # 生成分析师 -> 人类反馈\n",
    "\n",
    "# 添加条件边：根据是否有反馈决定下一步\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    should_continue,\n",
    "    [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# 编译工作流\n",
    "memory = MemorySaver()  # 使用内存检查点保存状态\n",
    "graph = builder.compile(\n",
    "    interrupt_before=['human_feedback'],  # 在人类反馈节点前中断\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# 展示图结构\n",
    "# 图可视化\n",
    "print(\"图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd42b0a-188c-4374-a03c-a2ca8272cdcf",
   "metadata": {},
   "source": [
    "![image-20250930152238678](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301522726.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
    "outputId": "ca196d63-2a1c-4019-b769-e85da2f5c873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Chen\n",
      "Affiliation: AI Research Institute\n",
      "Role: 技术专家\n",
      "Description: 专注于LangGraph框架的技术优势，尤其是其在处理复杂语言模型时的性能提升和效率优化。\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Smith\n",
      "Affiliation: Global AI Solutions\n",
      "Role: 商业分析师\n",
      "Description: 研究LangGraph在商业应用中的潜力，特别是其在自然语言处理任务中的应用场景和市场价值。\n",
      "--------------------------------------------------\n",
      "Name: Prof. Linda Johnson\n",
      "Affiliation: University of Technology\n",
      "Role: 教育研究员\n",
      "Description: 关注LangGraph框架的教育和学术影响，探讨其在AI课程中的应用以及对学生学习的促进作用。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "max_analysts = 3\n",
    "topic = \"采用LangGraph作为AI Agent框架的好处\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
    "outputId": "f2d95e38-5acf-4c43-8974-a637aac666ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state and look at next node\n",
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
    "outputId": "d5bb8423-a691-4567-83c1-4e1d072d54bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09dce5-c346-678c-8002-07893291b6dd'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_analyst_feedback\":\n",
    "      \"加入一位来自初创公司的人，以增加创业者的视角\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8816eb9-9906-441b-b552-be71107db14f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8816eb9-9906-441b-b552-be71107db14f",
    "outputId": "c250ffd5-e4a2-441b-851e-28e76f049f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Chen\n",
      "Affiliation: AI Research Institute\n",
      "Role: 技术专家\n",
      "Description: 专注于LangGraph框架的技术优势，尤其是其在处理复杂语言模型时的性能提升和效率优化。\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Smith\n",
      "Affiliation: Global AI Solutions\n",
      "Role: 商业分析师\n",
      "Description: 研究LangGraph在商业应用中的潜力，特别是其在自然语言处理任务中的应用场景和市场价值。\n",
      "--------------------------------------------------\n",
      "Name: Prof. Linda Johnson\n",
      "Affiliation: University of Technology\n",
      "Role: 教育研究员\n",
      "Description: 关注LangGraph框架的教育和学术影响，探讨其在AI课程中的应用以及对学生学习的促进作用。\n",
      "--------------------------------------------------\n",
      "Name: Dr. Emily Zhang\n",
      "Affiliation: Tech Research Institute\n",
      "Role: 技术专家\n",
      "Description: 专注于LangGraph框架在技术上的优势，尤其是其在数据处理和分析中的高效性。\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Doe\n",
      "Affiliation: Business Strategy Group\n",
      "Role: 商业分析师\n",
      "Description: 研究LangGraph框架在商业应用中的潜力，特别是其在提高企业决策效率方面的贡献。\n",
      "--------------------------------------------------\n",
      "Name: Ms. Lisa Wang\n",
      "Affiliation: Startup Innovators\n",
      "Role: 创业者\n",
      "Description: 从创业者的角度分析LangGraph框架的灵活性和可扩展性，以及它如何帮助初创公司快速实现产品迭代。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43ac322-5926-4932-8653-68206fec0d2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a43ac322-5926-4932-8653-68206fec0d2c",
    "outputId": "771be8c3-bb76-4d6f-e00b-06afcf620aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09dce5-dc1d-6c45-8004-cdef70574b29'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果我们满意，那么我们就简单地不提供反馈\n",
    "further_feedack = None\n",
    "graph.update_state(thread, {\"human_analyst_feedback\":\n",
    "                            further_feedack}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab034e65-aeee-4723-8d6d-74541b548425",
   "metadata": {
    "id": "ab034e65-aeee-4723-8d6d-74541b548425"
   },
   "outputs": [],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f204e8a-285c-4e46-8223-a695caec7764",
   "metadata": {
    "id": "2f204e8a-285c-4e46-8223-a695caec7764"
   },
   "outputs": [],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get('analysts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
    "outputId": "d16bd1df-ec14-43c0-aa4e-8440c4bef1e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
    "outputId": "bd20945d-6717-4ee7-c981-36ffb06f9e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Zhang\n",
      "Affiliation: Tech Research Institute\n",
      "Role: 技术专家\n",
      "Description: 专注于LangGraph框架在技术上的优势，尤其是其在数据处理和分析中的高效性。\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Doe\n",
      "Affiliation: Business Strategy Group\n",
      "Role: 商业分析师\n",
      "Description: 研究LangGraph框架在商业应用中的潜力，特别是其在提高企业决策效率方面的贡献。\n",
      "--------------------------------------------------\n",
      "Name: Ms. Lisa Wang\n",
      "Affiliation: Startup Innovators\n",
      "Role: 创业者\n",
      "Description: 从创业者的角度分析LangGraph框架的灵活性和可扩展性，以及它如何帮助初创公司快速实现产品迭代。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7",
   "metadata": {
    "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7"
   },
   "source": [
    "## 进行访谈（Conduct Interview）\n",
    "\n",
    "### 生成问题（Generate Question）\n",
    "\n",
    "分析师将向专家提出问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c",
   "metadata": {
    "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    \"\"\"\n",
    "    访谈状态管理类\n",
    "\n",
    "    继承自MessagesState，用于管理分析师与专家之间的对话状态\n",
    "    包含访谈过程中的所有必要信息和上下文\n",
    "    \"\"\"\n",
    "    max_num_turns: int  # 对话轮次上限，控制访谈深度\n",
    "    context: Annotated[list, operator.add]  # 检索到的源文档列表，使用operator.add进行累加\n",
    "    analyst: Analyst  # 当前进行访谈的分析师对象\n",
    "    interview: str  # 完整的访谈记录文本\n",
    "    sections: list  # 访谈摘要小节列表，用于最终报告生成\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    搜索查询数据模型\n",
    "\n",
    "    用于结构化生成搜索查询，确保搜索请求格式正确\n",
    "    \"\"\"\n",
    "    search_query: str = Field(None, description=\"用于检索的搜索查询语句\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0",
   "metadata": {
    "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0"
   },
   "outputs": [],
   "source": [
    "# 问题生成指令模板\n",
    "# 指导AI分析师如何与专家进行有效的访谈对话\n",
    "question_instructions = \"\"\"你是一名分析师，需要通过访谈专家来了解一个具体主题。\n",
    "\n",
    "你的目标是提炼与该主题相关的「有趣且具体」的洞见。\n",
    "\n",
    "1. 有趣（Interesting）：让人感到意外或非显而易见的观点。\n",
    "\n",
    "2. 具体（Specific）：避免泛泛而谈，包含专家提供的具体案例或细节。\n",
    "\n",
    "以下是你的关注主题与目标设定：{goals}\n",
    "\n",
    "请先用符合你人设的名字进行自我介绍，然后提出你的第一个问题。\n",
    "\n",
    "持续追问，逐步深入，逐步完善你对该主题的理解。\n",
    "\n",
    "当你认为信息已充分，请以这句话结束访谈：「非常感谢您的帮助!」\n",
    "\n",
    "请始终保持与你的人设与目标一致的说话方式。\"\"\"\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\"\n",
    "    生成访谈问题的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 根据分析师的人设和当前对话历史生成下一个问题\n",
    "        2. 确保问题符合分析师的关注点和角色定位\n",
    "        3. 维护对话的连贯性和深度\n",
    "\n",
    "    参数:\n",
    "        state: 包含分析师信息和对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含新生成问题的消息列表\n",
    "    \"\"\"\n",
    "    # 从状态中获取分析师信息和当前对话历史\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 构建系统消息，包含分析师的人设信息\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "\n",
    "    # 调用大模型生成下一个问题\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 将生成的问题添加到消息历史中\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ff33a-6232-4a79-8a82-882a645394f5",
   "metadata": {
    "id": "be2ff33a-6232-4a79-8a82-882a645394f5"
   },
   "source": [
    "### 生成回答：并行化（Parallelization）\n",
    "\n",
    "专家将并行地从多个来源收集信息来回答问题。\n",
    "\n",
    "例如，我们可以使用：\n",
    "\n",
    "- 具体网站（例如通过 [`WebBaseLoader`](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/) 抓取）\n",
    "- 已建立索引的文档（例如基于 [RAG](https://python.langchain.com/v0.2/docs/tutorials/rag/) 的检索）\n",
    "- Web 搜索\n",
    "- 维基百科搜索\n",
    "\n",
    "你也可以尝试不同的 Web 搜索工具，比如 [Tavily](https://tavily.com/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "606ea95b-e811-4299-8b66-835d4016c338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "606ea95b-e811-4299-8b66-835d4016c338",
    "outputId": "52495f61-eb30-4785-8b55-fdfc00eca71a"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全设置环境变量的辅助函数（重复定义，保持代码完整性）\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置Tavily搜索API密钥\n",
    "# Tavily是一个专门为AI应用优化的搜索API，提供高质量的搜索结果\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
    "outputId": "01c8bb14-5945-4d7e-c81f-dcdaf6ec9a37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tmp/ipykernel_11236/1003905209.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "# 网络搜索工具配置\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 初始化Tavily搜索工具\n",
    "# max_results=3 限制每次搜索返回的结果数量，平衡信息丰富度和处理效率\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c",
   "metadata": {
    "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c"
   },
   "outputs": [],
   "source": [
    "# 维基百科搜索工具配置\n",
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb1603",
   "metadata": {
    "id": "06cb1603"
   },
   "source": [
    "接下来，我们将创建用于 Web 与维基百科检索的节点。\n",
    "\n",
    "还会创建一个用于回答分析师问题的节点。\n",
    "\n",
    "最后，创建用于保存完整访谈内容，以及撰写访谈摘要（“section”）的节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
    "outputId": "c0063b88-b11a-4fa3-e621-34dab72fa120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图可视化：\n",
      "❌ Pyppeteer 渲染失败: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "📝 图结构（Mermaid 文本格式）：\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\task_question(ask_question)\n",
      "\tsearch_web(search_web)\n",
      "\tsearch_wikipedia(search_wikipedia)\n",
      "\tanswer_question(answer_question)\n",
      "\tsave_interview(save_interview)\n",
      "\twrite_section(write_section)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> ask_question;\n",
      "\tanswer_question -.-> ask_question;\n",
      "\tanswer_question -.-> save_interview;\n",
      "\task_question --> search_web;\n",
      "\task_question --> search_wikipedia;\n",
      "\tsave_interview --> write_section;\n",
      "\tsearch_web --> answer_question;\n",
      "\tsearch_wikipedia --> answer_question;\n",
      "\twrite_section --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "🔗 图结构信息：\n",
      "节点: ['__start__', 'ask_question', 'search_web', 'search_wikipedia', 'answer_question', 'save_interview', 'write_section', '__end__']\n",
      "边: [Edge(source='__start__', target='ask_question', data=None, conditional=False), Edge(source='answer_question', target='ask_question', data=None, conditional=True), Edge(source='answer_question', target='save_interview', data=None, conditional=True), Edge(source='ask_question', target='search_web', data=None, conditional=False), Edge(source='ask_question', target='search_wikipedia', data=None, conditional=False), Edge(source='save_interview', target='write_section', data=None, conditional=False), Edge(source='search_web', target='answer_question', data=None, conditional=False), Edge(source='search_wikipedia', target='answer_question', data=None, conditional=False), Edge(source='write_section', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "💡 手动渲染说明：\n",
      "1. 复制上面的 Mermaid 文本\n",
      "2. 访问 https://mermaid.live/\n",
      "3. 粘贴文本到编辑器中查看图形\n",
      "4. 或者使用支持 Mermaid 的 Markdown 编辑器\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "# 搜索查询生成指令\n",
    "# 指导AI如何从对话中提取有效的搜索查询\n",
    "search_instructions = SystemMessage(content=f\"\"\"你将获得一段分析师与专家之间的对话。\n",
    "\n",
    "你的目标是基于这段对话，为Web搜索生成一条结构良好的查询语句。\n",
    "\n",
    "首先，通读整段对话。\n",
    "\n",
    "特别关注分析师最后提出的问题。\n",
    "\n",
    "将这个最终问题转化为结构良好的 Web 搜索查询。\"\"\")\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"\n",
    "    通过Web搜索检索相关文档\n",
    "\n",
    "    功能:\n",
    "        1. 分析当前对话内容，生成合适的搜索查询\n",
    "        2. 使用Tavily API执行网络搜索\n",
    "        3. 格式化搜索结果，便于后续处理\n",
    "\n",
    "    参数:\n",
    "        state: 包含对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含格式化搜索结果的上下文信息\n",
    "    \"\"\"\n",
    "    # 使用结构化输出生成搜索查询\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    # 执行Tavily网络搜索\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # 格式化搜索结果，添加来源信息\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
    "        f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "        for doc in search_docs\n",
    "    ])\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \"\"\"\n",
    "    通过维基百科检索相关文档\n",
    "\n",
    "    功能:\n",
    "        1. 分析当前对话内容，生成维基百科搜索查询\n",
    "        2. 使用WikipediaLoader获取维基百科内容\n",
    "        3. 格式化搜索结果，便于后续处理\n",
    "\n",
    "    参数:\n",
    "        state: 包含对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含格式化维基百科搜索结果的上下文信息\n",
    "    \"\"\"\n",
    "    # 使用结构化输出生成搜索查询\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    # 执行维基百科搜索，限制最多2个文档\n",
    "    search_docs = WikipediaLoader(\n",
    "        query=search_query.search_query,\n",
    "        load_max_docs=2\n",
    "    ).load()\n",
    "\n",
    "    # 格式化维基百科搜索结果\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
    "        f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "        for doc in search_docs\n",
    "    ])\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "# 专家回答指令模板\n",
    "# 指导AI专家如何基于检索到的信息回答分析师的问题\n",
    "answer_instructions = \"\"\"你是一位被分析师访谈的专家。\n",
    "\n",
    "以下是分析师的关注领域：{goals}。\n",
    "\n",
    "你的目标是回答访谈者提出的问题。\n",
    "\n",
    "回答问题时，请仅使用以下上下文：\n",
    "\n",
    "{context}\n",
    "\n",
    "回答须遵循如下要求：\n",
    "\n",
    "1. 只使用上下文中提供的信息。\n",
    "\n",
    "2. 不要引入上下文之外的信息，也不要做未在上下文明确说明的假设。\n",
    "\n",
    "3. 上下文在每段文档顶部包含来源信息。\n",
    "\n",
    "4. 在涉及具体论断时，请在相应内容旁标注引用来源编号。例如，针对来源 1 使用 [1]。\n",
    "\n",
    "5. 在答案结尾处按顺序列出引用来源，如：[1] Source 1, [2] Source 2 等。\n",
    "\n",
    "6. 若来源形如：<Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>，则在引用列表中只写：\n",
    "\n",
    "[1] assistant/docs/llama3_1.pdf, page 7\n",
    "\n",
    "并且不要再重复加中括号，也不要附加 Document source 前缀。\"\"\"\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"\n",
    "    生成专家回答的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于检索到的上下文信息回答分析师的问题\n",
    "        2. 确保回答符合专家的角色定位\n",
    "        3. 提供准确的引用和来源信息\n",
    "\n",
    "    参数:\n",
    "        state: 包含分析师信息、对话历史和检索上下文的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含专家回答的消息列表\n",
    "    \"\"\"\n",
    "    # 从状态中获取必要信息\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # 构建系统消息，包含分析师关注点和检索上下文\n",
    "    system_message = answer_instructions.format(\n",
    "        goals=analyst.persona,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成专家回答\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 标记该消息来自专家，便于后续路由\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # 将专家回答添加到消息历史中\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"\n",
    "    保存访谈内容的函数\n",
    "\n",
    "    功能:\n",
    "        1. 将完整的对话历史转换为文本格式\n",
    "        2. 保存访谈记录，供后续报告生成使用\n",
    "\n",
    "    参数:\n",
    "        state: 包含对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含完整访谈记录的字典\n",
    "    \"\"\"\n",
    "    # 获取所有对话消息\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 将消息列表转换为格式化的字符串\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # 将访谈记录保存到状态中\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"\n",
    "    消息路由函数：决定访谈流程的下一步\n",
    "\n",
    "    功能:\n",
    "        1. 检查是否达到最大对话轮次\n",
    "        2. 检查分析师是否表示访谈结束\n",
    "        3. 决定是继续提问还是保存访谈\n",
    "\n",
    "    参数:\n",
    "        state: 当前访谈状态\n",
    "        name: 专家消息的标识符，默认为\"expert\"\n",
    "\n",
    "    返回:\n",
    "        str: 下一个要执行的节点名称\n",
    "    \"\"\"\n",
    "    # 获取对话消息和最大轮次设置\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns', 2)\n",
    "\n",
    "    # 统计专家回答次数\n",
    "    num_responses = len([\n",
    "        m for m in messages\n",
    "        if isinstance(m, AIMessage) and m.name == name\n",
    "    ])\n",
    "\n",
    "    # 如果达到最大轮次，结束访谈\n",
    "    if num_responses >= max_num_turns:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # 检查上一个问题是否表明对话结束\n",
    "    # 注意：这里假设倒数第二个消息是分析师的问题\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"非常感谢您的帮助!\" in last_question.content:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # 继续提问\n",
    "    return \"ask_question\"\n",
    "\n",
    "# 报告小节写作指令模板\n",
    "# 指导AI如何将访谈内容转换为结构化的报告小节\n",
    "section_writer_instructions = \"\"\"你是一名资深技术写作者。\n",
    "\n",
    "你的任务是基于一组来源文档，撰写一段简洁、易读的报告小节。\n",
    "\n",
    "1. 先分析来源文档内容：\n",
    "- 每个文档的名称在文档开头，以 <Document 标签呈现。\n",
    "\n",
    "2. 使用 Markdown 制作小节结构：\n",
    "- 用 ## 作为小节标题\n",
    "- 用 ### 作为小节内的小标题\n",
    "\n",
    "3. 按结构撰写：\n",
    " a. 标题（## 头）\n",
    " b. 摘要（### 头）\n",
    " c. 参考来源（### 头）\n",
    "\n",
    "4. 标题需要贴合分析师的关注点并具有吸引力：\n",
    "{focus}\n",
    "\n",
    "5. 关于摘要部分：\n",
    "- 先给出与分析师关注点相关的背景/上下文\n",
    "- 强调访谈中获得的新颖、有趣或令人意外的洞见\n",
    "- 使用到来源文档时，按使用顺序创建编号\n",
    "- 不要提及访谈者或专家的名字\n",
    "- 控制在约 400 字以内\n",
    "- 在报告正文中使用数字引用（如 [1]、[2]），基于来源文档信息\n",
    "\n",
    "6. 在参考来源部分：\n",
    "- 列出报告中使用到的全部来源\n",
    "- 给出完整链接或具体文档路径\n",
    "- 每个来源单独一行；在行尾加两个空格以产生 Markdown 换行\n",
    "- 参考格式：\n",
    "\n",
    "### Sources\n",
    "[1] 链接或文档名\n",
    "[2] 链接或文档名\n",
    "\n",
    "7. 合并重复来源。例如以下是不正确的：\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "应去重为：\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "8. 最终检查：\n",
    "- 确保报告结构符合要求\n",
    "- 标题前不要有任何前言\n",
    "- 检查是否遵循了全部规范\"\"\"\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"\n",
    "    生成报告小节的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于访谈内容和检索到的文档生成结构化的报告小节\n",
    "        2. 确保小节内容符合分析师的专业关注点\n",
    "        3. 提供准确的引用和来源信息\n",
    "\n",
    "    参数:\n",
    "        state: 包含访谈记录、检索上下文和分析师信息的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含生成的小节内容的字典\n",
    "    \"\"\"\n",
    "    # 从状态中获取必要信息\n",
    "    interview = state[\"interview\"]\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 构建系统消息，包含分析师的关注点描述\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "\n",
    "    # 调用大模型生成报告小节\n",
    "    section = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=f\"使用这个来源来撰写你的小节: {context}\")\n",
    "    ])\n",
    "\n",
    "    # 将生成的小节添加到状态中\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "# 构建访谈工作流\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "\n",
    "# 添加各个功能节点\n",
    "interview_builder.add_node(\"ask_question\", generate_question)      # 生成问题节点\n",
    "interview_builder.add_node(\"search_web\", search_web)              # 网络搜索节点\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)  # 维基百科搜索节点\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)    # 生成回答节点\n",
    "interview_builder.add_node(\"save_interview\", save_interview)      # 保存访谈节点\n",
    "interview_builder.add_node(\"write_section\", write_section)        # 撰写小节节点\n",
    "\n",
    "# 定义工作流连接关系\n",
    "interview_builder.add_edge(START, \"ask_question\")  # 开始 -> 提问\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")  # 提问 -> 网络搜索\n",
    "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")  # 提问 -> 维基百科搜索\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")  # 网络搜索 -> 回答\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")  # 维基百科搜索 -> 回答\n",
    "\n",
    "# 条件边：根据对话状态决定下一步\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\",\n",
    "    route_messages,\n",
    "    ['ask_question', 'save_interview']\n",
    ")\n",
    "\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")  # 保存访谈 -> 撰写小节\n",
    "interview_builder.add_edge(\"write_section\", END)  # 撰写小节 -> 结束\n",
    "\n",
    "# 编译访谈工作流\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n",
    "\n",
    "\n",
    "# 图可视化\n",
    "print(\"图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = interview_graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(interview_graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(interview_graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd4445-7147-41d3-91f5-21bbe48f2e00",
   "metadata": {},
   "source": [
    "![image-20250930152452478](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301524600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
    "outputId": "b65a5a0a-3276-4cb0-98e1-0f948ffa5295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation='Tech Research Institute', name='Dr. Emily Zhang', role='技术专家', description='专注于LangGraph框架在技术上的优势，尤其是其在数据处理和分析中的高效性。')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick one analyst\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750ac4f-f458-4b2d-8bad-32ce34895758",
   "metadata": {
    "id": "3750ac4f-f458-4b2d-8bad-32ce34895758"
   },
   "source": [
    "此处我们运行一次访谈，并传入与主题相关的 llama3.1 论文索引作为参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1642c58-2e68-45ad-b6d9-20ab26bf530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://en.wikipedia.org/w/rest.php/v1/page/Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
    "outputId": "db68b62e-29b6-4372-80a0-50e906028c33"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/util/util.py:38\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m messages = [HumanMessage(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m所以你说你在写一篇关于\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m的文章?\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m      3\u001b[39m thread = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m interview = \u001b[43minterview_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysts\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_num_turns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m Markdown(interview[\u001b[33m'\u001b[39m\u001b[33msections\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36msearch_wikipedia\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     62\u001b[39m search_query = structured_llm.invoke([search_instructions] + state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# 执行维基百科搜索，限制最多2个文档\u001b[39;00m\n\u001b[32m     65\u001b[39m search_docs = \u001b[43mWikipediaLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_max_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 格式化维基百科搜索结果\u001b[39;00m\n\u001b[32m     71\u001b[39m formatted_search_docs = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\n\u001b[32m     72\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<Document source=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata[\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m page=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc.page_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Document>\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m search_docs\n\u001b[32m     74\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_community/document_loaders/wikipedia.py:59\u001b[39m, in \u001b[36mWikipediaLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mLoads the query result from Wikipedia into a list of Documents.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m        Wikipedia pages.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m client = WikipediaAPIWrapper(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m     54\u001b[39m     lang=\u001b[38;5;28mself\u001b[39m.lang,\n\u001b[32m     55\u001b[39m     top_k_results=\u001b[38;5;28mself\u001b[39m.load_max_docs,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     56\u001b[39m     load_all_available_meta=\u001b[38;5;28mself\u001b[39m.load_all_available_meta,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     57\u001b[39m     doc_content_chars_max=\u001b[38;5;28mself\u001b[39m.doc_content_chars_max,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_community/utilities/wikipedia.py:111\u001b[39m, in \u001b[36mWikipediaAPIWrapper.load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_community/utilities/wikipedia.py:121\u001b[39m, in \u001b[36mWikipediaAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> Iterator[Document]:\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m \n\u001b[32m    120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     page_titles = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwiki_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mWIKIPEDIA_MAX_QUERY_LENGTH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtop_k_results\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m page_title \u001b[38;5;129;01min\u001b[39;00m page_titles[: \u001b[38;5;28mself\u001b[39m.top_k_results]:\n\u001b[32m    125\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m wiki_page := \u001b[38;5;28mself\u001b[39m._fetch_page(page_title):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/wikipedia/util.py:28\u001b[39m, in \u001b[36mcache.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m   ret = \u001b[38;5;28mself\u001b[39m._cache[key]\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m   ret = \u001b[38;5;28mself\u001b[39m._cache[key] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/wikipedia/wikipedia.py:103\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(query, results, suggestion)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion:\n\u001b[32m    101\u001b[39m   search_params[\u001b[33m'\u001b[39m\u001b[33msrinfo\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33msuggestion\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m raw_results = \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_results:\n\u001b[32m    106\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m raw_results[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33minfo\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mHTTP request timed out.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPool queue is full\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/wikipedia/wikipedia.py:737\u001b[39m, in \u001b[36m_wiki_request\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    734\u001b[39m   wait_time = (RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT) - datetime.now()\n\u001b[32m    735\u001b[39m   time.sleep(\u001b[38;5;28mint\u001b[39m(wait_time.total_seconds()))\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[32m    740\u001b[39m   RATE_LIMIT_LAST_CALL = datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/adapters.py:659\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    644\u001b[39m     resp = conn.urlopen(\n\u001b[32m    645\u001b[39m         method=request.method,\n\u001b[32m    646\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    655\u001b[39m         chunked=chunked,\n\u001b[32m    656\u001b[39m     )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    663\u001b[39m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "During task with name 'search_wikipedia' and id 'f4094682-8f61-5811-6c4f-73492ecdaed6'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "messages = [HumanMessage(f\"所以你说你在写一篇关于{topic}的文章?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "interview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread)\n",
    "Markdown(interview['sections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739e87-68bb-4e96-a86a-704e84240a6c",
   "metadata": {
    "id": "3b739e87-68bb-4e96-a86a-704e84240a6c"
   },
   "source": [
    "### 并行访谈：Map-Reduce\n",
    "\n",
    "我们通过 `Send()` API 并行运行每个访谈（map 步）。\n",
    "\n",
    "随后在 reduce 步中将它们合并为报告主体。\n",
    "\n",
    "### 收尾（Finalize）\n",
    "\n",
    "最后增加一步，为最终报告写出引言与结论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140",
   "metadata": {
    "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    研究图状态管理类\n",
    "\n",
    "    用于管理整个研究流程的状态信息，包括分析师生成、并行访谈和报告生成\n",
    "    这是整个研究助理系统的核心状态管理类\n",
    "    \"\"\"\n",
    "    topic: str  # 研究主题\n",
    "    max_analysts: int  # 分析师数量上限\n",
    "    human_analyst_feedback: str  # 人类反馈信息\n",
    "    analysts: List[Analyst]  # 分析师列表\n",
    "    sections: Annotated[list, operator.add]  # 报告小节列表，使用operator.add进行累加\n",
    "    introduction: str  # 最终报告的引言部分\n",
    "    content: str  # 最终报告的主体内容\n",
    "    conclusion: str  # 最终报告的结论部分\n",
    "    final_report: str  # 完整的最终报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2224592-d2ff-469d-97bd-928809f896d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c2224592-d2ff-469d-97bd-928809f896d7",
    "outputId": "9031b724-0a08-49bd-fc4a-74dbcb81ad3f"
   },
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    启动所有并行访谈的Map步骤\n",
    "\n",
    "    功能:\n",
    "        1. 检查是否有人类反馈，如果有则重新生成分析师\n",
    "        2. 如果没有反馈，则并行启动所有分析师的访谈流程\n",
    "        3. 使用Send API实现真正的并行执行\n",
    "\n",
    "    参数:\n",
    "        state: 包含分析师列表和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        str 或 List[Send]: 如果有人类反馈返回节点名，否则返回Send对象列表\n",
    "    \"\"\"\n",
    "    # 检查是否有人类反馈\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback')\n",
    "    if human_analyst_feedback:\n",
    "        # 有人类反馈，重新生成分析师\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # 没有反馈，并行启动所有访谈\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        # 为每个分析师创建一个Send对象，实现并行执行\n",
    "        return [\n",
    "            Send(\"conduct_interview\", {\n",
    "                \"analyst\": analyst,\n",
    "                \"messages\": [HumanMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\"\n",
    "                )]\n",
    "            })\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]\n",
    "\n",
    "# 报告写作指令模板\n",
    "# 指导AI如何将多个分析师的小节整合为统一的报告主体\n",
    "report_writer_instructions = \"\"\"你是一名技术写作者，正在为如下主题撰写报告：\n",
    "\n",
    "{topic}\n",
    "\n",
    "你拥有一支分析师团队。每位分析师完成了两件事：\n",
    "\n",
    "1. 围绕一个具体子主题，访谈了一位专家。\n",
    "2. 将发现写成一份备忘录（memo）。\n",
    "\n",
    "你的任务：\n",
    "\n",
    "1. 你将收到分析师们的备忘录集合。\n",
    "2. 仔细思考每份备忘录的洞见。\n",
    "3. 将它们整合为简洁的总体总结，串联起所有备忘录的中心观点。\n",
    "4. 把每份备忘录的关键信息归纳成一个连贯的单一叙述。\n",
    "\n",
    "报告格式要求：\n",
    "\n",
    "1. 使用 Markdown 格式。\n",
    "2. 报告不要有任何前言。\n",
    "3. 不使用任何小标题。\n",
    "4. 报告以一个标题开头：## Insights\n",
    "5. 报告中不要提及任何分析师的名字。\n",
    "6. 保留备忘录中的引用标注（如 [1]、[2]）。\n",
    "7. 汇总最终来源列表，并以 `## Sources` 作为小节标题。\n",
    "8. 按顺序列出来源且不要重复。\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "以下是分析师提供的备忘录，请基于此撰写报告：\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    生成最终报告主体内容的函数（Reduce步骤）\n",
    "\n",
    "    功能:\n",
    "        1. 收集所有分析师的小节内容\n",
    "        2. 将多个小节整合为统一的报告主体\n",
    "        3. 确保报告结构清晰、内容连贯\n",
    "\n",
    "    参数:\n",
    "        state: 包含所有小节内容和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含报告主体内容的字典\n",
    "    \"\"\"\n",
    "    # 获取所有小节内容和研究主题\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 将所有小节拼接为完整文本\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 构建系统消息，包含研究主题和小节内容\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic,\n",
    "        context=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成报告主体\n",
    "    report = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=f\"Write a report based upon these memos.\")\n",
    "    ])\n",
    "\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "# 引言和结论写作指令模板\n",
    "# 指导AI如何为报告生成简洁有力的引言和结论\n",
    "intro_conclusion_instructions = \"\"\"你是一名技术写作者，正在完成主题为 {topic} 的报告。\n",
    "\n",
    "你将获得报告的全部小节。\n",
    "\n",
    "你的任务是撰写简洁而有说服力的引言或结论。\n",
    "\n",
    "由用户告知写引言还是结论。\n",
    "\n",
    "两者均不需要任何前言。\n",
    "\n",
    "目标约 100 字：\n",
    "- 引言：精炼预览各小节要点\n",
    "- 结论：精炼回顾各小节要点\n",
    "\n",
    "使用 Markdown 格式。\n",
    "\n",
    "生成的报告使用中文，所有内容都要使用中文，对于特殊的英文术语，可以使用中英文一起表示。\n",
    "\n",
    "引言要求：创建一个有吸引力的标题，并用 # 作为标题头。\n",
    "\n",
    "引言小节标题使用：## Introduction\n",
    "\n",
    "结论小节标题使用：## Conclusion\n",
    "\n",
    "撰写时可参考以下小节内容：{formatted_str_sections}\"\"\"\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    生成报告引言的函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于所有小节内容生成报告引言\n",
    "        2. 提供报告的整体概览和吸引力\n",
    "        3. 为读者提供阅读指导\n",
    "\n",
    "    参数:\n",
    "        state: 包含所有小节内容和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含引言内容的字典\n",
    "    \"\"\"\n",
    "    # 获取所有小节内容和研究主题\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 将所有小节拼接为完整文本\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 构建指令，包含研究主题和小节内容\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic,\n",
    "        formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成引言\n",
    "    intro = llm.invoke([\n",
    "        instructions,\n",
    "        HumanMessage(content=f\"Write the report introduction\")\n",
    "    ])\n",
    "\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    生成报告结论的函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于所有小节内容生成报告结论\n",
    "        2. 总结报告的主要发现和洞察\n",
    "        3. 为读者提供清晰的总结\n",
    "\n",
    "    参数:\n",
    "        state: 包含所有小节内容和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含结论内容的字典\n",
    "    \"\"\"\n",
    "    # 获取所有小节内容和研究主题\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 将所有小节拼接为完整文本\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 构建指令，包含研究主题和小节内容\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic,\n",
    "        formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成结论\n",
    "    conclusion = llm.invoke([\n",
    "        instructions,\n",
    "        HumanMessage(content=f\"Write the report conclusion\")\n",
    "    ])\n",
    "\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    最终报告生成函数（Reduce步骤的最终阶段）\n",
    "\n",
    "    功能:\n",
    "        1. 整合引言、主体内容和结论\n",
    "        2. 处理来源信息的格式\n",
    "        3. 生成完整的最终报告\n",
    "\n",
    "    参数:\n",
    "        state: 包含引言、主体内容、结论和来源信息的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含完整最终报告的字典\n",
    "    \"\"\"\n",
    "    # 获取报告主体内容\n",
    "    content = state[\"content\"]\n",
    "\n",
    "    # 清理内容格式，移除重复的标题\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "\n",
    "    # 分离主体内容和来源信息\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    # 组合完整的最终报告\n",
    "    final_report = (\n",
    "        state[\"introduction\"] +\n",
    "        \"\\n\\n---\\n\\n\" +\n",
    "        content +\n",
    "        \"\\n\\n---\\n\\n\" +\n",
    "        state[\"conclusion\"]\n",
    "    )\n",
    "\n",
    "    # 如果有来源信息，添加到报告末尾\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# 构建完整的研究图工作流\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# 添加所有功能节点\n",
    "builder.add_node(\"create_analysts\", create_analysts)  # 分析师生成节点\n",
    "builder.add_node(\"human_feedback\", human_feedback)    # 人类反馈节点\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())  # 访谈子图节点\n",
    "builder.add_node(\"write_report\", write_report)        # 报告主体写作节点\n",
    "builder.add_node(\"write_introduction\", write_introduction)  # 引言写作节点\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)      # 结论写作节点\n",
    "builder.add_node(\"finalize_report\", finalize_report)        # 最终报告生成节点\n",
    "\n",
    "# 定义工作流连接关系\n",
    "builder.add_edge(START, \"create_analysts\")  # 开始 -> 生成分析师\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")  # 生成分析师 -> 人类反馈\n",
    "\n",
    "# 条件边：根据是否有反馈决定下一步\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    initiate_all_interviews,\n",
    "    [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# 并行执行：访谈完成后同时进行报告写作、引言写作和结论写作\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# 等待所有写作任务完成后，进行最终报告生成\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"],\n",
    "    \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)  # 最终报告生成 -> 结束\n",
    "\n",
    "# 编译完整的研究图工作流\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(\n",
    "    interrupt_before=['human_feedback'],  # 在人类反馈节点前中断\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# 显示完整的工作流图\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0",
   "metadata": {
    "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0"
   },
   "source": [
    "我们来就 LangGraph 提一个开放式问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
    "outputId": "2498f7ea-2c34-426b-caa2-efe6c2137bdf"
   },
   "outputs": [],
   "source": [
    "# 演示：运行研究助理系统\n",
    "# 设置输入参数\n",
    "max_analysts = 3  # 分析师数量\n",
    "topic = \"采用LangGraph作为AI Agent框架的好处\"  # 研究主题\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}  # 线程ID，用于状态管理\n",
    "\n",
    "# 运行工作流直到第一个中断点（人类反馈节点）\n",
    "for event in graph.stream({\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts\n",
    "}, thread, stream_mode=\"values\"):\n",
    "\n",
    "    # 检查是否有分析师信息输出\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        # 显示生成的分析师信息\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
    "outputId": "9ac87141-cc10-4e31-b453-f7c1d288bc06"
   },
   "outputs": [],
   "source": [
    "# 模拟人类反馈：添加一个AI原生初创公司的CEO视角\n",
    "# 这展示了人机协同（Human-in-the-loop）功能的使用\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": \"添加一个AI原生初创公司的CEO\"\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
    "outputId": "d507e3b8-68c4-43bd-da1a-026413e162f8"
   },
   "outputs": [],
   "source": [
    "# 检查更新后的分析师列表\n",
    "# 系统会根据人类反馈重新生成分析师团队\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        # 显示更新后的分析师信息\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af41f54-88d9-4597-98b0-444c08322095",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0af41f54-88d9-4597-98b0-444c08322095",
    "outputId": "44858e67-bb8a-44de-c5ef-2b191f117013"
   },
   "outputs": [],
   "source": [
    "# 确认满意当前的分析师团队，继续执行后续流程\n",
    "# 设置反馈为None表示没有进一步的修改需求\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": None\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
    "outputId": "dc7eee62-2047-4aff-9fd7-bae60e914aa7"
   },
   "outputs": [],
   "source": [
    "# 继续执行完整的研究流程\n",
    "# 包括并行访谈、报告生成等所有后续步骤\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
    "outputId": "14cfbc76-d0f9-436f-ccd1-c30110f6d3c7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 获取最终状态并显示生成的报告\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "\n",
    "# 使用Markdown格式显示最终报告\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3",
   "metadata": {
    "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3"
   },
   "source": [
    "# LangSmith执行追踪\n",
    "\n",
    "我们可以查看一次完整的执行追踪（trace），了解整个研究流程的详细执行情况：\n",
    "\n",
    "`https://smith.langchain.com/public/2933a7bb-bcef-4d2d-9b85-cc735b22ca0c/r`\n",
    "\n",
    "## 追踪内容说明\n",
    "\n",
    "这个追踪链接展示了：\n",
    "\n",
    "1. **分析师生成过程**：如何根据研究主题创建不同的分析师角色\n",
    "2. **并行访谈执行**：多个分析师同时进行访谈的详细过程\n",
    "3. **信息检索流程**：网络搜索和维基百科搜索的具体执行\n",
    "4. **报告生成步骤**：从访谈内容到最终报告的完整转换过程\n",
    "5. **人机协同交互**：人类反馈如何影响系统行为\n",
    "\n",
    "通过这个追踪，你可以深入了解LangGraph工作流的内部执行机制和每个节点的具体功能。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
