{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ğŸ”§ ç¯å¢ƒé…ç½®å’Œæ£€æŸ¥\n",
    "\n",
    "#### æ¦‚è¿°\n",
    "\n",
    "æœ¬æ•™ç¨‹éœ€è¦ç‰¹å®šçš„ç¯å¢ƒé…ç½®ä»¥ç¡®ä¿æœ€ä½³å­¦ä¹ ä½“éªŒã€‚ä»¥ä¸‹é…ç½®å°†å¸®åŠ©æ‚¨ï¼š\n",
    "\n",
    "- ä½¿ç”¨ç»Ÿä¸€çš„condaç¯å¢ƒï¼šæ¿€æ´»ç»Ÿä¸€çš„å­¦ä¹ ç¯å¢ƒ\n",
    "- é€šè¿‡å›½å†…é•œåƒæºå¿«é€Ÿå®‰è£…ä¾èµ–ï¼šé…ç½®pipä½¿ç”¨æ¸…åé•œåƒæº\n",
    "- åŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼šè®¾ç½®HuggingFaceé•œåƒä»£ç†\n",
    "- æ£€æŸ¥ç³»ç»Ÿé…ç½®ï¼šæ£€æŸ¥ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®\n",
    "\n",
    "#### é…ç½®\n",
    "\n",
    "- **æ‰€éœ€ç¯å¢ƒåŠå…¶ä¾èµ–å·²ç»éƒ¨ç½²å¥½**\n",
    "- åœ¨`Notebook`å³ä¸Šè§’é€‰æ‹©`jupyterå†…æ ¸`ä¸º`python(flyai_agent_in_action)`ï¼Œå³å¯æ‰§è¡Œä¸‹æ–¹ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda ç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š (ä»…é’ˆå¯¹å½“å‰ Bash å­è¿›ç¨‹) ==\n",
      "=========================================\n",
      "âœ… å½“å‰å•å…ƒæ ¼å·²æˆåŠŸæ¿€æ´»åˆ° flyai_agent_in_action ç¯å¢ƒã€‚\n",
      "âœ… æ­£åœ¨ä½¿ç”¨çš„ç¯å¢ƒè·¯å¾„: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "ğŸ’¡ æç¤º: åç»­çš„ Python å•å…ƒæ ¼å°†ä½¿ç”¨ Notebook å½“å‰é€‰æ‹©çš„ Jupyter å†…æ ¸ã€‚\n",
      "   å¦‚æœéœ€è¦åç»­å•å…ƒæ ¼ä¹Ÿä½¿ç”¨æ­¤ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:\n",
      "   1. æ£€æŸ¥ Notebook å³ä¸Šè§’æ˜¯å¦å·²é€‰æ‹© 'python(flyai_agent_in_action)'ã€‚\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. æ¿€æ´» conda ç¯å¢ƒ (ä»…å¯¹å½“å‰å•å…ƒæ ¼æœ‰æ•ˆ)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda ç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š (ä»…é’ˆå¯¹å½“å‰ Bash å­è¿›ç¨‹) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. æ£€æŸ¥å½“å‰æ¿€æ´»çš„ç¯å¢ƒ\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"âœ… å½“å‰å•å…ƒæ ¼å·²æˆåŠŸæ¿€æ´»åˆ° flyai_agent_in_action ç¯å¢ƒã€‚\"\n",
    "    echo \"âœ… æ­£åœ¨ä½¿ç”¨çš„ç¯å¢ƒè·¯å¾„: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"ğŸ’¡ æç¤º: åç»­çš„ Python å•å…ƒæ ¼å°†ä½¿ç”¨ Notebook å½“å‰é€‰æ‹©çš„ Jupyter å†…æ ¸ã€‚\"\n",
    "    echo \"   å¦‚æœéœ€è¦åç»­å•å…ƒæ ¼ä¹Ÿä½¿ç”¨æ­¤ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:\"\n",
    "    echo \"   1. æ£€æŸ¥ Notebook å³ä¸Šè§’æ˜¯å¦å·²é€‰æ‹© 'python(flyai_agent_in_action)'ã€‚\"\n",
    "else\n",
    "    echo \"âŒ æ¿€æ´»å¤±è´¥æˆ–ç¯å¢ƒåç§°ä¸åŒ¹é…ã€‚å½“å‰ç¯å¢ƒ: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸ ä¸¥é‡æç¤º: å»ºè®®å°† Notebook çš„ Jupyter **å†…æ ¸ (Kernel)** åˆ‡æ¢ä¸º 'python(flyai_agent_in_action)'ã€‚\"\n",
    "    echo \"   (é€šå¸¸ä½äº Notebook å³ä¸Šè§’æˆ– 'å†…æ ¸' èœå•ä¸­)\"\n",
    "    echo \"\"\n",
    "    echo \"ğŸ“š å¤‡ç”¨æ–¹æ³• (ä¸æ¨è): å¦‚æœæ— æ³•åˆ‡æ¢å†…æ ¸ï¼Œåˆ™å¿…é¡»åœ¨**æ¯ä¸ª**ä»£ç å•å…ƒæ ¼çš„å¤´éƒ¨é‡å¤ä»¥ä¸‹å‘½ä»¤:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# å¿…é¡»åœ¨æ¯ä¸ªå•å…ƒæ ¼éƒ½æ‰§è¡Œ\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. è®¾ç½®pip ä¸ºæ¸…åæº\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. è®¾ç½®HuggingFaceä»£ç†\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# éªŒè¯ï¼šä½¿ç”¨shellå‘½ä»¤æ£€æŸ¥\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### ç¯å¢ƒä¿¡æ¯\n",
      "| é¡¹ç›®         | ä¿¡æ¯                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| æ“ä½œç³»ç»Ÿ     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU ä¿¡æ¯     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| å†…å­˜ä¿¡æ¯     | 2015.36 GB (Available: 1696.30 GB)                                    |\n",
      "| GPU ä¿¡æ¯     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA ä¿¡æ¯    | 12.6                                                                  |\n",
      "| Python ç‰ˆæœ¬  | 3.12.11                                                               |\n",
      "| Conda ç‰ˆæœ¬   | conda 25.7.0                                                          |\n",
      "| ç‰©ç†ç£ç›˜ç©ºé—´ | Total: 2014.78 GB, Used: 651.77 GB, Free: 1260.59 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
    "#\n",
    "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
    "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
    "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
    "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
    "#\n",
    "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©æ‚¨ï¼š\n",
    "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
    "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
    "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "\n",
    "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
    "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
    "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
    "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
    "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
    "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
    "        try:\n",
    "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
    "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "\n",
    "\n",
    "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    if platform.system() == \"Windows\":\n",
    "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
    "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
    "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
    "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
    "\n",
    "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
    "        else:\n",
    "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "\n",
    "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
    "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
    "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_python_version():\n",
    "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
    "\n",
    "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
    "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
    "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
    "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
    "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
    "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
    "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
    "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
    "                     # å°è¯•è·å– codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
    "                     except:\n",
    "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
    "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
    "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
    "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
    "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
    "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "\n",
    "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "env_data = {\n",
    "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
    "        \"æ“ä½œç³»ç»Ÿ\",\n",
    "        \"CPU ä¿¡æ¯\",\n",
    "        \"å†…å­˜ä¿¡æ¯\",\n",
    "        \"GPU ä¿¡æ¯\",\n",
    "        \"CUDA ä¿¡æ¯\",\n",
    "        \"Python ç‰ˆæœ¬\",\n",
    "        \"Conda ç‰ˆæœ¬\",\n",
    "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
    "    ],\n",
    "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# æ‰“å°è¡¨æ ¼\n",
    "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
    "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b"
   },
   "source": [
    "# ç ”ç©¶åŠ©ç†ï¼ˆResearch Assistantï¼‰\n",
    "\n",
    "## å›é¡¾\n",
    "\n",
    "æœ¬èŠ‚å°†ç”¨åˆ° LangGraph çš„å‡ ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼š\n",
    "\n",
    "- è®°å¿†ï¼ˆMemoryï¼‰\n",
    "- äººæœºååŒï¼ˆHuman-in-the-loopï¼‰\n",
    "- å¯æ§æ€§ï¼ˆControllabilityï¼‰\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬æŠŠè¿™äº›ç†å¿µæ•´åˆèµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªéå¸¸å¸¸è§ä¸”å®ç”¨çš„ AI åº”ç”¨ï¼šç ”ç©¶è‡ªåŠ¨åŒ–ã€‚\n",
    "\n",
    "ä¼ ç»Ÿç ”ç©¶é€šå¸¸ç”±åˆ†æå¸ˆæ‰¿æ‹…ï¼Œå·¥ä½œç¹çã€è€—æ—¶ã€‚AI åœ¨è¿™æ–¹é¢æœ‰å¾ˆå¤§åŠ©åŠ›ç©ºé—´ã€‚\n",
    "\n",
    "ä½†ç ”ç©¶ç¯èŠ‚éœ€è¦å¼ºå®šåˆ¶åŒ–ï¼šç›´æ¥ä½¿ç”¨å¤§æ¨¡å‹åŸå§‹è¾“å‡ºï¼Œå¾€å¾€ä¸é€‚åˆçœŸå®çš„å†³ç­–æµç¨‹ã€‚\n",
    "\n",
    "åŸºäº AI çš„å®šåˆ¶åŒ–ã€Œç ”ç©¶ä¸æŠ¥å‘Šç”Ÿæˆã€å·¥ä½œæµï¼Œæ˜¯ä¸€ä¸ªå¯è¡Œä¸”æœ‰å‰æ™¯çš„æ–¹å‘ï¼ˆå‚è€ƒï¼š[Reports over RAG](https://jxnl.co/writing/2024/06/05/predictions-for-the-future-of-rag/#reports-over-rag)ï¼‰ã€‚\n",
    "\n",
    "## ç›®æ ‡\n",
    "\n",
    "æ„å»ºä¸€ä¸ªå›´ç»•èŠå¤©æ¨¡å‹çš„ã€Œè½»é‡çº§å¤šæ™ºèƒ½ä½“ã€ç³»ç»Ÿï¼Œç”¨äºå®šåˆ¶åŒ–ç ”ç©¶æµç¨‹ã€‚\n",
    "\n",
    "`æ•°æ®æºé€‰æ‹©ï¼ˆSource Selectionï¼‰`\n",
    "- ç”¨æˆ·å¯ä»¥è‡ªç”±é€‰æ‹©ä»»æ„ç ”ç©¶è¾“å…¥æºã€‚\n",
    "\n",
    "`è§„åˆ’ï¼ˆPlanningï¼‰`\n",
    "- ç”¨æˆ·ç»™å‡ºä¸»é¢˜ï¼Œç³»ç»Ÿç”Ÿæˆä¸€æ”¯ AI åˆ†æå¸ˆå›¢é˜Ÿï¼Œæ¯ä½åˆ†æå¸ˆè´Ÿè´£ä¸€ä¸ªå­ä¸»é¢˜ã€‚\n",
    "- åœ¨ç ”ç©¶å¼€å§‹å‰ï¼Œé€šè¿‡ã€ŒäººæœºååŒã€æ¥å¾®è°ƒ/ç¡®è®¤è¿™äº›å­ä¸»é¢˜ã€‚\n",
    "\n",
    "`å¤§æ¨¡å‹ä½¿ç”¨ï¼ˆLLM Utilizationï¼‰`\n",
    "- æ¯ä½åˆ†æå¸ˆä¼šåŸºäºæ‰€é€‰æ•°æ®æºï¼Œä¸ä¸“å®¶å‹ AI è¿›è¡Œæ·±å…¥ã€Œè®¿è°ˆã€ã€‚\n",
    "- è®¿è°ˆä¸ºå¤šè½®å¯¹è¯ï¼Œæ—¨åœ¨æŠ½å–æ›´å…·ä½“ã€æ›´æœ‰æ´è§çš„ä¿¡æ¯ï¼ˆç±»ä¼¼äº [STORM](https://arxiv.org/abs/2402.14207) è®ºæ–‡ä¸­çš„åšæ³•ï¼‰ã€‚\n",
    "- è¿™äº›è®¿è°ˆå°†ä½¿ç”¨å¸¦æœ‰å†…éƒ¨çŠ¶æ€çš„ `å­å›¾ï¼ˆsub-graphsï¼‰` æ¥æ‰¿è½½ä¸è¿½è¸ªã€‚\n",
    "\n",
    "`ç ”ç©¶è¿‡ç¨‹ï¼ˆResearch Processï¼‰`\n",
    "- ä¸“å®¶ä¼šå¹¶è¡Œæ”¶é›†ä¿¡æ¯ä»¥å›ç­”åˆ†æå¸ˆçš„é—®é¢˜ï¼ˆ`parallel`ï¼‰ã€‚\n",
    "- æ‰€æœ‰è®¿è°ˆå°†é€šè¿‡ `map-reduce` åŒæ­¥è¿›è¡Œä¸æ±‡æ€»ã€‚\n",
    "\n",
    "`è¾“å‡ºæ ¼å¼ï¼ˆOutput Formatï¼‰`\n",
    "- å°†æ¯æ¬¡è®¿è°ˆä¸­å¾—åˆ°çš„æ´è§è¿›è¡Œç»¼åˆï¼Œäº§å‡ºæœ€ç»ˆæŠ¥å‘Šã€‚\n",
    "- æŠ¥å‘Šä½¿ç”¨å¯å®šåˆ¶æç¤ºè¯ï¼Œä¾¿äºçµæ´»è°ƒæ•´è¾“å‡ºæ ¼å¼ã€‚\n",
    "\n",
    "![Screenshot 2024-08-26 at 7.26.33 PM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
   "metadata": {
    "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# å®‰è£…é¡¹ç›®æ‰€éœ€çš„PythonåŒ…\n",
    "# ä½¿ç”¨ %%capture --no-stderr æ¥éšè—å®‰è£…è¿‡ç¨‹ä¸­çš„è¾“å‡ºä¿¡æ¯ï¼Œä¿æŒnotebookæ•´æ´\n",
    "# %pip install --quiet -U langgraph langchain_openai langchain_community langchain_core tavily-python wikipedia\n",
    "%pip install --quiet langgraph==0.6.7 langchain_openai==0.3.32 langchain_community==0.3.29 langchain_core==0.3.75 tavily-python==0.7.12 wikipedia==1.4.0\n",
    "\n",
    "# åŒ…è¯´æ˜ï¼š\n",
    "# - langgraph: LangGraphæ¡†æ¶ï¼Œç”¨äºæ„å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµ\n",
    "# - langchain_openai: LangChainçš„OpenAIé›†æˆï¼Œç”¨äºè°ƒç”¨GPTæ¨¡å‹\n",
    "# - langchain_community: LangChainç¤¾åŒºå·¥å…·é›†ï¼ŒåŒ…å«å„ç§ç¬¬ä¸‰æ–¹é›†æˆ\n",
    "# - langchain_core: LangChainæ ¸å¿ƒç»„ä»¶ï¼Œæä¾›åŸºç¡€åŠŸèƒ½\n",
    "# - tavily-python: Tavilyæœç´¢APIå®¢æˆ·ç«¯ï¼Œç”¨äºç½‘ç»œæœç´¢\n",
    "# - wikipedia: ç»´åŸºç™¾ç§‘APIå®¢æˆ·ç«¯ï¼Œç”¨äºè·å–ç»´åŸºç™¾ç§‘å†…å®¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {
    "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914"
   },
   "source": [
    "## ç¯å¢ƒå‡†å¤‡ï¼ˆSetupï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
    "outputId": "d651b4af-0c90-457a-f4c7-33c8d2b91688"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "OPENAI_BASE_URL:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    å®‰å…¨è®¾ç½®ç¯å¢ƒå˜é‡çš„è¾…åŠ©å‡½æ•°\n",
    "\n",
    "    å‚æ•°:\n",
    "        var (str): ç¯å¢ƒå˜é‡åç§°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        - æ£€æŸ¥ç¯å¢ƒå˜é‡æ˜¯å¦å·²å­˜åœ¨\n",
    "        - å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™é€šè¿‡getpasså®‰å…¨åœ°è·å–ç”¨æˆ·è¾“å…¥\n",
    "        - å°†ç”¨æˆ·è¾“å…¥è®¾ç½®ä¸ºç¯å¢ƒå˜é‡å€¼\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# è®¾ç½® OpenAI API å¯†é’¥\n",
    "# è¿™æ˜¯ä½¿ç”¨ OpenAI æ¨¡å‹æ‰€å¿…éœ€çš„ï¼Œç”¨äºèº«ä»½éªŒè¯\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# è®¾ç½® OpenAI APIä»£ç†åœ°å€ (ä¾‹å¦‚ï¼šhttps://api.apiyi.com/v1ï¼‰\n",
    "# ç”¨äºé…ç½®APIè¯·æ±‚çš„åŸºç¡€URLï¼Œæ”¯æŒä½¿ç”¨ä»£ç†æœåŠ¡\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {
    "id": "afe9ff57-0826-4669-b88b-4d0501a509f5"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# åˆå§‹åŒ–OpenAIèŠå¤©æ¨¡å‹\n",
    "# ä½¿ç”¨GPT-4oæ¨¡å‹ï¼Œè¿™æ˜¯OpenAIæœ€æ–°çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹\n",
    "# temperature=0 ç¡®ä¿è¾“å‡ºç»“æœå…·æœ‰ç¡®å®šæ€§å’Œä¸€è‡´æ€§ï¼Œé€‚åˆéœ€è¦ç¨³å®šè¾“å‡ºçš„åœºæ™¯\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "id": "3419257b-2c6b-4d68-ae38-4a266cc02982"
   },
   "source": [
    "æˆ‘ä»¬å°†ä½¿ç”¨ [LangSmith](https://docs.smith.langchain.com/) è¿›è¡Œ[é“¾è·¯è¿½è¸ªï¼ˆtracingï¼‰](https://docs.smith.langchain.com/concepts/tracing)ï¼Œä¾¿äºè°ƒè¯•ä¸åˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
    "outputId": "9ef35fad-1514-41ed-e436-bdf87699f20e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®LangSmithè¿½è¸ªé…ç½®\n",
    "# LangSmithæ˜¯LangChainçš„å®˜æ–¹ç›‘æ§å’Œè°ƒè¯•å¹³å°\n",
    "_set_env(\"LANGSMITH_API_KEY\")  # è®¾ç½®LangSmith APIå¯†é’¥\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # å¯ç”¨é“¾è·¯è¿½è¸ªåŠŸèƒ½\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\" # è®¾ç½®é¡¹ç›®åç§°ï¼Œç”¨äºç»„ç»‡è¿½è¸ªæ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea",
   "metadata": {
    "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea"
   },
   "source": [
    "## åˆ†æå¸ˆï¼šäººæœºååŒï¼ˆHuman-In-The-Loopï¼‰\n",
    "\n",
    "é€šè¿‡äººæœºååŒçš„æ–¹å¼ç”Ÿæˆå¹¶å®¡æ ¸åˆ†æå¸ˆè§’è‰²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e",
   "metadata": {
    "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"\n",
    "    åˆ†æå¸ˆæ•°æ®æ¨¡å‹\n",
    "\n",
    "    ç”¨äºå®šä¹‰æ¯ä¸ªAIåˆ†æå¸ˆçš„åŸºæœ¬ä¿¡æ¯å’Œè§’è‰²ç‰¹å¾\n",
    "    æ¯ä¸ªåˆ†æå¸ˆä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„ç ”ç©¶è§†è§’å’Œä¸“é•¿é¢†åŸŸ\n",
    "    \"\"\"\n",
    "    affiliation: str = Field(\n",
    "        description=\"åˆ†æå¸ˆçš„ä¸»è¦éš¶å±æœºæ„æˆ–ç»„ç»‡\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"åˆ†æå¸ˆå§“å\"\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"åˆ†æå¸ˆåœ¨ç ”ç©¶ä¸»é¢˜ä¸­çš„å…·ä½“è§’è‰²å®šä½\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"åˆ†æå¸ˆçš„å…³æ³¨ç„¦ç‚¹ã€å…³åˆ‡ç‚¹å’ŒåŠ¨æœºçš„è¯¦ç»†æè¿°\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆåˆ†æå¸ˆäººè®¾æè¿°\n",
    "\n",
    "        è¿”å›:\n",
    "            str: æ ¼å¼åŒ–çš„åˆ†æå¸ˆäººè®¾ä¿¡æ¯ï¼Œç”¨äºåç»­çš„AIå¯¹è¯ä¸­\n",
    "        \"\"\"\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    \"\"\"\n",
    "    åˆ†æå¸ˆé›†åˆæ•°æ®æ¨¡å‹\n",
    "\n",
    "    ç”¨äºå­˜å‚¨å’Œç®¡ç†å¤šä¸ªåˆ†æå¸ˆçš„ä¿¡æ¯\n",
    "    æ”¯æŒç»“æ„åŒ–è¾“å‡ºï¼Œç¡®ä¿AIç”Ÿæˆçš„åˆ†æå¸ˆä¿¡æ¯æ ¼å¼æ­£ç¡®\n",
    "    \"\"\"\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"åŒ…å«æ‰€æœ‰åˆ†æå¸ˆè§’è‰²å’Œéš¶å±æœºæ„çš„ç»¼åˆåˆ—è¡¨\",\n",
    "    )\n",
    "\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    \"\"\"\n",
    "    åˆ†æå¸ˆç”ŸæˆçŠ¶æ€ç®¡ç†\n",
    "\n",
    "    ç”¨äºåœ¨LangGraphå·¥ä½œæµä¸­ç®¡ç†åˆ†æå¸ˆç”Ÿæˆè¿‡ç¨‹çš„çŠ¶æ€ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    topic: str  # ç ”ç©¶ä¸»é¢˜\n",
    "    max_analysts: int  # åˆ†æå¸ˆæ•°é‡ä¸Šé™\n",
    "    human_analyst_feedback: str  # äººç±»åé¦ˆä¿¡æ¯ï¼Œç”¨äºäººæœºååŒè°ƒæ•´\n",
    "    analysts: List[Analyst]  # ç”Ÿæˆçš„åˆ†æå¸ˆåˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
    "outputId": "a7d4bbfb-b9e1-4c81-d490-f8f9214425ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›¾å¯è§†åŒ–ï¼š\n",
      "âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "ğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tcreate_analysts(create_analysts)\n",
      "\thuman_feedback(human_feedback<hr/><small><em>__interrupt = before</em></small>)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> create_analysts;\n",
      "\tcreate_analysts --> human_feedback;\n",
      "\thuman_feedback -.-> __end__;\n",
      "\thuman_feedback -.-> create_analysts;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\n",
      "èŠ‚ç‚¹: ['__start__', 'create_analysts', 'human_feedback', '__end__']\n",
      "è¾¹: [Edge(source='__start__', target='create_analysts', data=None, conditional=False), Edge(source='create_analysts', target='human_feedback', data=None, conditional=False), Edge(source='human_feedback', target='__end__', data=None, conditional=True), Edge(source='human_feedback', target='create_analysts', data=None, conditional=True)]\n",
      "\n",
      "ğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\n",
      "1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\n",
      "2. è®¿é—® https://mermaid.live/\n",
      "3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\n",
      "4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# åˆ†æå¸ˆç”ŸæˆæŒ‡ä»¤æ¨¡æ¿\n",
    "# è¿™ä¸ªæç¤ºè¯æŒ‡å¯¼AIå¦‚ä½•æ ¹æ®ç ”ç©¶ä¸»é¢˜åˆ›å»ºåˆé€‚çš„åˆ†æå¸ˆå›¢é˜Ÿ\n",
    "analyst_instructions=\"\"\"ä½ éœ€è¦åˆ›å»ºä¸€ç»„ AI åˆ†æå¸ˆäººè®¾ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æŒ‡å¼•ï¼š\n",
    "\n",
    "1. å…ˆå®¡é˜…ç ”ç©¶ä¸»é¢˜ï¼š\n",
    "{topic}\n",
    "\n",
    "2. æŸ¥çœ‹ï¼ˆå¯é€‰çš„ï¼‰ç¼–è¾‘åé¦ˆï¼Œå®ƒå°†æŒ‡å¯¼åˆ†æå¸ˆçš„äººè®¾åˆ›å»ºï¼š\n",
    "\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. åŸºäºä¸Šè¿°æ–‡æ¡£ä¸/æˆ–åé¦ˆï¼Œè¯†åˆ«æœ€å€¼å¾—å…³æ³¨çš„ä¸»é¢˜ã€‚\n",
    "\n",
    "4. é€‰å‡ºå‰ {max_analysts} ä¸ªä¸»é¢˜ã€‚\n",
    "\n",
    "5. ä¸ºæ¯ä¸ªä¸»é¢˜åˆ†é…ä¸€ä½åˆ†æå¸ˆã€‚\"\"\"\n",
    "\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºåˆ†æå¸ˆäººè®¾çš„æ ¸å¿ƒå‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. æ ¹æ®ç ”ç©¶ä¸»é¢˜å’Œäººç±»åé¦ˆç”Ÿæˆåˆ†æå¸ˆå›¢é˜Ÿ\n",
    "        2. ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç¡®ä¿ç”Ÿæˆçš„åˆ†æå¸ˆä¿¡æ¯æ ¼å¼æ­£ç¡®\n",
    "        3. å°†ç”Ÿæˆçš„åˆ†æå¸ˆä¿¡æ¯å­˜å‚¨åˆ°çŠ¶æ€ä¸­\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«ç ”ç©¶ä¸»é¢˜ã€åˆ†æå¸ˆæ•°é‡é™åˆ¶å’Œäººç±»åé¦ˆçš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«ç”Ÿæˆçš„åˆ†æå¸ˆåˆ—è¡¨çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # ä»çŠ¶æ€ä¸­æå–å¿…è¦ä¿¡æ¯\n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
    "\n",
    "    # é…ç½®ç»“æ„åŒ–è¾“å‡ºï¼Œç¡®ä¿è¿”å›Perspectivesæ ¼å¼çš„æ•°æ®\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«ç ”ç©¶ä¸»é¢˜ã€åé¦ˆå’Œæ•°é‡é™åˆ¶\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts\n",
    "    )\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆåˆ†æå¸ˆé›†åˆ\n",
    "    analysts = structured_llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=\"ç”Ÿæˆåˆ†æå¸ˆé›†åˆã€‚\")\n",
    "    ])\n",
    "\n",
    "    # å°†åˆ†æå¸ˆåˆ—è¡¨å†™å…¥çŠ¶æ€ï¼Œä¾›åç»­èŠ‚ç‚¹ä½¿ç”¨\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    äººæœºååŒä¸­æ–­ç‚¹èŠ‚ç‚¹\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        - ä½œä¸ºå·¥ä½œæµçš„ä¸­æ–­ç‚¹ï¼Œå…è®¸äººç±»å®¡æŸ¥å’Œä¿®æ”¹ç”Ÿæˆçš„åˆ†æå¸ˆ\n",
    "        - è¿™æ˜¯ä¸€ä¸ªç©ºæ“ä½œèŠ‚ç‚¹ï¼Œä¸»è¦ç”¨äºæµç¨‹æ§åˆ¶\n",
    "        - äººç±»å¯ä»¥åœ¨æ­¤èŠ‚ç‚¹æä¾›åé¦ˆï¼Œç³»ç»Ÿä¼šæ ¹æ®åé¦ˆé‡æ–°ç”Ÿæˆåˆ†æå¸ˆ\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: å½“å‰çŠ¶æ€å¯¹è±¡\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    æ¡ä»¶è·¯ç”±å‡½æ•°ï¼šå†³å®šå·¥ä½œæµçš„ä¸‹ä¸€æ­¥æ‰§è¡Œ\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        - æ£€æŸ¥æ˜¯å¦æœ‰äººç±»åé¦ˆ\n",
    "        - å¦‚æœæœ‰åé¦ˆï¼Œé‡æ–°ç”Ÿæˆåˆ†æå¸ˆ\n",
    "        - å¦‚æœæ²¡æœ‰åé¦ˆï¼Œç»“æŸæµç¨‹\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: å½“å‰çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        str: ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹åç§°\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰äººç±»åé¦ˆ\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', None)\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"  # æœ‰åé¦ˆï¼Œé‡æ–°ç”Ÿæˆåˆ†æå¸ˆ\n",
    "\n",
    "    # æ²¡æœ‰åé¦ˆï¼Œç»“æŸæµç¨‹\n",
    "    return END\n",
    "\n",
    "# æ„å»ºLangGraphå·¥ä½œæµ\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹åˆ°å·¥ä½œæµ\n",
    "builder.add_node(\"create_analysts\", create_analysts)  # åˆ†æå¸ˆç”ŸæˆèŠ‚ç‚¹\n",
    "builder.add_node(\"human_feedback\", human_feedback)    # äººç±»åé¦ˆèŠ‚ç‚¹\n",
    "\n",
    "# æ·»åŠ è¾¹è¿æ¥èŠ‚ç‚¹\n",
    "builder.add_edge(START, \"create_analysts\")  # å¼€å§‹ -> ç”Ÿæˆåˆ†æå¸ˆ\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")  # ç”Ÿæˆåˆ†æå¸ˆ -> äººç±»åé¦ˆ\n",
    "\n",
    "# æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®æ˜¯å¦æœ‰åé¦ˆå†³å®šä¸‹ä¸€æ­¥\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    should_continue,\n",
    "    [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# ç¼–è¯‘å·¥ä½œæµ\n",
    "memory = MemorySaver()  # ä½¿ç”¨å†…å­˜æ£€æŸ¥ç‚¹ä¿å­˜çŠ¶æ€\n",
    "graph = builder.compile(\n",
    "    interrupt_before=['human_feedback'],  # åœ¨äººç±»åé¦ˆèŠ‚ç‚¹å‰ä¸­æ–­\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# å±•ç¤ºå›¾ç»“æ„\n",
    "# å›¾å¯è§†åŒ–\n",
    "print(\"å›¾å¯è§†åŒ–ï¼š\")\n",
    "\n",
    "# æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ Pyppeteer æœ¬åœ°æ¸²æŸ“ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    # å¯è§†åŒ–ï¼šé€šè¿‡ Mermaid æ¸²æŸ“å›¾ç»“æ„\n",
    "    display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "    print(\"âœ… å›¾æ¸²æŸ“æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: {e}\")\n",
    "    \n",
    "    # æ–¹æ¡ˆ2ï¼šæ˜¾ç¤º Mermaid æ–‡æœ¬æ ¼å¼\n",
    "    print(\"\\nğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ–¹æ¡ˆ3ï¼šæ˜¾ç¤ºå›¾çš„èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯\n",
    "    print(\"\\nğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\")\n",
    "    print(\"èŠ‚ç‚¹:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"è¾¹:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # æ–¹æ¡ˆ4ï¼šæä¾›æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜\n",
    "    print(\"\\nğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\")\n",
    "    print(\"1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\")\n",
    "    print(\"2. è®¿é—® https://mermaid.live/\")\n",
    "    print(\"3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\")\n",
    "    print(\"4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd42b0a-188c-4374-a03c-a2ca8272cdcf",
   "metadata": {},
   "source": [
    "![image-20250930152238678](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301522726.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
    "outputId": "ca196d63-2a1c-4019-b769-e85da2f5c873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Chen\n",
      "Affiliation: AI Research Institute\n",
      "Role: æŠ€æœ¯ä¸“å®¶\n",
      "Description: ä¸“æ³¨äºLangGraphæ¡†æ¶çš„æŠ€æœ¯ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å…¶åœ¨å¤„ç†å¤æ‚è¯­è¨€æ¨¡å‹æ—¶çš„æ€§èƒ½æå‡å’Œæ•ˆç‡ä¼˜åŒ–ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Smith\n",
      "Affiliation: Global AI Solutions\n",
      "Role: å•†ä¸šåˆ†æå¸ˆ\n",
      "Description: ç ”ç©¶LangGraphåœ¨å•†ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨åœºæ™¯å’Œå¸‚åœºä»·å€¼ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Prof. Linda Johnson\n",
      "Affiliation: University of Technology\n",
      "Role: æ•™è‚²ç ”ç©¶å‘˜\n",
      "Description: å…³æ³¨LangGraphæ¡†æ¶çš„æ•™è‚²å’Œå­¦æœ¯å½±å“ï¼Œæ¢è®¨å…¶åœ¨AIè¯¾ç¨‹ä¸­çš„åº”ç”¨ä»¥åŠå¯¹å­¦ç”Ÿå­¦ä¹ çš„ä¿ƒè¿›ä½œç”¨ã€‚\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "max_analysts = 3\n",
    "topic = \"é‡‡ç”¨LangGraphä½œä¸ºAI Agentæ¡†æ¶çš„å¥½å¤„\"\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Run the graph until the first interruption\n",
    "for event in graph.stream({\"topic\":topic,\"max_analysts\":max_analysts,}, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
    "outputId": "f2d95e38-5acf-4c43-8974-a637aac666ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('human_feedback',)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get state and look at next node\n",
    "state = graph.get_state(thread)\n",
    "state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
    "outputId": "d5bb8423-a691-4567-83c1-4e1d072d54bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09dce5-c346-678c-8002-07893291b6dd'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now update the state as if we are the human_feedback node\n",
    "graph.update_state(thread, {\"human_analyst_feedback\":\n",
    "      \"åŠ å…¥ä¸€ä½æ¥è‡ªåˆåˆ›å…¬å¸çš„äººï¼Œä»¥å¢åŠ åˆ›ä¸šè€…çš„è§†è§’\"}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8816eb9-9906-441b-b552-be71107db14f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8816eb9-9906-441b-b552-be71107db14f",
    "outputId": "c250ffd5-e4a2-441b-851e-28e76f049f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Chen\n",
      "Affiliation: AI Research Institute\n",
      "Role: æŠ€æœ¯ä¸“å®¶\n",
      "Description: ä¸“æ³¨äºLangGraphæ¡†æ¶çš„æŠ€æœ¯ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å…¶åœ¨å¤„ç†å¤æ‚è¯­è¨€æ¨¡å‹æ—¶çš„æ€§èƒ½æå‡å’Œæ•ˆç‡ä¼˜åŒ–ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Smith\n",
      "Affiliation: Global AI Solutions\n",
      "Role: å•†ä¸šåˆ†æå¸ˆ\n",
      "Description: ç ”ç©¶LangGraphåœ¨å•†ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„åº”ç”¨åœºæ™¯å’Œå¸‚åœºä»·å€¼ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Prof. Linda Johnson\n",
      "Affiliation: University of Technology\n",
      "Role: æ•™è‚²ç ”ç©¶å‘˜\n",
      "Description: å…³æ³¨LangGraphæ¡†æ¶çš„æ•™è‚²å’Œå­¦æœ¯å½±å“ï¼Œæ¢è®¨å…¶åœ¨AIè¯¾ç¨‹ä¸­çš„åº”ç”¨ä»¥åŠå¯¹å­¦ç”Ÿå­¦ä¹ çš„ä¿ƒè¿›ä½œç”¨ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Dr. Emily Zhang\n",
      "Affiliation: Tech Research Institute\n",
      "Role: æŠ€æœ¯ä¸“å®¶\n",
      "Description: ä¸“æ³¨äºLangGraphæ¡†æ¶åœ¨æŠ€æœ¯ä¸Šçš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å…¶åœ¨æ•°æ®å¤„ç†å’Œåˆ†æä¸­çš„é«˜æ•ˆæ€§ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Doe\n",
      "Affiliation: Business Strategy Group\n",
      "Role: å•†ä¸šåˆ†æå¸ˆ\n",
      "Description: ç ”ç©¶LangGraphæ¡†æ¶åœ¨å•†ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨æé«˜ä¼ä¸šå†³ç­–æ•ˆç‡æ–¹é¢çš„è´¡çŒ®ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Ms. Lisa Wang\n",
      "Affiliation: Startup Innovators\n",
      "Role: åˆ›ä¸šè€…\n",
      "Description: ä»åˆ›ä¸šè€…çš„è§’åº¦åˆ†æLangGraphæ¡†æ¶çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠå®ƒå¦‚ä½•å¸®åŠ©åˆåˆ›å…¬å¸å¿«é€Ÿå®ç°äº§å“è¿­ä»£ã€‚\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Continue the graph execution\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    # Review\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43ac322-5926-4932-8653-68206fec0d2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a43ac322-5926-4932-8653-68206fec0d2c",
    "outputId": "771be8c3-bb76-4d6f-e00b-06afcf620aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f09dce5-dc1d-6c45-8004-cdef70574b29'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¦‚æœæˆ‘ä»¬æ»¡æ„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±ç®€å•åœ°ä¸æä¾›åé¦ˆ\n",
    "further_feedack = None\n",
    "graph.update_state(thread, {\"human_analyst_feedback\":\n",
    "                            further_feedack}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab034e65-aeee-4723-8d6d-74541b548425",
   "metadata": {
    "id": "ab034e65-aeee-4723-8d6d-74541b548425"
   },
   "outputs": [],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f204e8a-285c-4e46-8223-a695caec7764",
   "metadata": {
    "id": "2f204e8a-285c-4e46-8223-a695caec7764"
   },
   "outputs": [],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get('analysts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
    "outputId": "d16bd1df-ec14-43c0-aa4e-8440c4bef1e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
    "outputId": "bd20945d-6717-4ee7-c981-36ffb06f9e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dr. Emily Zhang\n",
      "Affiliation: Tech Research Institute\n",
      "Role: æŠ€æœ¯ä¸“å®¶\n",
      "Description: ä¸“æ³¨äºLangGraphæ¡†æ¶åœ¨æŠ€æœ¯ä¸Šçš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å…¶åœ¨æ•°æ®å¤„ç†å’Œåˆ†æä¸­çš„é«˜æ•ˆæ€§ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Mr. John Doe\n",
      "Affiliation: Business Strategy Group\n",
      "Role: å•†ä¸šåˆ†æå¸ˆ\n",
      "Description: ç ”ç©¶LangGraphæ¡†æ¶åœ¨å•†ä¸šåº”ç”¨ä¸­çš„æ½œåŠ›ï¼Œç‰¹åˆ«æ˜¯å…¶åœ¨æé«˜ä¼ä¸šå†³ç­–æ•ˆç‡æ–¹é¢çš„è´¡çŒ®ã€‚\n",
      "--------------------------------------------------\n",
      "Name: Ms. Lisa Wang\n",
      "Affiliation: Startup Innovators\n",
      "Role: åˆ›ä¸šè€…\n",
      "Description: ä»åˆ›ä¸šè€…çš„è§’åº¦åˆ†æLangGraphæ¡†æ¶çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä»¥åŠå®ƒå¦‚ä½•å¸®åŠ©åˆåˆ›å…¬å¸å¿«é€Ÿå®ç°äº§å“è¿­ä»£ã€‚\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7",
   "metadata": {
    "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7"
   },
   "source": [
    "## è¿›è¡Œè®¿è°ˆï¼ˆConduct Interviewï¼‰\n",
    "\n",
    "### ç”Ÿæˆé—®é¢˜ï¼ˆGenerate Questionï¼‰\n",
    "\n",
    "åˆ†æå¸ˆå°†å‘ä¸“å®¶æå‡ºé—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c",
   "metadata": {
    "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    \"\"\"\n",
    "    è®¿è°ˆçŠ¶æ€ç®¡ç†ç±»\n",
    "\n",
    "    ç»§æ‰¿è‡ªMessagesStateï¼Œç”¨äºç®¡ç†åˆ†æå¸ˆä¸ä¸“å®¶ä¹‹é—´çš„å¯¹è¯çŠ¶æ€\n",
    "    åŒ…å«è®¿è°ˆè¿‡ç¨‹ä¸­çš„æ‰€æœ‰å¿…è¦ä¿¡æ¯å’Œä¸Šä¸‹æ–‡\n",
    "    \"\"\"\n",
    "    max_num_turns: int  # å¯¹è¯è½®æ¬¡ä¸Šé™ï¼Œæ§åˆ¶è®¿è°ˆæ·±åº¦\n",
    "    context: Annotated[list, operator.add]  # æ£€ç´¢åˆ°çš„æºæ–‡æ¡£åˆ—è¡¨ï¼Œä½¿ç”¨operator.addè¿›è¡Œç´¯åŠ \n",
    "    analyst: Analyst  # å½“å‰è¿›è¡Œè®¿è°ˆçš„åˆ†æå¸ˆå¯¹è±¡\n",
    "    interview: str  # å®Œæ•´çš„è®¿è°ˆè®°å½•æ–‡æœ¬\n",
    "    sections: list  # è®¿è°ˆæ‘˜è¦å°èŠ‚åˆ—è¡¨ï¼Œç”¨äºæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    æœç´¢æŸ¥è¯¢æ•°æ®æ¨¡å‹\n",
    "\n",
    "    ç”¨äºç»“æ„åŒ–ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œç¡®ä¿æœç´¢è¯·æ±‚æ ¼å¼æ­£ç¡®\n",
    "    \"\"\"\n",
    "    search_query: str = Field(None, description=\"ç”¨äºæ£€ç´¢çš„æœç´¢æŸ¥è¯¢è¯­å¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0",
   "metadata": {
    "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0"
   },
   "outputs": [],
   "source": [
    "# é—®é¢˜ç”ŸæˆæŒ‡ä»¤æ¨¡æ¿\n",
    "# æŒ‡å¯¼AIåˆ†æå¸ˆå¦‚ä½•ä¸ä¸“å®¶è¿›è¡Œæœ‰æ•ˆçš„è®¿è°ˆå¯¹è¯\n",
    "question_instructions = \"\"\"ä½ æ˜¯ä¸€ååˆ†æå¸ˆï¼Œéœ€è¦é€šè¿‡è®¿è°ˆä¸“å®¶æ¥äº†è§£ä¸€ä¸ªå…·ä½“ä¸»é¢˜ã€‚\n",
    "\n",
    "ä½ çš„ç›®æ ‡æ˜¯æç‚¼ä¸è¯¥ä¸»é¢˜ç›¸å…³çš„ã€Œæœ‰è¶£ä¸”å…·ä½“ã€çš„æ´è§ã€‚\n",
    "\n",
    "1. æœ‰è¶£ï¼ˆInterestingï¼‰ï¼šè®©äººæ„Ÿåˆ°æ„å¤–æˆ–éæ˜¾è€Œæ˜“è§çš„è§‚ç‚¹ã€‚\n",
    "\n",
    "2. å…·ä½“ï¼ˆSpecificï¼‰ï¼šé¿å…æ³›æ³›è€Œè°ˆï¼ŒåŒ…å«ä¸“å®¶æä¾›çš„å…·ä½“æ¡ˆä¾‹æˆ–ç»†èŠ‚ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä½ çš„å…³æ³¨ä¸»é¢˜ä¸ç›®æ ‡è®¾å®šï¼š{goals}\n",
    "\n",
    "è¯·å…ˆç”¨ç¬¦åˆä½ äººè®¾çš„åå­—è¿›è¡Œè‡ªæˆ‘ä»‹ç»ï¼Œç„¶åæå‡ºä½ çš„ç¬¬ä¸€ä¸ªé—®é¢˜ã€‚\n",
    "\n",
    "æŒç»­è¿½é—®ï¼Œé€æ­¥æ·±å…¥ï¼Œé€æ­¥å®Œå–„ä½ å¯¹è¯¥ä¸»é¢˜çš„ç†è§£ã€‚\n",
    "\n",
    "å½“ä½ è®¤ä¸ºä¿¡æ¯å·²å……åˆ†ï¼Œè¯·ä»¥è¿™å¥è¯ç»“æŸè®¿è°ˆï¼šã€Œéå¸¸æ„Ÿè°¢æ‚¨çš„å¸®åŠ©!ã€\n",
    "\n",
    "è¯·å§‹ç»ˆä¿æŒä¸ä½ çš„äººè®¾ä¸ç›®æ ‡ä¸€è‡´çš„è¯´è¯æ–¹å¼ã€‚\"\"\"\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆè®¿è°ˆé—®é¢˜çš„æ ¸å¿ƒå‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. æ ¹æ®åˆ†æå¸ˆçš„äººè®¾å’Œå½“å‰å¯¹è¯å†å²ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜\n",
    "        2. ç¡®ä¿é—®é¢˜ç¬¦åˆåˆ†æå¸ˆçš„å…³æ³¨ç‚¹å’Œè§’è‰²å®šä½\n",
    "        3. ç»´æŠ¤å¯¹è¯çš„è¿è´¯æ€§å’Œæ·±åº¦\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«åˆ†æå¸ˆä¿¡æ¯å’Œå¯¹è¯å†å²çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ–°ç”Ÿæˆé—®é¢˜çš„æ¶ˆæ¯åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # ä»çŠ¶æ€ä¸­è·å–åˆ†æå¸ˆä¿¡æ¯å’Œå½“å‰å¯¹è¯å†å²\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«åˆ†æå¸ˆçš„äººè®¾ä¿¡æ¯\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªé—®é¢˜\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # å°†ç”Ÿæˆçš„é—®é¢˜æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ff33a-6232-4a79-8a82-882a645394f5",
   "metadata": {
    "id": "be2ff33a-6232-4a79-8a82-882a645394f5"
   },
   "source": [
    "### ç”Ÿæˆå›ç­”ï¼šå¹¶è¡ŒåŒ–ï¼ˆParallelizationï¼‰\n",
    "\n",
    "ä¸“å®¶å°†å¹¶è¡Œåœ°ä»å¤šä¸ªæ¥æºæ”¶é›†ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ï¼š\n",
    "\n",
    "- å…·ä½“ç½‘ç«™ï¼ˆä¾‹å¦‚é€šè¿‡ [`WebBaseLoader`](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/) æŠ“å–ï¼‰\n",
    "- å·²å»ºç«‹ç´¢å¼•çš„æ–‡æ¡£ï¼ˆä¾‹å¦‚åŸºäº [RAG](https://python.langchain.com/v0.2/docs/tutorials/rag/) çš„æ£€ç´¢ï¼‰\n",
    "- Web æœç´¢\n",
    "- ç»´åŸºç™¾ç§‘æœç´¢\n",
    "\n",
    "ä½ ä¹Ÿå¯ä»¥å°è¯•ä¸åŒçš„ Web æœç´¢å·¥å…·ï¼Œæ¯”å¦‚ [Tavily](https://tavily.com/)ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "606ea95b-e811-4299-8b66-835d4016c338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "606ea95b-e811-4299-8b66-835d4016c338",
    "outputId": "52495f61-eb30-4785-8b55-fdfc00eca71a"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    å®‰å…¨è®¾ç½®ç¯å¢ƒå˜é‡çš„è¾…åŠ©å‡½æ•°ï¼ˆé‡å¤å®šä¹‰ï¼Œä¿æŒä»£ç å®Œæ•´æ€§ï¼‰\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# è®¾ç½®Tavilyæœç´¢APIå¯†é’¥\n",
    "# Tavilyæ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºAIåº”ç”¨ä¼˜åŒ–çš„æœç´¢APIï¼Œæä¾›é«˜è´¨é‡çš„æœç´¢ç»“æœ\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
    "outputId": "01c8bb14-5945-4d7e-c81f-dcdaf6ec9a37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tmp/ipykernel_11236/1003905209.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "# ç½‘ç»œæœç´¢å·¥å…·é…ç½®\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# åˆå§‹åŒ–Tavilyæœç´¢å·¥å…·\n",
    "# max_results=3 é™åˆ¶æ¯æ¬¡æœç´¢è¿”å›çš„ç»“æœæ•°é‡ï¼Œå¹³è¡¡ä¿¡æ¯ä¸°å¯Œåº¦å’Œå¤„ç†æ•ˆç‡\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c",
   "metadata": {
    "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c"
   },
   "outputs": [],
   "source": [
    "# ç»´åŸºç™¾ç§‘æœç´¢å·¥å…·é…ç½®\n",
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb1603",
   "metadata": {
    "id": "06cb1603"
   },
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºç”¨äº Web ä¸ç»´åŸºç™¾ç§‘æ£€ç´¢çš„èŠ‚ç‚¹ã€‚\n",
    "\n",
    "è¿˜ä¼šåˆ›å»ºä¸€ä¸ªç”¨äºå›ç­”åˆ†æå¸ˆé—®é¢˜çš„èŠ‚ç‚¹ã€‚\n",
    "\n",
    "æœ€åï¼Œåˆ›å»ºç”¨äºä¿å­˜å®Œæ•´è®¿è°ˆå†…å®¹ï¼Œä»¥åŠæ’°å†™è®¿è°ˆæ‘˜è¦ï¼ˆâ€œsectionâ€ï¼‰çš„èŠ‚ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
    "outputId": "c0063b88-b11a-4fa3-e621-34dab72fa120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å›¾å¯è§†åŒ–ï¼š\n",
      "âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n",
      "\n",
      "ğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\n",
      "==================================================\n",
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\task_question(ask_question)\n",
      "\tsearch_web(search_web)\n",
      "\tsearch_wikipedia(search_wikipedia)\n",
      "\tanswer_question(answer_question)\n",
      "\tsave_interview(save_interview)\n",
      "\twrite_section(write_section)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> ask_question;\n",
      "\tanswer_question -.-> ask_question;\n",
      "\tanswer_question -.-> save_interview;\n",
      "\task_question --> search_web;\n",
      "\task_question --> search_wikipedia;\n",
      "\tsave_interview --> write_section;\n",
      "\tsearch_web --> answer_question;\n",
      "\tsearch_wikipedia --> answer_question;\n",
      "\twrite_section --> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\n",
      "èŠ‚ç‚¹: ['__start__', 'ask_question', 'search_web', 'search_wikipedia', 'answer_question', 'save_interview', 'write_section', '__end__']\n",
      "è¾¹: [Edge(source='__start__', target='ask_question', data=None, conditional=False), Edge(source='answer_question', target='ask_question', data=None, conditional=True), Edge(source='answer_question', target='save_interview', data=None, conditional=True), Edge(source='ask_question', target='search_web', data=None, conditional=False), Edge(source='ask_question', target='search_wikipedia', data=None, conditional=False), Edge(source='save_interview', target='write_section', data=None, conditional=False), Edge(source='search_web', target='answer_question', data=None, conditional=False), Edge(source='search_wikipedia', target='answer_question', data=None, conditional=False), Edge(source='write_section', target='__end__', data=None, conditional=False)]\n",
      "\n",
      "ğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\n",
      "1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\n",
      "2. è®¿é—® https://mermaid.live/\n",
      "3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\n",
      "4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "# æœç´¢æŸ¥è¯¢ç”ŸæˆæŒ‡ä»¤\n",
    "# æŒ‡å¯¼AIå¦‚ä½•ä»å¯¹è¯ä¸­æå–æœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢\n",
    "search_instructions = SystemMessage(content=f\"\"\"ä½ å°†è·å¾—ä¸€æ®µåˆ†æå¸ˆä¸ä¸“å®¶ä¹‹é—´çš„å¯¹è¯ã€‚\n",
    "\n",
    "ä½ çš„ç›®æ ‡æ˜¯åŸºäºè¿™æ®µå¯¹è¯ï¼Œä¸ºWebæœç´¢ç”Ÿæˆä¸€æ¡ç»“æ„è‰¯å¥½çš„æŸ¥è¯¢è¯­å¥ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œé€šè¯»æ•´æ®µå¯¹è¯ã€‚\n",
    "\n",
    "ç‰¹åˆ«å…³æ³¨åˆ†æå¸ˆæœ€åæå‡ºçš„é—®é¢˜ã€‚\n",
    "\n",
    "å°†è¿™ä¸ªæœ€ç»ˆé—®é¢˜è½¬åŒ–ä¸ºç»“æ„è‰¯å¥½çš„ Web æœç´¢æŸ¥è¯¢ã€‚\"\"\")\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"\n",
    "    é€šè¿‡Webæœç´¢æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. åˆ†æå½“å‰å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆåˆé€‚çš„æœç´¢æŸ¥è¯¢\n",
    "        2. ä½¿ç”¨Tavily APIæ‰§è¡Œç½‘ç»œæœç´¢\n",
    "        3. æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œä¾¿äºåç»­å¤„ç†\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«å¯¹è¯å†å²çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ ¼å¼åŒ–æœç´¢ç»“æœçš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆæœç´¢æŸ¥è¯¢\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    # æ‰§è¡ŒTavilyç½‘ç»œæœç´¢\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œæ·»åŠ æ¥æºä¿¡æ¯\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
    "        f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "        for doc in search_docs\n",
    "    ])\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    \"\"\"\n",
    "    é€šè¿‡ç»´åŸºç™¾ç§‘æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. åˆ†æå½“å‰å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆç»´åŸºç™¾ç§‘æœç´¢æŸ¥è¯¢\n",
    "        2. ä½¿ç”¨WikipediaLoaderè·å–ç»´åŸºç™¾ç§‘å†…å®¹\n",
    "        3. æ ¼å¼åŒ–æœç´¢ç»“æœï¼Œä¾¿äºåç»­å¤„ç†\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«å¯¹è¯å†å²çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ ¼å¼åŒ–ç»´åŸºç™¾ç§‘æœç´¢ç»“æœçš„ä¸Šä¸‹æ–‡ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç”Ÿæˆæœç´¢æŸ¥è¯¢\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    # æ‰§è¡Œç»´åŸºç™¾ç§‘æœç´¢ï¼Œé™åˆ¶æœ€å¤š2ä¸ªæ–‡æ¡£\n",
    "    search_docs = WikipediaLoader(\n",
    "        query=search_query.search_query,\n",
    "        load_max_docs=2\n",
    "    ).load()\n",
    "\n",
    "    # æ ¼å¼åŒ–ç»´åŸºç™¾ç§‘æœç´¢ç»“æœ\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
    "        f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "        for doc in search_docs\n",
    "    ])\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "# ä¸“å®¶å›ç­”æŒ‡ä»¤æ¨¡æ¿\n",
    "# æŒ‡å¯¼AIä¸“å®¶å¦‚ä½•åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”åˆ†æå¸ˆçš„é—®é¢˜\n",
    "answer_instructions = \"\"\"ä½ æ˜¯ä¸€ä½è¢«åˆ†æå¸ˆè®¿è°ˆçš„ä¸“å®¶ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯åˆ†æå¸ˆçš„å…³æ³¨é¢†åŸŸï¼š{goals}ã€‚\n",
    "\n",
    "ä½ çš„ç›®æ ‡æ˜¯å›ç­”è®¿è°ˆè€…æå‡ºçš„é—®é¢˜ã€‚\n",
    "\n",
    "å›ç­”é—®é¢˜æ—¶ï¼Œè¯·ä»…ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼š\n",
    "\n",
    "{context}\n",
    "\n",
    "å›ç­”é¡»éµå¾ªå¦‚ä¸‹è¦æ±‚ï¼š\n",
    "\n",
    "1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­æä¾›çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "2. ä¸è¦å¼•å…¥ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä¿¡æ¯ï¼Œä¹Ÿä¸è¦åšæœªåœ¨ä¸Šä¸‹æ–‡æ˜ç¡®è¯´æ˜çš„å‡è®¾ã€‚\n",
    "\n",
    "3. ä¸Šä¸‹æ–‡åœ¨æ¯æ®µæ–‡æ¡£é¡¶éƒ¨åŒ…å«æ¥æºä¿¡æ¯ã€‚\n",
    "\n",
    "4. åœ¨æ¶‰åŠå…·ä½“è®ºæ–­æ—¶ï¼Œè¯·åœ¨ç›¸åº”å†…å®¹æ—æ ‡æ³¨å¼•ç”¨æ¥æºç¼–å·ã€‚ä¾‹å¦‚ï¼Œé’ˆå¯¹æ¥æº 1 ä½¿ç”¨ [1]ã€‚\n",
    "\n",
    "5. åœ¨ç­”æ¡ˆç»“å°¾å¤„æŒ‰é¡ºåºåˆ—å‡ºå¼•ç”¨æ¥æºï¼Œå¦‚ï¼š[1] Source 1, [2] Source 2 ç­‰ã€‚\n",
    "\n",
    "6. è‹¥æ¥æºå½¢å¦‚ï¼š<Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>ï¼Œåˆ™åœ¨å¼•ç”¨åˆ—è¡¨ä¸­åªå†™ï¼š\n",
    "\n",
    "[1] assistant/docs/llama3_1.pdf, page 7\n",
    "\n",
    "å¹¶ä¸”ä¸è¦å†é‡å¤åŠ ä¸­æ‹¬å·ï¼Œä¹Ÿä¸è¦é™„åŠ  Document source å‰ç¼€ã€‚\"\"\"\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆä¸“å®¶å›ç­”çš„æ ¸å¿ƒå‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”åˆ†æå¸ˆçš„é—®é¢˜\n",
    "        2. ç¡®ä¿å›ç­”ç¬¦åˆä¸“å®¶çš„è§’è‰²å®šä½\n",
    "        3. æä¾›å‡†ç¡®çš„å¼•ç”¨å’Œæ¥æºä¿¡æ¯\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«åˆ†æå¸ˆä¿¡æ¯ã€å¯¹è¯å†å²å’Œæ£€ç´¢ä¸Šä¸‹æ–‡çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«ä¸“å®¶å›ç­”çš„æ¶ˆæ¯åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # ä»çŠ¶æ€ä¸­è·å–å¿…è¦ä¿¡æ¯\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«åˆ†æå¸ˆå…³æ³¨ç‚¹å’Œæ£€ç´¢ä¸Šä¸‹æ–‡\n",
    "    system_message = answer_instructions.format(\n",
    "        goals=analyst.persona,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆä¸“å®¶å›ç­”\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # æ ‡è®°è¯¥æ¶ˆæ¯æ¥è‡ªä¸“å®¶ï¼Œä¾¿äºåç»­è·¯ç”±\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # å°†ä¸“å®¶å›ç­”æ·»åŠ åˆ°æ¶ˆæ¯å†å²ä¸­\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"\n",
    "    ä¿å­˜è®¿è°ˆå†…å®¹çš„å‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. å°†å®Œæ•´çš„å¯¹è¯å†å²è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼\n",
    "        2. ä¿å­˜è®¿è°ˆè®°å½•ï¼Œä¾›åç»­æŠ¥å‘Šç”Ÿæˆä½¿ç”¨\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«å¯¹è¯å†å²çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«å®Œæ•´è®¿è°ˆè®°å½•çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # è·å–æ‰€æœ‰å¯¹è¯æ¶ˆæ¯\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„å­—ç¬¦ä¸²\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # å°†è®¿è°ˆè®°å½•ä¿å­˜åˆ°çŠ¶æ€ä¸­\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"\n",
    "    æ¶ˆæ¯è·¯ç”±å‡½æ•°ï¼šå†³å®šè®¿è°ˆæµç¨‹çš„ä¸‹ä¸€æ­¥\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¯¹è¯è½®æ¬¡\n",
    "        2. æ£€æŸ¥åˆ†æå¸ˆæ˜¯å¦è¡¨ç¤ºè®¿è°ˆç»“æŸ\n",
    "        3. å†³å®šæ˜¯ç»§ç»­æé—®è¿˜æ˜¯ä¿å­˜è®¿è°ˆ\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: å½“å‰è®¿è°ˆçŠ¶æ€\n",
    "        name: ä¸“å®¶æ¶ˆæ¯çš„æ ‡è¯†ç¬¦ï¼Œé»˜è®¤ä¸º\"expert\"\n",
    "\n",
    "    è¿”å›:\n",
    "        str: ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹åç§°\n",
    "    \"\"\"\n",
    "    # è·å–å¯¹è¯æ¶ˆæ¯å’Œæœ€å¤§è½®æ¬¡è®¾ç½®\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns', 2)\n",
    "\n",
    "    # ç»Ÿè®¡ä¸“å®¶å›ç­”æ¬¡æ•°\n",
    "    num_responses = len([\n",
    "        m for m in messages\n",
    "        if isinstance(m, AIMessage) and m.name == name\n",
    "    ])\n",
    "\n",
    "    # å¦‚æœè¾¾åˆ°æœ€å¤§è½®æ¬¡ï¼Œç»“æŸè®¿è°ˆ\n",
    "    if num_responses >= max_num_turns:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # æ£€æŸ¥ä¸Šä¸€ä¸ªé—®é¢˜æ˜¯å¦è¡¨æ˜å¯¹è¯ç»“æŸ\n",
    "    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾å€’æ•°ç¬¬äºŒä¸ªæ¶ˆæ¯æ˜¯åˆ†æå¸ˆçš„é—®é¢˜\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"éå¸¸æ„Ÿè°¢æ‚¨çš„å¸®åŠ©!\" in last_question.content:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # ç»§ç»­æé—®\n",
    "    return \"ask_question\"\n",
    "\n",
    "# æŠ¥å‘Šå°èŠ‚å†™ä½œæŒ‡ä»¤æ¨¡æ¿\n",
    "# æŒ‡å¯¼AIå¦‚ä½•å°†è®¿è°ˆå†…å®¹è½¬æ¢ä¸ºç»“æ„åŒ–çš„æŠ¥å‘Šå°èŠ‚\n",
    "section_writer_instructions = \"\"\"ä½ æ˜¯ä¸€åèµ„æ·±æŠ€æœ¯å†™ä½œè€…ã€‚\n",
    "\n",
    "ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸€ç»„æ¥æºæ–‡æ¡£ï¼Œæ’°å†™ä¸€æ®µç®€æ´ã€æ˜“è¯»çš„æŠ¥å‘Šå°èŠ‚ã€‚\n",
    "\n",
    "1. å…ˆåˆ†ææ¥æºæ–‡æ¡£å†…å®¹ï¼š\n",
    "- æ¯ä¸ªæ–‡æ¡£çš„åç§°åœ¨æ–‡æ¡£å¼€å¤´ï¼Œä»¥ <Document æ ‡ç­¾å‘ˆç°ã€‚\n",
    "\n",
    "2. ä½¿ç”¨ Markdown åˆ¶ä½œå°èŠ‚ç»“æ„ï¼š\n",
    "- ç”¨ ## ä½œä¸ºå°èŠ‚æ ‡é¢˜\n",
    "- ç”¨ ### ä½œä¸ºå°èŠ‚å†…çš„å°æ ‡é¢˜\n",
    "\n",
    "3. æŒ‰ç»“æ„æ’°å†™ï¼š\n",
    " a. æ ‡é¢˜ï¼ˆ## å¤´ï¼‰\n",
    " b. æ‘˜è¦ï¼ˆ### å¤´ï¼‰\n",
    " c. å‚è€ƒæ¥æºï¼ˆ### å¤´ï¼‰\n",
    "\n",
    "4. æ ‡é¢˜éœ€è¦è´´åˆåˆ†æå¸ˆçš„å…³æ³¨ç‚¹å¹¶å…·æœ‰å¸å¼•åŠ›ï¼š\n",
    "{focus}\n",
    "\n",
    "5. å…³äºæ‘˜è¦éƒ¨åˆ†ï¼š\n",
    "- å…ˆç»™å‡ºä¸åˆ†æå¸ˆå…³æ³¨ç‚¹ç›¸å…³çš„èƒŒæ™¯/ä¸Šä¸‹æ–‡\n",
    "- å¼ºè°ƒè®¿è°ˆä¸­è·å¾—çš„æ–°é¢–ã€æœ‰è¶£æˆ–ä»¤äººæ„å¤–çš„æ´è§\n",
    "- ä½¿ç”¨åˆ°æ¥æºæ–‡æ¡£æ—¶ï¼ŒæŒ‰ä½¿ç”¨é¡ºåºåˆ›å»ºç¼–å·\n",
    "- ä¸è¦æåŠè®¿è°ˆè€…æˆ–ä¸“å®¶çš„åå­—\n",
    "- æ§åˆ¶åœ¨çº¦ 400 å­—ä»¥å†…\n",
    "- åœ¨æŠ¥å‘Šæ­£æ–‡ä¸­ä½¿ç”¨æ•°å­—å¼•ç”¨ï¼ˆå¦‚ [1]ã€[2]ï¼‰ï¼ŒåŸºäºæ¥æºæ–‡æ¡£ä¿¡æ¯\n",
    "\n",
    "6. åœ¨å‚è€ƒæ¥æºéƒ¨åˆ†ï¼š\n",
    "- åˆ—å‡ºæŠ¥å‘Šä¸­ä½¿ç”¨åˆ°çš„å…¨éƒ¨æ¥æº\n",
    "- ç»™å‡ºå®Œæ•´é“¾æ¥æˆ–å…·ä½“æ–‡æ¡£è·¯å¾„\n",
    "- æ¯ä¸ªæ¥æºå•ç‹¬ä¸€è¡Œï¼›åœ¨è¡Œå°¾åŠ ä¸¤ä¸ªç©ºæ ¼ä»¥äº§ç”Ÿ Markdown æ¢è¡Œ\n",
    "- å‚è€ƒæ ¼å¼ï¼š\n",
    "\n",
    "### Sources\n",
    "[1] é“¾æ¥æˆ–æ–‡æ¡£å\n",
    "[2] é“¾æ¥æˆ–æ–‡æ¡£å\n",
    "\n",
    "7. åˆå¹¶é‡å¤æ¥æºã€‚ä¾‹å¦‚ä»¥ä¸‹æ˜¯ä¸æ­£ç¡®çš„ï¼š\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "åº”å»é‡ä¸ºï¼š\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "8. æœ€ç»ˆæ£€æŸ¥ï¼š\n",
    "- ç¡®ä¿æŠ¥å‘Šç»“æ„ç¬¦åˆè¦æ±‚\n",
    "- æ ‡é¢˜å‰ä¸è¦æœ‰ä»»ä½•å‰è¨€\n",
    "- æ£€æŸ¥æ˜¯å¦éµå¾ªäº†å…¨éƒ¨è§„èŒƒ\"\"\"\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"\n",
    "    ç”ŸæˆæŠ¥å‘Šå°èŠ‚çš„æ ¸å¿ƒå‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. åŸºäºè®¿è°ˆå†…å®¹å’Œæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç»“æ„åŒ–çš„æŠ¥å‘Šå°èŠ‚\n",
    "        2. ç¡®ä¿å°èŠ‚å†…å®¹ç¬¦åˆåˆ†æå¸ˆçš„ä¸“ä¸šå…³æ³¨ç‚¹\n",
    "        3. æä¾›å‡†ç¡®çš„å¼•ç”¨å’Œæ¥æºä¿¡æ¯\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«è®¿è°ˆè®°å½•ã€æ£€ç´¢ä¸Šä¸‹æ–‡å’Œåˆ†æå¸ˆä¿¡æ¯çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«ç”Ÿæˆçš„å°èŠ‚å†…å®¹çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # ä»çŠ¶æ€ä¸­è·å–å¿…è¦ä¿¡æ¯\n",
    "    interview = state[\"interview\"]\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«åˆ†æå¸ˆçš„å…³æ³¨ç‚¹æè¿°\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆæŠ¥å‘Šå°èŠ‚\n",
    "    section = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=f\"ä½¿ç”¨è¿™ä¸ªæ¥æºæ¥æ’°å†™ä½ çš„å°èŠ‚: {context}\")\n",
    "    ])\n",
    "\n",
    "    # å°†ç”Ÿæˆçš„å°èŠ‚æ·»åŠ åˆ°çŠ¶æ€ä¸­\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "# æ„å»ºè®¿è°ˆå·¥ä½œæµ\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "\n",
    "# æ·»åŠ å„ä¸ªåŠŸèƒ½èŠ‚ç‚¹\n",
    "interview_builder.add_node(\"ask_question\", generate_question)      # ç”Ÿæˆé—®é¢˜èŠ‚ç‚¹\n",
    "interview_builder.add_node(\"search_web\", search_web)              # ç½‘ç»œæœç´¢èŠ‚ç‚¹\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)  # ç»´åŸºç™¾ç§‘æœç´¢èŠ‚ç‚¹\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)    # ç”Ÿæˆå›ç­”èŠ‚ç‚¹\n",
    "interview_builder.add_node(\"save_interview\", save_interview)      # ä¿å­˜è®¿è°ˆèŠ‚ç‚¹\n",
    "interview_builder.add_node(\"write_section\", write_section)        # æ’°å†™å°èŠ‚èŠ‚ç‚¹\n",
    "\n",
    "# å®šä¹‰å·¥ä½œæµè¿æ¥å…³ç³»\n",
    "interview_builder.add_edge(START, \"ask_question\")  # å¼€å§‹ -> æé—®\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")  # æé—® -> ç½‘ç»œæœç´¢\n",
    "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")  # æé—® -> ç»´åŸºç™¾ç§‘æœç´¢\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")  # ç½‘ç»œæœç´¢ -> å›ç­”\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")  # ç»´åŸºç™¾ç§‘æœç´¢ -> å›ç­”\n",
    "\n",
    "# æ¡ä»¶è¾¹ï¼šæ ¹æ®å¯¹è¯çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\",\n",
    "    route_messages,\n",
    "    ['ask_question', 'save_interview']\n",
    ")\n",
    "\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")  # ä¿å­˜è®¿è°ˆ -> æ’°å†™å°èŠ‚\n",
    "interview_builder.add_edge(\"write_section\", END)  # æ’°å†™å°èŠ‚ -> ç»“æŸ\n",
    "\n",
    "# ç¼–è¯‘è®¿è°ˆå·¥ä½œæµ\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n",
    "\n",
    "\n",
    "# å›¾å¯è§†åŒ–\n",
    "print(\"å›¾å¯è§†åŒ–ï¼š\")\n",
    "\n",
    "# æ–¹æ¡ˆ1ï¼šå°è¯•ä½¿ç”¨ Pyppeteer æœ¬åœ°æ¸²æŸ“ï¼ˆæ¨èï¼‰\n",
    "try:\n",
    "    # å¯è§†åŒ–ï¼šé€šè¿‡ Mermaid æ¸²æŸ“å›¾ç»“æ„\n",
    "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"âœ… å›¾æ¸²æŸ“æˆåŠŸï¼\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Pyppeteer æ¸²æŸ“å¤±è´¥: {e}\")\n",
    "    \n",
    "    # æ–¹æ¡ˆ2ï¼šæ˜¾ç¤º Mermaid æ–‡æœ¬æ ¼å¼\n",
    "    print(\"\\nğŸ“ å›¾ç»“æ„ï¼ˆMermaid æ–‡æœ¬æ ¼å¼ï¼‰ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = interview_graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # æ–¹æ¡ˆ3ï¼šæ˜¾ç¤ºå›¾çš„èŠ‚ç‚¹å’Œè¾¹ä¿¡æ¯\n",
    "    print(\"\\nğŸ”— å›¾ç»“æ„ä¿¡æ¯ï¼š\")\n",
    "    print(\"èŠ‚ç‚¹:\", list(interview_graph.get_graph().nodes.keys()))\n",
    "    print(\"è¾¹:\", list(interview_graph.get_graph().edges))\n",
    "    \n",
    "    # æ–¹æ¡ˆ4ï¼šæä¾›æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜\n",
    "    print(\"\\nğŸ’¡ æ‰‹åŠ¨æ¸²æŸ“è¯´æ˜ï¼š\")\n",
    "    print(\"1. å¤åˆ¶ä¸Šé¢çš„ Mermaid æ–‡æœ¬\")\n",
    "    print(\"2. è®¿é—® https://mermaid.live/\")\n",
    "    print(\"3. ç²˜è´´æ–‡æœ¬åˆ°ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å›¾å½¢\")\n",
    "    print(\"4. æˆ–è€…ä½¿ç”¨æ”¯æŒ Mermaid çš„ Markdown ç¼–è¾‘å™¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd4445-7147-41d3-91f5-21bbe48f2e00",
   "metadata": {},
   "source": [
    "![image-20250930152452478](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301524600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
    "outputId": "b65a5a0a-3276-4cb0-98e1-0f948ffa5295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analyst(affiliation='Tech Research Institute', name='Dr. Emily Zhang', role='æŠ€æœ¯ä¸“å®¶', description='ä¸“æ³¨äºLangGraphæ¡†æ¶åœ¨æŠ€æœ¯ä¸Šçš„ä¼˜åŠ¿ï¼Œå°¤å…¶æ˜¯å…¶åœ¨æ•°æ®å¤„ç†å’Œåˆ†æä¸­çš„é«˜æ•ˆæ€§ã€‚')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick one analyst\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750ac4f-f458-4b2d-8bad-32ce34895758",
   "metadata": {
    "id": "3750ac4f-f458-4b2d-8bad-32ce34895758"
   },
   "source": [
    "æ­¤å¤„æˆ‘ä»¬è¿è¡Œä¸€æ¬¡è®¿è°ˆï¼Œå¹¶ä¼ å…¥ä¸ä¸»é¢˜ç›¸å…³çš„ llama3.1 è®ºæ–‡ç´¢å¼•ä½œä¸ºå‚è€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1642c58-2e68-45ad-b6d9-20ab26bf530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://en.wikipedia.org/w/rest.php/v1/page/Earth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
    "outputId": "db68b62e-29b6-4372-80a0-50e906028c33"
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/util/util.py:38\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/urllib3/connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1429\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m messages = [HumanMessage(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mæ‰€ä»¥ä½ è¯´ä½ åœ¨å†™ä¸€ç¯‡å…³äº\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mçš„æ–‡ç« ?\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m      3\u001b[39m thread = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m interview = \u001b[43minterview_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysts\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_num_turns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m Markdown(interview[\u001b[33m'\u001b[39m\u001b[33msections\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36msearch_wikipedia\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     62\u001b[39m search_query = structured_llm.invoke([search_instructions] + state[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# æ‰§è¡Œç»´åŸºç™¾ç§‘æœç´¢ï¼Œé™åˆ¶æœ€å¤š2ä¸ªæ–‡æ¡£\u001b[39;00m\n\u001b[32m     65\u001b[39m search_docs = \u001b[43mWikipediaLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_max_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# æ ¼å¼åŒ–ç»´åŸºç™¾ç§‘æœç´¢ç»“æœ\u001b[39;00m\n\u001b[32m     71\u001b[39m formatted_search_docs = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([\n\u001b[32m     72\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<Document source=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata[\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m page=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mpage\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc.page_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Document>\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m search_docs\n\u001b[32m     74\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_community/document_loaders/wikipedia.py:59\u001b[39m, in \u001b[36mWikipediaLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[33;03mLoads the query result from Wikipedia into a list of Documents.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m        Wikipedia pages.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m client = WikipediaAPIWrapper(  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m     54\u001b[39m     lang=\u001b[38;5;28mself\u001b[39m.lang,\n\u001b[32m     55\u001b[39m     top_k_results=\u001b[38;5;28mself\u001b[39m.load_max_docs,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     56\u001b[39m     load_all_available_meta=\u001b[38;5;28mself\u001b[39m.load_all_available_meta,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     57\u001b[39m     doc_content_chars_max=\u001b[38;5;28mself\u001b[39m.doc_content_chars_max,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_community/utilities/wikipedia.py:111\u001b[39m, in \u001b[36mWikipediaAPIWrapper.load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m \n\u001b[32m    110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/langchain_community/utilities/wikipedia.py:121\u001b[39m, in \u001b[36mWikipediaAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> Iterator[Document]:\n\u001b[32m    114\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[33;03m    Run Wikipedia search and get the article text plus the meta information.\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m    See\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m \n\u001b[32m    120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     page_titles = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwiki_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mWIKIPEDIA_MAX_QUERY_LENGTH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtop_k_results\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m page_title \u001b[38;5;129;01min\u001b[39;00m page_titles[: \u001b[38;5;28mself\u001b[39m.top_k_results]:\n\u001b[32m    125\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m wiki_page := \u001b[38;5;28mself\u001b[39m._fetch_page(page_title):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/wikipedia/util.py:28\u001b[39m, in \u001b[36mcache.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m   ret = \u001b[38;5;28mself\u001b[39m._cache[key]\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m   ret = \u001b[38;5;28mself\u001b[39m._cache[key] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/wikipedia/wikipedia.py:103\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(query, results, suggestion)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion:\n\u001b[32m    101\u001b[39m   search_params[\u001b[33m'\u001b[39m\u001b[33msrinfo\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33msuggestion\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m raw_results = \u001b[43m_wiki_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m raw_results:\n\u001b[32m    106\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m raw_results[\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33minfo\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m'\u001b[39m\u001b[33mHTTP request timed out.\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPool queue is full\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/wikipedia/wikipedia.py:737\u001b[39m, in \u001b[36m_wiki_request\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m    734\u001b[39m   wait_time = (RATE_LIMIT_LAST_CALL + RATE_LIMIT_MIN_WAIT) - datetime.now()\n\u001b[32m    735\u001b[39m   time.sleep(\u001b[38;5;28mint\u001b[39m(wait_time.total_seconds()))\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m r = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RATE_LIMIT:\n\u001b[32m    740\u001b[39m   RATE_LIMIT_LAST_CALL = datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages/requests/adapters.py:659\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    644\u001b[39m     resp = conn.urlopen(\n\u001b[32m    645\u001b[39m         method=request.method,\n\u001b[32m    646\u001b[39m         url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    655\u001b[39m         chunked=chunked,\n\u001b[32m    656\u001b[39m     )\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ConnectTimeoutError):\n\u001b[32m    663\u001b[39m         \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "During task with name 'search_wikipedia' and id 'f4094682-8f61-5811-6c4f-73492ecdaed6'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "messages = [HumanMessage(f\"æ‰€ä»¥ä½ è¯´ä½ åœ¨å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« ?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "interview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread)\n",
    "Markdown(interview['sections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739e87-68bb-4e96-a86a-704e84240a6c",
   "metadata": {
    "id": "3b739e87-68bb-4e96-a86a-704e84240a6c"
   },
   "source": [
    "### å¹¶è¡Œè®¿è°ˆï¼šMap-Reduce\n",
    "\n",
    "æˆ‘ä»¬é€šè¿‡ `Send()` API å¹¶è¡Œè¿è¡Œæ¯ä¸ªè®¿è°ˆï¼ˆmap æ­¥ï¼‰ã€‚\n",
    "\n",
    "éšååœ¨ reduce æ­¥ä¸­å°†å®ƒä»¬åˆå¹¶ä¸ºæŠ¥å‘Šä¸»ä½“ã€‚\n",
    "\n",
    "### æ”¶å°¾ï¼ˆFinalizeï¼‰\n",
    "\n",
    "æœ€åå¢åŠ ä¸€æ­¥ï¼Œä¸ºæœ€ç»ˆæŠ¥å‘Šå†™å‡ºå¼•è¨€ä¸ç»“è®ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140",
   "metadata": {
    "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    ç ”ç©¶å›¾çŠ¶æ€ç®¡ç†ç±»\n",
    "\n",
    "    ç”¨äºç®¡ç†æ•´ä¸ªç ”ç©¶æµç¨‹çš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬åˆ†æå¸ˆç”Ÿæˆã€å¹¶è¡Œè®¿è°ˆå’ŒæŠ¥å‘Šç”Ÿæˆ\n",
    "    è¿™æ˜¯æ•´ä¸ªç ”ç©¶åŠ©ç†ç³»ç»Ÿçš„æ ¸å¿ƒçŠ¶æ€ç®¡ç†ç±»\n",
    "    \"\"\"\n",
    "    topic: str  # ç ”ç©¶ä¸»é¢˜\n",
    "    max_analysts: int  # åˆ†æå¸ˆæ•°é‡ä¸Šé™\n",
    "    human_analyst_feedback: str  # äººç±»åé¦ˆä¿¡æ¯\n",
    "    analysts: List[Analyst]  # åˆ†æå¸ˆåˆ—è¡¨\n",
    "    sections: Annotated[list, operator.add]  # æŠ¥å‘Šå°èŠ‚åˆ—è¡¨ï¼Œä½¿ç”¨operator.addè¿›è¡Œç´¯åŠ \n",
    "    introduction: str  # æœ€ç»ˆæŠ¥å‘Šçš„å¼•è¨€éƒ¨åˆ†\n",
    "    content: str  # æœ€ç»ˆæŠ¥å‘Šçš„ä¸»ä½“å†…å®¹\n",
    "    conclusion: str  # æœ€ç»ˆæŠ¥å‘Šçš„ç»“è®ºéƒ¨åˆ†\n",
    "    final_report: str  # å®Œæ•´çš„æœ€ç»ˆæŠ¥å‘Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2224592-d2ff-469d-97bd-928809f896d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c2224592-d2ff-469d-97bd-928809f896d7",
    "outputId": "9031b724-0a08-49bd-fc4a-74dbcb81ad3f"
   },
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    å¯åŠ¨æ‰€æœ‰å¹¶è¡Œè®¿è°ˆçš„Mapæ­¥éª¤\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. æ£€æŸ¥æ˜¯å¦æœ‰äººç±»åé¦ˆï¼Œå¦‚æœæœ‰åˆ™é‡æ–°ç”Ÿæˆåˆ†æå¸ˆ\n",
    "        2. å¦‚æœæ²¡æœ‰åé¦ˆï¼Œåˆ™å¹¶è¡Œå¯åŠ¨æ‰€æœ‰åˆ†æå¸ˆçš„è®¿è°ˆæµç¨‹\n",
    "        3. ä½¿ç”¨Send APIå®ç°çœŸæ­£çš„å¹¶è¡Œæ‰§è¡Œ\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«åˆ†æå¸ˆåˆ—è¡¨å’Œç ”ç©¶ä¸»é¢˜çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        str æˆ– List[Send]: å¦‚æœæœ‰äººç±»åé¦ˆè¿”å›èŠ‚ç‚¹åï¼Œå¦åˆ™è¿”å›Sendå¯¹è±¡åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰äººç±»åé¦ˆ\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback')\n",
    "    if human_analyst_feedback:\n",
    "        # æœ‰äººç±»åé¦ˆï¼Œé‡æ–°ç”Ÿæˆåˆ†æå¸ˆ\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # æ²¡æœ‰åé¦ˆï¼Œå¹¶è¡Œå¯åŠ¨æ‰€æœ‰è®¿è°ˆ\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        # ä¸ºæ¯ä¸ªåˆ†æå¸ˆåˆ›å»ºä¸€ä¸ªSendå¯¹è±¡ï¼Œå®ç°å¹¶è¡Œæ‰§è¡Œ\n",
    "        return [\n",
    "            Send(\"conduct_interview\", {\n",
    "                \"analyst\": analyst,\n",
    "                \"messages\": [HumanMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\"\n",
    "                )]\n",
    "            })\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]\n",
    "\n",
    "# æŠ¥å‘Šå†™ä½œæŒ‡ä»¤æ¨¡æ¿\n",
    "# æŒ‡å¯¼AIå¦‚ä½•å°†å¤šä¸ªåˆ†æå¸ˆçš„å°èŠ‚æ•´åˆä¸ºç»Ÿä¸€çš„æŠ¥å‘Šä¸»ä½“\n",
    "report_writer_instructions = \"\"\"ä½ æ˜¯ä¸€åæŠ€æœ¯å†™ä½œè€…ï¼Œæ­£åœ¨ä¸ºå¦‚ä¸‹ä¸»é¢˜æ’°å†™æŠ¥å‘Šï¼š\n",
    "\n",
    "{topic}\n",
    "\n",
    "ä½ æ‹¥æœ‰ä¸€æ”¯åˆ†æå¸ˆå›¢é˜Ÿã€‚æ¯ä½åˆ†æå¸ˆå®Œæˆäº†ä¸¤ä»¶äº‹ï¼š\n",
    "\n",
    "1. å›´ç»•ä¸€ä¸ªå…·ä½“å­ä¸»é¢˜ï¼Œè®¿è°ˆäº†ä¸€ä½ä¸“å®¶ã€‚\n",
    "2. å°†å‘ç°å†™æˆä¸€ä»½å¤‡å¿˜å½•ï¼ˆmemoï¼‰ã€‚\n",
    "\n",
    "ä½ çš„ä»»åŠ¡ï¼š\n",
    "\n",
    "1. ä½ å°†æ”¶åˆ°åˆ†æå¸ˆä»¬çš„å¤‡å¿˜å½•é›†åˆã€‚\n",
    "2. ä»”ç»†æ€è€ƒæ¯ä»½å¤‡å¿˜å½•çš„æ´è§ã€‚\n",
    "3. å°†å®ƒä»¬æ•´åˆä¸ºç®€æ´çš„æ€»ä½“æ€»ç»“ï¼Œä¸²è”èµ·æ‰€æœ‰å¤‡å¿˜å½•çš„ä¸­å¿ƒè§‚ç‚¹ã€‚\n",
    "4. æŠŠæ¯ä»½å¤‡å¿˜å½•çš„å…³é”®ä¿¡æ¯å½’çº³æˆä¸€ä¸ªè¿è´¯çš„å•ä¸€å™è¿°ã€‚\n",
    "\n",
    "æŠ¥å‘Šæ ¼å¼è¦æ±‚ï¼š\n",
    "\n",
    "1. ä½¿ç”¨ Markdown æ ¼å¼ã€‚\n",
    "2. æŠ¥å‘Šä¸è¦æœ‰ä»»ä½•å‰è¨€ã€‚\n",
    "3. ä¸ä½¿ç”¨ä»»ä½•å°æ ‡é¢˜ã€‚\n",
    "4. æŠ¥å‘Šä»¥ä¸€ä¸ªæ ‡é¢˜å¼€å¤´ï¼š## Insights\n",
    "5. æŠ¥å‘Šä¸­ä¸è¦æåŠä»»ä½•åˆ†æå¸ˆçš„åå­—ã€‚\n",
    "6. ä¿ç•™å¤‡å¿˜å½•ä¸­çš„å¼•ç”¨æ ‡æ³¨ï¼ˆå¦‚ [1]ã€[2]ï¼‰ã€‚\n",
    "7. æ±‡æ€»æœ€ç»ˆæ¥æºåˆ—è¡¨ï¼Œå¹¶ä»¥ `## Sources` ä½œä¸ºå°èŠ‚æ ‡é¢˜ã€‚\n",
    "8. æŒ‰é¡ºåºåˆ—å‡ºæ¥æºä¸”ä¸è¦é‡å¤ã€‚\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "ä»¥ä¸‹æ˜¯åˆ†æå¸ˆæä¾›çš„å¤‡å¿˜å½•ï¼Œè¯·åŸºäºæ­¤æ’°å†™æŠ¥å‘Šï¼š\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šä¸»ä½“å†…å®¹çš„å‡½æ•°ï¼ˆReduceæ­¥éª¤ï¼‰\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. æ”¶é›†æ‰€æœ‰åˆ†æå¸ˆçš„å°èŠ‚å†…å®¹\n",
    "        2. å°†å¤šä¸ªå°èŠ‚æ•´åˆä¸ºç»Ÿä¸€çš„æŠ¥å‘Šä¸»ä½“\n",
    "        3. ç¡®ä¿æŠ¥å‘Šç»“æ„æ¸…æ™°ã€å†…å®¹è¿è´¯\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«æ‰€æœ‰å°èŠ‚å†…å®¹å’Œç ”ç©¶ä¸»é¢˜çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æŠ¥å‘Šä¸»ä½“å†…å®¹çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # è·å–æ‰€æœ‰å°èŠ‚å†…å®¹å’Œç ”ç©¶ä¸»é¢˜\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # å°†æ‰€æœ‰å°èŠ‚æ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # æ„å»ºç³»ç»Ÿæ¶ˆæ¯ï¼ŒåŒ…å«ç ”ç©¶ä¸»é¢˜å’Œå°èŠ‚å†…å®¹\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic,\n",
    "        context=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆæŠ¥å‘Šä¸»ä½“\n",
    "    report = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=f\"Write a report based upon these memos.\")\n",
    "    ])\n",
    "\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "# å¼•è¨€å’Œç»“è®ºå†™ä½œæŒ‡ä»¤æ¨¡æ¿\n",
    "# æŒ‡å¯¼AIå¦‚ä½•ä¸ºæŠ¥å‘Šç”Ÿæˆç®€æ´æœ‰åŠ›çš„å¼•è¨€å’Œç»“è®º\n",
    "intro_conclusion_instructions = \"\"\"ä½ æ˜¯ä¸€åæŠ€æœ¯å†™ä½œè€…ï¼Œæ­£åœ¨å®Œæˆä¸»é¢˜ä¸º {topic} çš„æŠ¥å‘Šã€‚\n",
    "\n",
    "ä½ å°†è·å¾—æŠ¥å‘Šçš„å…¨éƒ¨å°èŠ‚ã€‚\n",
    "\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æ’°å†™ç®€æ´è€Œæœ‰è¯´æœåŠ›çš„å¼•è¨€æˆ–ç»“è®ºã€‚\n",
    "\n",
    "ç”±ç”¨æˆ·å‘ŠçŸ¥å†™å¼•è¨€è¿˜æ˜¯ç»“è®ºã€‚\n",
    "\n",
    "ä¸¤è€…å‡ä¸éœ€è¦ä»»ä½•å‰è¨€ã€‚\n",
    "\n",
    "ç›®æ ‡çº¦ 100 å­—ï¼š\n",
    "- å¼•è¨€ï¼šç²¾ç‚¼é¢„è§ˆå„å°èŠ‚è¦ç‚¹\n",
    "- ç»“è®ºï¼šç²¾ç‚¼å›é¡¾å„å°èŠ‚è¦ç‚¹\n",
    "\n",
    "ä½¿ç”¨ Markdown æ ¼å¼ã€‚\n",
    "\n",
    "ç”Ÿæˆçš„æŠ¥å‘Šä½¿ç”¨ä¸­æ–‡ï¼Œæ‰€æœ‰å†…å®¹éƒ½è¦ä½¿ç”¨ä¸­æ–‡ï¼Œå¯¹äºç‰¹æ®Šçš„è‹±æ–‡æœ¯è¯­ï¼Œå¯ä»¥ä½¿ç”¨ä¸­è‹±æ–‡ä¸€èµ·è¡¨ç¤ºã€‚\n",
    "\n",
    "å¼•è¨€è¦æ±‚ï¼šåˆ›å»ºä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„æ ‡é¢˜ï¼Œå¹¶ç”¨ # ä½œä¸ºæ ‡é¢˜å¤´ã€‚\n",
    "\n",
    "å¼•è¨€å°èŠ‚æ ‡é¢˜ä½¿ç”¨ï¼š## Introduction\n",
    "\n",
    "ç»“è®ºå°èŠ‚æ ‡é¢˜ä½¿ç”¨ï¼š## Conclusion\n",
    "\n",
    "æ’°å†™æ—¶å¯å‚è€ƒä»¥ä¸‹å°èŠ‚å†…å®¹ï¼š{formatted_str_sections}\"\"\"\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    ç”ŸæˆæŠ¥å‘Šå¼•è¨€çš„å‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. åŸºäºæ‰€æœ‰å°èŠ‚å†…å®¹ç”ŸæˆæŠ¥å‘Šå¼•è¨€\n",
    "        2. æä¾›æŠ¥å‘Šçš„æ•´ä½“æ¦‚è§ˆå’Œå¸å¼•åŠ›\n",
    "        3. ä¸ºè¯»è€…æä¾›é˜…è¯»æŒ‡å¯¼\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«æ‰€æœ‰å°èŠ‚å†…å®¹å’Œç ”ç©¶ä¸»é¢˜çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«å¼•è¨€å†…å®¹çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # è·å–æ‰€æœ‰å°èŠ‚å†…å®¹å’Œç ”ç©¶ä¸»é¢˜\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # å°†æ‰€æœ‰å°èŠ‚æ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # æ„å»ºæŒ‡ä»¤ï¼ŒåŒ…å«ç ”ç©¶ä¸»é¢˜å’Œå°èŠ‚å†…å®¹\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic,\n",
    "        formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå¼•è¨€\n",
    "    intro = llm.invoke([\n",
    "        instructions,\n",
    "        HumanMessage(content=f\"Write the report introduction\")\n",
    "    ])\n",
    "\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    ç”ŸæˆæŠ¥å‘Šç»“è®ºçš„å‡½æ•°\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. åŸºäºæ‰€æœ‰å°èŠ‚å†…å®¹ç”ŸæˆæŠ¥å‘Šç»“è®º\n",
    "        2. æ€»ç»“æŠ¥å‘Šçš„ä¸»è¦å‘ç°å’Œæ´å¯Ÿ\n",
    "        3. ä¸ºè¯»è€…æä¾›æ¸…æ™°çš„æ€»ç»“\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«æ‰€æœ‰å°èŠ‚å†…å®¹å’Œç ”ç©¶ä¸»é¢˜çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«ç»“è®ºå†…å®¹çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # è·å–æ‰€æœ‰å°èŠ‚å†…å®¹å’Œç ”ç©¶ä¸»é¢˜\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # å°†æ‰€æœ‰å°èŠ‚æ‹¼æ¥ä¸ºå®Œæ•´æ–‡æœ¬\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # æ„å»ºæŒ‡ä»¤ï¼ŒåŒ…å«ç ”ç©¶ä¸»é¢˜å’Œå°èŠ‚å†…å®¹\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic,\n",
    "        formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆç»“è®º\n",
    "    conclusion = llm.invoke([\n",
    "        instructions,\n",
    "        HumanMessage(content=f\"Write the report conclusion\")\n",
    "    ])\n",
    "\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå‡½æ•°ï¼ˆReduceæ­¥éª¤çš„æœ€ç»ˆé˜¶æ®µï¼‰\n",
    "\n",
    "    åŠŸèƒ½:\n",
    "        1. æ•´åˆå¼•è¨€ã€ä¸»ä½“å†…å®¹å’Œç»“è®º\n",
    "        2. å¤„ç†æ¥æºä¿¡æ¯çš„æ ¼å¼\n",
    "        3. ç”Ÿæˆå®Œæ•´çš„æœ€ç»ˆæŠ¥å‘Š\n",
    "\n",
    "    å‚æ•°:\n",
    "        state: åŒ…å«å¼•è¨€ã€ä¸»ä½“å†…å®¹ã€ç»“è®ºå’Œæ¥æºä¿¡æ¯çš„çŠ¶æ€å¯¹è±¡\n",
    "\n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«å®Œæ•´æœ€ç»ˆæŠ¥å‘Šçš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # è·å–æŠ¥å‘Šä¸»ä½“å†…å®¹\n",
    "    content = state[\"content\"]\n",
    "\n",
    "    # æ¸…ç†å†…å®¹æ ¼å¼ï¼Œç§»é™¤é‡å¤çš„æ ‡é¢˜\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "\n",
    "    # åˆ†ç¦»ä¸»ä½“å†…å®¹å’Œæ¥æºä¿¡æ¯\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    # ç»„åˆå®Œæ•´çš„æœ€ç»ˆæŠ¥å‘Š\n",
    "    final_report = (\n",
    "        state[\"introduction\"] +\n",
    "        \"\\n\\n---\\n\\n\" +\n",
    "        content +\n",
    "        \"\\n\\n---\\n\\n\" +\n",
    "        state[\"conclusion\"]\n",
    "    )\n",
    "\n",
    "    # å¦‚æœæœ‰æ¥æºä¿¡æ¯ï¼Œæ·»åŠ åˆ°æŠ¥å‘Šæœ«å°¾\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# æ„å»ºå®Œæ•´çš„ç ”ç©¶å›¾å·¥ä½œæµ\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# æ·»åŠ æ‰€æœ‰åŠŸèƒ½èŠ‚ç‚¹\n",
    "builder.add_node(\"create_analysts\", create_analysts)  # åˆ†æå¸ˆç”ŸæˆèŠ‚ç‚¹\n",
    "builder.add_node(\"human_feedback\", human_feedback)    # äººç±»åé¦ˆèŠ‚ç‚¹\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())  # è®¿è°ˆå­å›¾èŠ‚ç‚¹\n",
    "builder.add_node(\"write_report\", write_report)        # æŠ¥å‘Šä¸»ä½“å†™ä½œèŠ‚ç‚¹\n",
    "builder.add_node(\"write_introduction\", write_introduction)  # å¼•è¨€å†™ä½œèŠ‚ç‚¹\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)      # ç»“è®ºå†™ä½œèŠ‚ç‚¹\n",
    "builder.add_node(\"finalize_report\", finalize_report)        # æœ€ç»ˆæŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹\n",
    "\n",
    "# å®šä¹‰å·¥ä½œæµè¿æ¥å…³ç³»\n",
    "builder.add_edge(START, \"create_analysts\")  # å¼€å§‹ -> ç”Ÿæˆåˆ†æå¸ˆ\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")  # ç”Ÿæˆåˆ†æå¸ˆ -> äººç±»åé¦ˆ\n",
    "\n",
    "# æ¡ä»¶è¾¹ï¼šæ ¹æ®æ˜¯å¦æœ‰åé¦ˆå†³å®šä¸‹ä¸€æ­¥\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    initiate_all_interviews,\n",
    "    [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# å¹¶è¡Œæ‰§è¡Œï¼šè®¿è°ˆå®ŒæˆååŒæ—¶è¿›è¡ŒæŠ¥å‘Šå†™ä½œã€å¼•è¨€å†™ä½œå’Œç»“è®ºå†™ä½œ\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# ç­‰å¾…æ‰€æœ‰å†™ä½œä»»åŠ¡å®Œæˆåï¼Œè¿›è¡Œæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"],\n",
    "    \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)  # æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ -> ç»“æŸ\n",
    "\n",
    "# ç¼–è¯‘å®Œæ•´çš„ç ”ç©¶å›¾å·¥ä½œæµ\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(\n",
    "    interrupt_before=['human_feedback'],  # åœ¨äººç±»åé¦ˆèŠ‚ç‚¹å‰ä¸­æ–­\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# æ˜¾ç¤ºå®Œæ•´çš„å·¥ä½œæµå›¾\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0",
   "metadata": {
    "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0"
   },
   "source": [
    "æˆ‘ä»¬æ¥å°± LangGraph æä¸€ä¸ªå¼€æ”¾å¼é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
    "outputId": "2498f7ea-2c34-426b-caa2-efe6c2137bdf"
   },
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºï¼šè¿è¡Œç ”ç©¶åŠ©ç†ç³»ç»Ÿ\n",
    "# è®¾ç½®è¾“å…¥å‚æ•°\n",
    "max_analysts = 3  # åˆ†æå¸ˆæ•°é‡\n",
    "topic = \"é‡‡ç”¨LangGraphä½œä¸ºAI Agentæ¡†æ¶çš„å¥½å¤„\"  # ç ”ç©¶ä¸»é¢˜\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}  # çº¿ç¨‹IDï¼Œç”¨äºçŠ¶æ€ç®¡ç†\n",
    "\n",
    "# è¿è¡Œå·¥ä½œæµç›´åˆ°ç¬¬ä¸€ä¸ªä¸­æ–­ç‚¹ï¼ˆäººç±»åé¦ˆèŠ‚ç‚¹ï¼‰\n",
    "for event in graph.stream({\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts\n",
    "}, thread, stream_mode=\"values\"):\n",
    "\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æå¸ˆä¿¡æ¯è¾“å‡º\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        # æ˜¾ç¤ºç”Ÿæˆçš„åˆ†æå¸ˆä¿¡æ¯\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
    "outputId": "9ac87141-cc10-4e31-b453-f7c1d288bc06"
   },
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿäººç±»åé¦ˆï¼šæ·»åŠ ä¸€ä¸ªAIåŸç”Ÿåˆåˆ›å…¬å¸çš„CEOè§†è§’\n",
    "# è¿™å±•ç¤ºäº†äººæœºååŒï¼ˆHuman-in-the-loopï¼‰åŠŸèƒ½çš„ä½¿ç”¨\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": \"æ·»åŠ ä¸€ä¸ªAIåŸç”Ÿåˆåˆ›å…¬å¸çš„CEO\"\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
    "outputId": "d507e3b8-68c4-43bd-da1a-026413e162f8"
   },
   "outputs": [],
   "source": [
    "# æ£€æŸ¥æ›´æ–°åçš„åˆ†æå¸ˆåˆ—è¡¨\n",
    "# ç³»ç»Ÿä¼šæ ¹æ®äººç±»åé¦ˆé‡æ–°ç”Ÿæˆåˆ†æå¸ˆå›¢é˜Ÿ\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        # æ˜¾ç¤ºæ›´æ–°åçš„åˆ†æå¸ˆä¿¡æ¯\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af41f54-88d9-4597-98b0-444c08322095",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0af41f54-88d9-4597-98b0-444c08322095",
    "outputId": "44858e67-bb8a-44de-c5ef-2b191f117013"
   },
   "outputs": [],
   "source": [
    "# ç¡®è®¤æ»¡æ„å½“å‰çš„åˆ†æå¸ˆå›¢é˜Ÿï¼Œç»§ç»­æ‰§è¡Œåç»­æµç¨‹\n",
    "# è®¾ç½®åé¦ˆä¸ºNoneè¡¨ç¤ºæ²¡æœ‰è¿›ä¸€æ­¥çš„ä¿®æ”¹éœ€æ±‚\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": None\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
    "outputId": "dc7eee62-2047-4aff-9fd7-bae60e914aa7"
   },
   "outputs": [],
   "source": [
    "# ç»§ç»­æ‰§è¡Œå®Œæ•´çš„ç ”ç©¶æµç¨‹\n",
    "# åŒ…æ‹¬å¹¶è¡Œè®¿è°ˆã€æŠ¥å‘Šç”Ÿæˆç­‰æ‰€æœ‰åç»­æ­¥éª¤\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
    "outputId": "14cfbc76-d0f9-436f-ccd1-c30110f6d3c7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# è·å–æœ€ç»ˆçŠ¶æ€å¹¶æ˜¾ç¤ºç”Ÿæˆçš„æŠ¥å‘Š\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "\n",
    "# ä½¿ç”¨Markdownæ ¼å¼æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3",
   "metadata": {
    "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3"
   },
   "source": [
    "# LangSmithæ‰§è¡Œè¿½è¸ª\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä¸€æ¬¡å®Œæ•´çš„æ‰§è¡Œè¿½è¸ªï¼ˆtraceï¼‰ï¼Œäº†è§£æ•´ä¸ªç ”ç©¶æµç¨‹çš„è¯¦ç»†æ‰§è¡Œæƒ…å†µï¼š\n",
    "\n",
    "`https://smith.langchain.com/public/2933a7bb-bcef-4d2d-9b85-cc735b22ca0c/r`\n",
    "\n",
    "## è¿½è¸ªå†…å®¹è¯´æ˜\n",
    "\n",
    "è¿™ä¸ªè¿½è¸ªé“¾æ¥å±•ç¤ºäº†ï¼š\n",
    "\n",
    "1. **åˆ†æå¸ˆç”Ÿæˆè¿‡ç¨‹**ï¼šå¦‚ä½•æ ¹æ®ç ”ç©¶ä¸»é¢˜åˆ›å»ºä¸åŒçš„åˆ†æå¸ˆè§’è‰²\n",
    "2. **å¹¶è¡Œè®¿è°ˆæ‰§è¡Œ**ï¼šå¤šä¸ªåˆ†æå¸ˆåŒæ—¶è¿›è¡Œè®¿è°ˆçš„è¯¦ç»†è¿‡ç¨‹\n",
    "3. **ä¿¡æ¯æ£€ç´¢æµç¨‹**ï¼šç½‘ç»œæœç´¢å’Œç»´åŸºç™¾ç§‘æœç´¢çš„å…·ä½“æ‰§è¡Œ\n",
    "4. **æŠ¥å‘Šç”Ÿæˆæ­¥éª¤**ï¼šä»è®¿è°ˆå†…å®¹åˆ°æœ€ç»ˆæŠ¥å‘Šçš„å®Œæ•´è½¬æ¢è¿‡ç¨‹\n",
    "5. **äººæœºååŒäº¤äº’**ï¼šäººç±»åé¦ˆå¦‚ä½•å½±å“ç³»ç»Ÿè¡Œä¸º\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªè¿½è¸ªï¼Œä½ å¯ä»¥æ·±å…¥äº†è§£LangGraphå·¥ä½œæµçš„å†…éƒ¨æ‰§è¡Œæœºåˆ¶å’Œæ¯ä¸ªèŠ‚ç‚¹çš„å…·ä½“åŠŸèƒ½ã€‚"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
