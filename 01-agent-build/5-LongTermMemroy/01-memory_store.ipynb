{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "# 🔧 环境配置和检查\n",
    "\n",
    "## 概述\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "- 使用统一的conda环境\n",
    "- 通过国内镜像源快速安装依赖\n",
    "- 加速模型下载\n",
    "- 检查系统配置\n",
    "\n",
    "## 配置步骤\n",
    "1. **Conda环境管理** - 激活统一的学习环境\n",
    "2. **包管理器优化** - 配置pip使用清华镜像源\n",
    "3. **模型下载加速** - 设置HuggingFace镜像代理\n",
    "4. **系统环境诊断** - 检查硬件和软件配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 激活conda环境\n",
    "%%script bash\n",
    "# 初始化 conda\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/langchain-academy/blob/fly101/module-5/memory_store.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGeJpfJRSv7L"
   },
   "source": [
    "# 具备记忆能力的聊天机器人\n",
    "\n",
    "## 回顾\n",
    "\n",
    "[记忆](https://pmc.ncbi.nlm.nih.gov/articles/PMC10410470/)是人类的一种认知功能，用于存储、检索和使用信息，从而更好地理解当下与未来。\n",
    "\n",
    "在 AI 应用中可以利用[多种长期记忆类型](https://langchain-ai.github.io/langgraph/concepts/memory/#memory)。\n",
    "\n",
    "## 目标\n",
    "\n",
    "本节将介绍 [LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)——一种在系统中保存与检索长期记忆的机制。\n",
    "\n",
    "我们将构建一个同时使用`短期记忆（单线程内）`与`长期记忆（跨线程）`的聊天机器人。\n",
    "\n",
    "**这里重点关注长期的[语义记忆](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory)，即关于用户的客观事实信息。**\n",
    "\n",
    "**这些长期记忆将用于个性化对话，使机器人能够“记住”用户信息。**\n",
    "\n",
    "记忆会在对话过程中[实时写入（hot path）](https://langchain-ai.github.io/langgraph/concepts/memory/#writing-memories)。 为初学者：可将其理解为“边聊边记”，无需单独的离线处理环节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wmSIOew0Sv7P"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install -U langchain_openai langgraph langchain_core\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pA-H02laSv7Q"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[调用链追踪（tracing）](https://docs.smith.langchain.com/concepts/tracing)，便于观察每一步推理与存取记忆的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "c92MFinuSv7Q",
    "outputId": "1e2a6b81-1348-4273-9d9a-02d5bc5557e8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY: ··········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "# 读取或安全输入环境变量的小工具函数\n",
    "# 若未在系统环境中设置，则在运行时提示输入（避免把密钥写入代码）\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# LangSmith 相关配置（用于追踪与可观测性）\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIDVTTuHSv7R"
   },
   "source": [
    "## LangGraph Store 简介\n",
    "\n",
    "[LangGraph Memory Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 提供了在 LangGraph 中于“跨线程（across threads）”范围内存取信息的能力。\n",
    "\n",
    "它是一个用于持久化 `key-value` 存储的[开源基础类](https://blog.langchain.dev/launching-long-term-memory-support-in-langgraph/)。初学者可将其理解为“分层命名空间 + 键值对”的抽象：在不同用户、不同会话之间进行隔离管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Daa4-9giSv7R"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# 初始化内存型 Store（适合示例与本地开发，生产可替换为持久化实现）\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-OzPIdjSv7R"
   },
   "source": [
    "当我们将对象（如“记忆”）保存到[Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 时，需要提供：\n",
    "\n",
    "- `namespace`（命名空间）：一个元组，类似多级目录\n",
    "- `key`（键）：类似文件名\n",
    "- `value`（值）：类似文件内容\n",
    "\n",
    "使用 [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) 方法按 `namespace` 与 `key` 将对象写入存储。\n",
    "\n",
    "\n",
    "为初学者：`namespace` 常用来表示“用户 ID + 分类”，例如 `(user_id, \"memories\")`，从而区分不同用户与不同类型的长期记忆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IkmKW63gSv7S"
   },
   "outputs": [],
   "source": [
    "# 要保存记忆的命名空间（跨用户/分类进行隔离）\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")\n",
    "\n",
    "# 在该命名空间下生成一个唯一键（类似文件名）\n",
    "key = str(uuid.uuid4())\n",
    "\n",
    "# 值需为字典（便于后续扩展与合并）\n",
    "value = {\"食物偏好\" : \"我喜欢披萨\"}\n",
    "\n",
    "# 写入记忆\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT0dR_uKSv7S"
   },
   "source": [
    "我们使用 [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) 按 `namespace` 从存储中检索对象。\n",
    "\n",
    "该方法返回一个列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4tbNyO8Sv7S",
    "outputId": "18823938-605c-41cb-9841-56b1b8b79228"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 查询（按命名空间检索）\n",
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "type(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mB9UbwISv7T",
    "outputId": "a45abb23-1ae3-47ca-c478-c9459350e008"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'namespace': ['1', 'memories'],\n",
       " 'key': '47425283-5656-4103-87bc-1e40ded68077',\n",
       " 'value': {'食物偏好': '我喜欢披萨'},\n",
       " 'created_at': '2025-09-17T07:41:14.916499+00:00',\n",
       " 'updated_at': '2025-09-17T07:41:14.916502+00:00',\n",
       " 'score': None}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# 元数据\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S1crYTS3Sv7T",
    "outputId": "c7346a47-e626-4c39-e16a-3f9ffc67fdd6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47425283-5656-4103-87bc-1e40ded68077 {'食物偏好': '我喜欢披萨'}\n"
     ]
    }
   ],
   "source": [
    "# 键与值\n",
    "print(memories[0].key, memories[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4xuQg97Sv7T"
   },
   "source": [
    "我们也可以使用 [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) 通过 `namespace` 与 `key` 来获取对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Wc9p3nPSv7T",
    "outputId": "8301e848-7ba5-4014-c27c-e004184ed8dd"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'namespace': ['1', 'memories'],\n",
       " 'key': '47425283-5656-4103-87bc-1e40ded68077',\n",
       " 'value': {'食物偏好': '我喜欢披萨'},\n",
       " 'created_at': '2025-09-17T07:41:14.916499+00:00',\n",
       " 'updated_at': '2025-09-17T07:41:14.916502+00:00'}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# 通过命名空间与键获取记忆\n",
    "memory = in_memory_store.get(namespace_for_memory, key)\n",
    "memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiQYW7_6Sv7U"
   },
   "source": [
    "## 拥有长期记忆的聊天机器人\n",
    "\n",
    "我们希望聊天机器人[具备两类记忆](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_156)：\n",
    "\n",
    "1. `短期记忆（单线程内）`：在同一会话线程中，持续保存对话历史，并可支持中断/恢复。\n",
    "2. `长期记忆（跨线程）`：在所有会话中，持续记住特定用户的相关信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axeVjDNvSv7U",
    "outputId": "477e0c6a-3cd8-4243-bc22-0d223fd06842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n"
     ]
    }
   ],
   "source": [
    "# 设置OpenAI API密钥\n",
    "# 您需要从 https://api.apiyi.com/v1 获取API密钥\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJ0yxD0fSv7U"
   },
   "source": [
    "对于`短期记忆`，我们将使用[检查点（checkpointer）](https://langchain-ai.github.io/langgraph/concepts/persistence/#checkpointer-libraries)。\n",
    "\n",
    "更多细节可参考第 2 模块与[持久化概念文档](https://langchain-ai.github.io/langgraph/concepts/persistence/)。简而言之：\n",
    "\n",
    "* 在每一步将图的状态写入某个线程（thread）。\n",
    "* 在线程中持久化聊天历史。\n",
    "* 允许从线程中的任意一步进行中断与恢复。\n",
    "\n",
    "而对于`长期记忆`，我们将使用上文介绍的 [LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P4NUHD6iSv7U"
   },
   "outputs": [],
   "source": [
    "# 聊天模型\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化 LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ItyzJnGSv7U"
   },
   "source": [
    "聊天历史会通过 checkpointer 保存在短期记忆中。\n",
    "\n",
    "随后，聊天机器人会“反思”聊天历史。\n",
    "\n",
    "并基于历史生成新的长期记忆并写入 [LangGraph Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)。\n",
    "\n",
    "这些长期记忆会在未来会话中被读取，从而实现个性化回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ZkHmflJpSv7U",
    "outputId": "02e161a6-c52b-4d57-e211-4728c9231fee"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# 聊天机器人系统提示（说明如何使用已有记忆进行个性化）\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"您是一个具有记忆力的有帮助的助手，可以提供有关用户的信息。如果您有关于此用户的记忆，请使用它来个性化您的回复。以下是记忆（可能为空）：{memory}\"\"\"\n",
    "\n",
    "# 从聊天历史与现有记忆中提炼并生成新的长期记忆（写入 Store）\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"您正在收集用户信息以个性化您的回复。\n",
    "\n",
    "当前用户信息：\n",
    "{memory}\n",
    "\n",
    "指令：\n",
    "1. 仔细回顾以下聊天记录\n",
    "2. 识别关于用户的新信息，例如：\n",
    "   - 个人详细信息（姓名、位置）\n",
    "   - 偏好（喜欢、不喜欢）\n",
    "   - 兴趣和爱好\n",
    "   - 过往经历\n",
    "   - 目标或未来计划\n",
    "3. 将任何新信息与现有记忆合并\n",
    "4. 将记忆格式化为清晰的、项目符号列表\n",
    "5. 如果新信息与现有记忆冲突，保留最新版本\n",
    "\n",
    "请记住：只包含用户直接陈述的事实信息。不要做假设或推断。\n",
    "\n",
    "根据以下聊天记录，请更新用户信息：\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"\n",
    "    调用语言模型，并利用长期记忆进行个性化回复。\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 当前图的状态，包含了聊天历史。\n",
    "        config (RunnableConfig): 运行配置，包含可配置参数，如用户 ID。\n",
    "        store (BaseStore): 用于存储和检索长期记忆的 Store 实例。\n",
    "\n",
    "    Returns:\n",
    "        dict: 包含模型回复消息的字典。\n",
    "    \"\"\"\n",
    "\n",
    "    # 从配置中获取用户 ID（用于命名空间隔离，区分不同用户的记忆）\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 从存储中读取该用户的长期记忆\n",
    "    # 命名空间 (namespace) 用于组织数据，这里使用 (\"memory\", user_id) 来表示特定用户的记忆空间\n",
    "    # 键 (key) 用于唯一标识存储的对象，这里使用 \"user_memory\"\n",
    "    namespace = (\"memory\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace, key)\n",
    "\n",
    "    # 提取记忆内容（存储的值是一个字典，实际记忆内容存储在 'memory' 键下）\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\" # 如果没有找到记忆，则使用默认提示\n",
    "\n",
    "    # 将用户的长期记忆注入到系统提示中，用于个性化模型回复\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "\n",
    "    # 调用语言模型生成回复\n",
    "    # 输入包括带有注入记忆的系统提示和当前的聊天历史\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    # 返回更新后的状态，将模型的回复添加到消息列表中\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"\n",
    "    反思聊天历史，提取新的用户信息，并更新长期记忆到 Store 中。\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 当前图的状态，包含了聊天历史。\n",
    "        config (RunnableConfig): 运行配置，包含可配置参数，如用户 ID。\n",
    "        store (BaseStore): 用于存储和检索长期记忆的 Store 实例。\n",
    "    \"\"\"\n",
    "\n",
    "    # 从配置中获取用户 ID\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 从存储中检索该用户的现有长期记忆\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # 提取现有记忆内容\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\" # 如果没有找到记忆，则使用默认提示\n",
    "\n",
    "    # 准备用于生成新记忆的系统提示\n",
    "    # 将现有记忆和聊天历史注入到 CREATE_MEMORY_INSTRUCTION 提示中\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "\n",
    "    # 将提炼出的新记忆写入（或覆盖）Store 中的现有长期记忆\n",
    "    # 键 (key) 仍然是 \"user_memory\"\n",
    "    # 值 (value) 存储为一个字典 {\"memory\": new_memory.content}\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "\n",
    "# 定义图（节点与边）\n",
    "# StateGraph 用于构建基于状态的图\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# 添加节点，将 call_model 和 write_memory 函数注册为图中的节点\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "# 定义边，指定节点之间的流程顺序\n",
    "# 从 START 节点开始，首先进入 \"call_model\" 节点\n",
    "builder.add_edge(START, \"call_model\")\n",
    "# 从 \"call_model\" 节点处理完成后，进入 \"write_memory\" 节点\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "# 从 \"write_memory\" 节点处理完成后，结束当前流程，进入 END 节点\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# 初始化长期记忆（跨线程）存储实例\n",
    "# InMemoryStore 是一个简单的内存存储实现，适合示例和本地开发\n",
    "# 在生产环境中，需要使用持久化的 Store 实现，如 RedisStore, PostgresStore 等\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# 初始化短期记忆（单线程）检查点实现\n",
    "# MemorySaver 用于在单个会话线程中保存和恢复聊天历史\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# 编译图：将定义的节点和边连接起来，并挂载短期记忆检查点与长期记忆存储实例\n",
    "# checkpointer 参数用于配置短期记忆\n",
    "# store 参数用于配置长期记忆\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# 可视化查看图结构\n",
    "# get_graph() 方法用于获取图的结构信息\n",
    "# xray=1 参数通常用于显示更详细的图信息\n",
    "# draw_mermaid_png() 方法将图结构转换为 Mermaid 格式并渲染为 PNG 图片，便于理解图的流程\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l53f8RvmSv7U"
   },
   "source": [
    "与聊天机器人交互时，我们需要提供两类标识：\n",
    "\n",
    "1. `短期记忆（单线程内）`：提供 `thread ID` 以持久化该线程的聊天历史。\n",
    "2. `长期记忆（跨线程）`：提供 `user ID` 用作命名空间前缀，将长期记忆与特定用户绑定。\n",
    "\n",
    "下面一起看看它们如何协同工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zj8yTUAbSv7V",
    "outputId": "f34810e2-a03f-4ab6-92c6-6af4b75e91c1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "嗨，我是FLY\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "嗨，FLY！很高兴认识你！有什么我可以帮忙的吗？\n"
     ]
    }
   ],
   "source": [
    "# 提供线程 ID（短期记忆）\n",
    "# 提供用户 ID（长期记忆）\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# 用户输入\n",
    "input_messages = [HumanMessage(content=\"嗨，我是FLY\")]\n",
    "\n",
    "# 运行图\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyOlyLJKSv7V",
    "outputId": "ce20438b-374a-4511-f753-8169bf9a67e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "我喜欢在湖边骑自行车\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "那听起来很棒，FLY！湖边骑自行车一定很惬意吧，微风拂面，风景优美。你通常喜欢在早晨、傍晚，还是某个特定的季节骑车呢？或者，有没有什么特别的湖是你最喜欢的？\n"
     ]
    }
   ],
   "source": [
    "# 用户输入\n",
    "input_messages = [HumanMessage(content=\"我喜欢在湖边骑自行车\")]\n",
    "\n",
    "# 运行图\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK_0v_8lSv7V"
   },
   "source": [
    "我们使用 `MemorySaver` 作为单线程内（短期）记忆的检查点实现。\n",
    "\n",
    "它会把聊天历史保存到对应的线程。\n",
    "\n",
    "我们可以查看该线程中保存的聊天历史。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaPcWQNaSv7V",
    "outputId": "36bb181e-e3fb-4420-e2b0-fb44d714b6c2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "嗨，我是FLY\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "嗨，FLY！很高兴认识你！有什么我可以帮忙的吗？\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "我喜欢在湖边骑自行车\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "那听起来很棒，FLY！湖边骑自行车一定很惬意吧，微风拂面，风景优美。你通常喜欢在早晨、傍晚，还是某个特定的季节骑车呢？或者，有没有什么特别的湖是你最喜欢的？\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = graph.get_state(thread).values\n",
    "for m in state[\"messages\"]:\n",
    "    # 逐条打印该线程保存的消息\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nPs18gVSv7V"
   },
   "source": [
    "回顾一下，我们在编译图时配置了用于长期记忆（跨线程）的存储：\n",
    "\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "\n",
    "同时，我们在图中添加了一个节点（`write_memory`），该节点会“反思”聊天历史并把记忆写入存储。\n",
    "\n",
    "现在我们来验证记忆是否已保存到存储中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E7q-srwxSv7V",
    "outputId": "54b1c204-5ded-4791-b364-8593a3b8e782"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'namespace': ['memory', '1'],\n",
       " 'key': 'user_memory',\n",
       " 'value': {'memory': '**更新的用户信息：**\\n\\n- 名字：FLY\\n- 兴趣和爱好：\\n  - 喜欢在湖边骑自行车'},\n",
       " 'created_at': '2025-09-17T07:42:14.259762+00:00',\n",
       " 'updated_at': '2025-09-17T07:42:14.259765+00:00'}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 验证：从跨线程存储中读取该用户的长期记忆\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thzZA04eSv7V"
   },
   "source": [
    "接下来我们以“相同的用户 ID”启动“新的线程”。\n",
    "\n",
    "此时应当看到：机器人能够基于长期记忆中的用户画像进行个性化回复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jchwbuHSv7V",
    "outputId": "14ca7af9-6ae7-4b54-ed8c-3361c45bf760"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "嗨！你推荐我去哪里骑自行车？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "嗨，FLY！知道你喜欢在湖边骑自行车，我觉得这是个很棒的爱好！以下是一些适合骑行的湖边地点推荐，希望能激发你的兴趣：\n",
      "\n",
      "1. **杭州西湖（中国）**  \n",
      "   西湖的骑行道风景如画，环湖一圈大约15公里，既能欣赏湖光山色，又能感受浓厚的文化气息。\n",
      "\n",
      "2. **日内瓦湖（瑞士）**  \n",
      "   如果你有机会去欧洲，日内瓦湖周围的骑行路线非常适合喜欢湖边骑行的人。湖水清澈，周围还有阿尔卑斯山的壮丽景色。\n",
      "\n",
      "3. **太湖（中国江苏/浙江）**  \n",
      "   太湖的环湖骑行路线非常受欢迎，尤其是鼋头渚附近，湖面开阔，风景优美。\n",
      "\n",
      "4. **千岛湖（中国浙江）**  \n",
      "   千岛湖的骑行道非常适合喜欢自然风光的人，湖中星罗棋布的小岛让骑行体验更加独特。\n",
      "\n",
      "5. **明尼哈哈湖（美国明尼苏达州）**  \n",
      "   如果你喜欢安静的湖边骑行，这里是个不错的选择，周围还有很多绿地和公园。\n",
      "\n",
      "如果你有具体的时间或地点限制，可以告诉我，我再帮你推荐更合适的地方！另外，如果你最近有计划骑行，记得带好装备，注意安全哦！\n"
     ]
    }
   ],
   "source": [
    "# 提供相同的用户 ID（长期记忆），以及一个新的线程 ID\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# 用户输入\n",
    "input_messages = [HumanMessage(content=\"嗨！你推荐我去哪里骑自行车？\")]\n",
    "\n",
    "# 运行图\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAscdB5jSv7W",
    "outputId": "5cde93a0-bfa8-4220-c74a-167f5092f0dc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "太好了，附近有我可以去的咖啡店吗？我骑自行车后喜欢来喝一杯咖啡。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "骑完自行车后喝一杯咖啡，真是再惬意不过了！FLY，我不知道你目前的具体位置，但我可以给你一些建议，帮助你找到适合的咖啡店：\n",
      "\n",
      "1. **骑行路线附近的独立咖啡店**  \n",
      "   很多湖边骑行路线附近都会有一些独立咖啡店，它们通常有独特的氛围和手工咖啡。如果你骑行的地方是热门景点，试着在地图上搜索“咖啡”或“咖啡馆”，看看附近的推荐。\n",
      "\n",
      "2. **连锁咖啡店**  \n",
      "   如果你喜欢稳定的口味，像星巴克、Costa Coffee 或 Luckin Coffee（瑞幸咖啡）这样的连锁店通常很容易找到。\n",
      "\n",
      "3. **湖边咖啡馆**  \n",
      "   有些湖边会有专门的湖景咖啡馆，边喝咖啡边欣赏湖景，绝对是骑行后的完美放松方式。\n",
      "\n",
      "如果你告诉我你骑行的具体湖边位置，我可以帮你查找附近的咖啡店，甚至推荐一些评价不错的店铺！或者，如果你有偏好的咖啡类型（比如手冲、拿铁、冷萃），也可以告诉我，我会帮你找到更符合你口味的地方！\n"
     ]
    }
   ],
   "source": [
    "# 用户输入\n",
    "input_messages = [HumanMessage(content=\"太好了，附近有我可以去的咖啡店吗？我骑自行车后喜欢来喝一杯咖啡。\")]\n",
    "\n",
    "# 运行图\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JluZKVFjSv7W"
   },
   "source": [
    "## 在 LangSmith 中查看追踪\n",
    "\n",
    "你可以看到长期记忆会从存储中被读取，并作为系统提示的一部分传入模型（符合预期）：\n",
    "\n",
    "https://smith.langchain.com/o/7bfa9385-4ac5-468a-a06c-ffd7dbac42ec/projects/p/27f0e396-e7ab-4eac-9501-8df28b729149?timeModel=%7B%22duration%22%3A%227d%22%7D&peek=3068d3e2-cb03-4564-8cda-cc7679b09d24&peeked_trace=c1df4e6a-ca89-47fa-bd59-fb83ca4fc24c&runtab=0\n",
    "\n",
    "![在 LangSmith 中查看追踪](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509171549387.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRkQZQGsSv7W"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}