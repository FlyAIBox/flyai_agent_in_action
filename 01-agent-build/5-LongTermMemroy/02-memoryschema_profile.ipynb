{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "# 🔧 环境配置和检查\n",
    "\n",
    "## 概述\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "- 使用统一的conda环境\n",
    "- 通过国内镜像源快速安装依赖\n",
    "- 加速模型下载\n",
    "- 检查系统配置\n",
    "\n",
    "## 配置步骤\n",
    "1. **Conda环境管理** - 激活统一的学习环境\n",
    "2. **包管理器优化** - 配置pip使用清华镜像源\n",
    "3. **模型下载加速** - 设置HuggingFace镜像代理\n",
    "4. **系统环境诊断** - 检查硬件和软件配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 激活conda环境\n",
    "%%script bash\n",
    "# 初始化 conda\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/langchain-academy/blob/fly101/module-5/memoryschema_profile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEseDSUrxhUy"
   },
   "source": [
    "# 基于用户档案模式(Schema)的聊天机器人\n",
    "\n",
    "\n",
    "核心目标是将之前的简单字符串记忆升级为一个结构化的 **用户档案 (user profile)**，并利用 **Trustcall** 库来管理和更新这个档案。\n",
    "\n",
    "---\n",
    "\n",
    "### **大模型聊天机器人中的记忆机制：从字符串到结构化档案**\n",
    "\n",
    "在构建一个能够进行有意义对话的聊天机器人时，让它记住用户的信息至关重要。这通常涉及到两种类型的记忆：\n",
    "\n",
    "1.  **短期记忆 (Short-Term Memory)**：也称为“会话记忆”或“上下文记忆”。它存储了当前对话线程中的信息。举例来说，当您问“巴黎的天气怎么样？”然后立即问“那伦敦呢？”，机器人需要记住“伦敦”是与“天气”相关的。这种记忆通常在对话结束后就会被清除。\n",
    "    \n",
    "2.  **长期记忆 (Long-Term Memory)**：也称为“跨会话记忆”。它允许机器人在不同的对话之间记住关于用户的信息。例如，如果您告诉机器人您最喜欢的颜色是蓝色，它应该在下一次与您聊天时也能记住这一点。这正是 **LangGraph Memory Store** 的用途。\n",
    "    \n",
    "---\n",
    "\n",
    "### **从简单字符串记忆到结构化档案的演变**\n",
    "\n",
    "之前我们实现的机器人只是简单地将事实（例如“用户的名字是小明”）作为**字符串**存储起来。这种方式虽然简单，但效率低下且难以管理。\n",
    "\n",
    "**存在的问题**：\n",
    "\n",
    "-   **缺乏结构**：所有信息都混在一起，难以快速提取特定信息。比如，要找到用户的年龄，可能需要扫描一长串文本。\n",
    "-   **难以更新**：如果需要更新用户的某个信息，比如名字，就必须找到并替换字符串中的那一部分，这很容易出错。\n",
    "\n",
    "---\n",
    "\n",
    "### **引入“用户档案”和 Trustcall 库**\n",
    "\n",
    "为了解决这些问题，我们的目标是将记忆转化为一个**单一的、持续更新的结构化档案**，即“用户档案”。这个档案就像一个数据库记录，其中包含了一系列字段，例如：\n",
    "\n",
    "-   **姓名**：`string`\n",
    "-   **年龄**：`number`\n",
    "-   **城市**：`string`\n",
    "-   **兴趣爱好**：`list of strings`\n",
    "\n",
    "这个用户档案提供了清晰的结构，让机器人可以快速访问和修改特定信息。\n",
    "\n",
    "为了自动化地更新这个复杂的结构，我们将引入一个名为 **Trustcall** 的库。\n",
    "\n",
    "**Trustcall 的作用**：\n",
    "\n",
    "当用户在对话中提到新的信息（例如“我的生日是1995年1月1日”），**Trustcall** 会利用其能力，识别出这些信息，并根据预定义的档案模式，**自动地**将其解析并更新到相应的字段中，而不是简单地将整个句子作为字符串存储。\n",
    "\n",
    "这种方法被称为“在热路径（in the hot path）中存储记忆”，意味着在用户与机器人实时对话的过程中，记忆就被立即、有结构地更新和保存下来。\n",
    "\n",
    "---\n",
    "\n",
    "### **总结**\n",
    "\n",
    "这个项目旨在展示一个更高级的记忆管理方法：\n",
    "\n",
    "1.  **将记忆从无结构的字符串升级为有结构的**用户档案**。**\n",
    "2.  **利用** LangGraph Memory Store **实现跨会话的长期记忆。**\n",
    "3.  **使用** Trustcall **库自动化档案的更新，从而在实时对话中高效地保存新的用户事实。**\n",
    "\n",
    "这不仅让机器人能更可靠地记住用户，也为开发者提供了一种更高效、更可扩展的方式来管理和利用这些记忆。\n",
    "\n",
    "---\n",
    "\n",
    "## 回顾\n",
    "\n",
    "我们介绍了 [LangGraph 内存存储](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 作为保存和检索长期记忆的一种方式。\n",
    "\n",
    "我们构建了一个简单的聊天机器人，它同时使用 `短期记忆（线程内）` 和 `长期记忆（跨线程）`。\n",
    "\n",
    "它保存了长期 [语义记忆](https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory)（关于用户的事实信息）[\"在热路径中\"](https://langchain-ai.github.io/langgraph/concepts/memory/#writing-memories)，即用户与机器人聊天时。\n",
    "\n",
    "## 目标\n",
    "\n",
    "我们的聊天机器人将记忆保存为字符串。在实践中，我们通常希望记忆具有结构。\n",
    "\n",
    "例如，记忆可以是一个 [单一的、持续更新的模式(Schema)](https://langchain-ai.github.io/langgraph/concepts/memory/#profile)。\n",
    "\n",
    "在我们的案例中，我们希望这是一个单一的用户档案。\n",
    "\n",
    "我们将扩展我们的聊天机器人，将语义记忆保存到单个 [用户档案](https://langchain-ai.github.io/langgraph/concepts/memory/#profile) 中。\n",
    "\n",
    "我们还将介绍一个库 [Trustcall](https://github.com/hinthornw/trustcall)，用于使用新信息更新这个模式(Schema)。\n",
    "\n",
    "## 技术背景\n",
    "\n",
    "**什么是结构化记忆？**\n",
    "- 传统上，聊天机器人的记忆是简单的文本字符串\n",
    "- 结构化记忆使用预定义的数据模式(Schema)来组织信息\n",
    "- 这使得记忆更加可预测、可查询和可维护\n",
    "\n",
    "**为什么需要用户档案模式(Schema)？**\n",
    "- 提供一致的用户信息存储格式\n",
    "- 便于在不同会话间保持用户偏好\n",
    "- 支持更智能的个性化响应\n",
    "\n",
    "\n",
    "\n",
    "> 通俗来说，Schema 就像一张蓝图。\n",
    "\n",
    "> 它定义了数据的结构和组织方式，而不是具体的数据内容。\n",
    "\n",
    "> 举个简单的例子，假设我们要记录一个人的信息：\n",
    "\n",
    "> Schema 就是这张“信息表”的设计图。它会规定表里有哪些列（比如：姓名、年龄、城市），每列的数据类型是什么（姓名是文本，年龄是数字），以及这些列的排列顺序。\n",
    "\n",
    "> 实际数据才是具体的内容，比如：张三，30，北京。这些具体内容必须严格遵守 Schema 的规定。你不能在“年龄”那一栏填入“二十岁”这样的文本，因为 Schema 规定它必须是数字。\n",
    "\n",
    "> 所以，Schema 的核心作用是提供规则和约束，确保数据的一致性和有效性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erpj3JETxhU8"
   },
   "outputs": [],
   "source": [
    "# 安装必要的依赖包\n",
    "# 这些包是构建基于用户档案的聊天机器人所必需的\n",
    "%%capture --no-stderr\n",
    "# %pip install -U langchain_openai langgraph trustcall langchain_core\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langgraph==0.6.7 trustcall==0.0.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "toZb1LfoxhVB",
    "outputId": "7bf02c9a-69eb-4492-9bfc-ee6bd0113cec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY: ··········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 环境变量名称\n",
    "    \"\"\"\n",
    "    # 检查环境变量是否已在操作系统中设置\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        # 如果未设置，提示用户输入\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "    # 为当前进程设置环境变量\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "# 设置 LangSmith API 密钥用于追踪和监控\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # 启用追踪功能\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"FlyAIBox\"  # 设置项目名称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U6Tuo2pxhVC"
   },
   "source": [
    "## 定义用户档案模式(Schema)\n",
    "\n",
    "Python 有多种不同的 [结构化数据](https://python.langchain.com/docs/concepts/structured_outputs/#schema-definition) 类型，如 TypedDict、字典、JSON 和 [Pydantic](https://docs.pydantic.dev/latest/)。\n",
    "\n",
    "让我们从使用 TypedDict 定义用户档案模式(Schema)开始。\n",
    "\n",
    "### 什么是模式(Schema)？\n",
    "- **模式(Schema)** 是数据的结构定义，类似于数据库表结构\n",
    "- 它定义了数据应该包含哪些字段以及每个字段的类型\n",
    "- 在聊天机器人中，模式(Schema)帮助我们以一致的方式存储和检索用户信息\n",
    "\n",
    "### TypedDict vs Pydantic\n",
    "- **TypedDict**: Python 内置，轻量级，主要用于类型提示\n",
    "- **Pydantic**: 功能更强大，支持数据验证、序列化等高级功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO6aMc-RxhVD"
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "\n",
    "class UserProfile(TypedDict):\n",
    "    \"\"\"\n",
    "    用户档案模式(Schema)，包含类型化的字段\n",
    "\n",
    "    这个类定义了用户档案的基本结构，包含用户名和兴趣列表\n",
    "    使用 TypedDict 确保类型安全，同时保持字典的灵活性\n",
    "    \"\"\"\n",
    "    user_name: str  # 用户的首选名称\n",
    "    interests: List[str]  # 用户兴趣列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBSxuQyNxhVE"
   },
   "source": [
    "## 将模式(Schema)保存到存储中\n",
    "\n",
    "[LangGraph 存储](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) 接受任何 Python 字典作为 `value`。\n",
    "\n",
    "### 存储系统的工作原理\n",
    "- **命名空间（Namespace）**: 用于组织和隔离不同用户的数据\n",
    "- **键（Key）**: 在命名空间内唯一标识特定数据的标识符\n",
    "- **值（Value）**: 实际存储的数据，可以是任何 Python 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeUJH1tsxhVF",
    "outputId": "1b1ad75b-057d-4d0a-a2e8-b95ae1d9d9b5"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'interests': ['骑自行车', '技术', '咖啡']}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# 创建 TypedDict 实例\n",
    "# 这里我们创建一个具体的用户档案示例\n",
    "user_profile: UserProfile = {\n",
    "    \"user_name\": \"FLY\",\n",
    "    \"interests\": [\"骑自行车\", \"技术\", \"咖啡\"]\n",
    "}\n",
    "user_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIH8-YSQxhVI"
   },
   "source": [
    "我们使用 [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) 方法将 TypedDict 保存到存储中。\n",
    "\n",
    "### put 方法详解\n",
    "- **功能**: 将数据存储到指定的命名空间和键下\n",
    "- **参数**:\n",
    "  - `namespace`: 命名空间元组，用于组织数据\n",
    "  - `key`: 数据键，在命名空间内唯一\n",
    "  - `value`: 要存储的数据对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cb-hSNHUxhVK"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# 初始化内存存储\n",
    "# InMemoryStore 是一个简单的内存存储实现，适合演示和开发\n",
    "in_memory_store = InMemoryStore()\n",
    "\n",
    "# 为要保存的记忆设置命名空间\n",
    "# 命名空间格式: (用户ID, 记忆类型)\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "\n",
    "# 将记忆保存到命名空间，使用键值对存储\n",
    "key = \"user_profile\"  # 在命名空间内的唯一标识符\n",
    "value = user_profile  # 要存储的用户档案数据\n",
    "in_memory_store.put(namespace_for_memory, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT2mSab4xhVM"
   },
   "source": [
    "我们使用 [search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.search) 方法通过命名空间从存储中检索对象。\n",
    "\n",
    "### search 方法详解\n",
    "- **功能**: 搜索指定命名空间下的所有对象\n",
    "- **返回**: 包含匹配对象的迭代器\n",
    "- **用途**: 查看命名空间内存储的所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJPNBaMlxhVN",
    "outputId": "5698f9be-cc4e-4ace-a94c-19f9d2553678"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'namespace': ['1', 'memory'], 'key': 'user_profile', 'value': {'user_name': 'FLY', 'interests': ['骑自行车', '技术', '咖啡']}, 'created_at': '2025-09-17T08:59:13.224498+00:00', 'updated_at': '2025-09-17T08:59:13.224501+00:00', 'score': None}\n"
     ]
    }
   ],
   "source": [
    "# 搜索并显示存储的数据\n",
    "# 遍历命名空间中的所有对象并打印其详细信息\n",
    "for m in in_memory_store.search(namespace_for_memory):\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulDh74HixhVO"
   },
   "source": [
    "我们也可以使用 [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) 方法通过命名空间和键检索特定对象。\n",
    "\n",
    "### get 方法详解\n",
    "- **功能**: 根据命名空间和键精确检索单个对象\n",
    "- **参数**:\n",
    "  - `namespace`: 命名空间元组\n",
    "  - `key`: 对象键\n",
    "- **返回**: 包含对象数据的存储项\n",
    "- **用途**: 快速获取特定的用户档案数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cmd2c-14xhVO",
    "outputId": "2aeeb705-6f02-4928-c2ed-c441c3b345c3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'interests': ['骑自行车', '技术', '咖啡']}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# 通过命名空间和键获取记忆\n",
    "# 精确检索特定的用户档案数据\n",
    "profile = in_memory_store.get(namespace_for_memory, \"user_profile\")\n",
    "profile.value  # 获取存储项的值部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQBbNNfOxhVP"
   },
   "source": [
    "## 基于档案模式(Schema)的聊天机器人\n",
    "\n",
    "现在我们知道如何为记忆指定模式(Schema)并将其保存到存储中。\n",
    "\n",
    "但是，我们如何实际使用这个特定模式(Schema)*创建*记忆呢？\n",
    "\n",
    "在我们的聊天机器人中，我们 [希望从用户聊天中创建记忆](https://langchain-ai.github.io/langgraph/concepts/memory/#profile)。\n",
    "\n",
    "这就是 [结构化输出](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 概念发挥作用的地方。\n",
    "\n",
    "LangChain 的 [聊天模型](https://python.langchain.com/docs/concepts/chat_models/) 接口有一个 [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 方法来强制结构化输出。\n",
    "\n",
    "当我们希望确保输出符合模式(Schema)并为我们解析输出时，这非常有用。\n",
    "\n",
    "### 什么是结构化输出？\n",
    "- **定义**: 确保 AI 模型输出符合预定义格式的技术\n",
    "- **优势**:\n",
    "  - 保证数据格式一致性\n",
    "  - 便于后续处理和存储\n",
    "  - 减少解析错误\n",
    "- **实现**: 通过 `with_structured_output` 方法将模式(Schema)绑定到模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7Tc1mumxhVS",
    "outputId": "e157c31e-d332-46fe-e4a7-c591f595c33b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n"
     ]
    }
   ],
   "source": [
    "# 设置OpenAI API密钥\n",
    "# 您需要从 https://api.apiyi.com/v1 获取API密钥\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKaPS4ZtxhVT"
   },
   "source": [
    "让我们将创建的 `UserProfile` 模式(Schema)传递给 `with_structured_output` 方法。\n",
    "\n",
    "然后我们可以使用 [消息](https://python.langchain.com/docs/concepts/messages/) 列表调用聊天模型，并获得符合我们模式(Schema)的结构化输出。\n",
    "\n",
    "### 工作流程\n",
    "1. **模式(Schema)绑定**: 将用户档案模式(Schema)绑定到聊天模型\n",
    "2. **消息输入**: 提供用户对话消息\n",
    "3. **结构化输出**: 模型返回符合模式(Schema)的用户档案数据\n",
    "4. **数据验证**: 自动确保输出格式正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxlMqplExhVT",
    "outputId": "57ad02c1-e967-4119-8467-f251156d4a9d"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'interests': ['骑自行车']}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 初始化聊天模型\n",
    "# 使用 GPT-4o 模型，temperature=0 确保输出稳定\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 将模式(Schema)绑定到模型\n",
    "# 这确保模型输出符合 UserProfile 模式(Schema)\n",
    "model_with_structure = model.with_structured_output(UserProfile)\n",
    "\n",
    "# 调用模型生成符合模式(Schema)的结构化输出\n",
    "# 输入用户消息，输出结构化的用户档案\n",
    "structured_output = model_with_structure.invoke([HumanMessage(\"我是FLY，我喜欢骑自行车。\")])\n",
    "structured_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2stPiwyQxhVU"
   },
   "source": [
    "现在，让我们在聊天机器人中使用这个功能。\n",
    "\n",
    "这只需要对 `write_memory` 函数进行少量修改。\n",
    "\n",
    "我们使用上面定义的 `model_with_structure` 来生成符合我们模式(Schema)的档案。\n",
    "\n",
    "### 集成步骤\n",
    "1. **修改记忆写入函数**: 使用结构化输出模型\n",
    "2. **保持现有逻辑**: 其他部分保持不变\n",
    "3. **确保数据一致性**: 输出始终符合预定义模式(Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "ZvED02eExhVU",
    "outputId": "5a2511bc-929b-4614-919c-738ca30e9a6a"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAFNCAIAAABkBqGXAAAQAElEQVR4nOydCVwUZR/Hn5m9uZZbEEEEFBQPVBSPQgVBtLzSvPKo9LVDS0szDS01TZPsMLOyMsVUzKM83re8tdQ077wNQVA5BOQ+9pp5/7uD6wK7yC47u874fPXDZ/Z5nnn22fnN8zz/5xbSNI0wXEOIMBwEy8ZJsGycBMvGSbBsnATLxknsKVvmjdJ/z5YX5qoUlRStoTUUIglE0YggEU0hRBCIpkkBQWm0TRSSJChKd0EQFLg/+GgIQRLa9gzEgJDejyBIUkBr1NUOBCJonadAqI3ZsPlDwvfShKGT9ltoGhmEEYsJUkQ4Ogt8gx06x7ghO0HYvt125WTx6f33S/I1cC2WEkIxKYRngRClIZjnzcjG/CUFSKOhwQOoTiqjifaG2jETAp3e2lC0LhzjSkAk1APZQATd1yASZKMoRBH622kSRCMMRdKmgUaGLkIJgjdApdAoFbRGhURSolmIdMDLfsi22FS2a2dK/th2T6VA7r6iDk/L20S5Ii5TWan8c2tB5vUKRSXdNEgydIo/shW2k23Dx7eK8tTB7RwTJvgifpF5vezApryqCs2Al32ahzkh9rGRbKveSXVyFYxPbIH4y9/78k/vKQrp5Bg/hvX30hayfT3rZqtIWeyIpugJ4Jt3U+PGNglu54zYhHXZIJ9F9nPt2tcTPTF8O+dmQKhD/xdZzHMkYpPVc26GRzk/UZoBrywJzrhafu5QAWINFmVLWZ4hcyR7DW+Cnjyendz0+O5CxBpsyQbGVUGWatxcPtsg9dAs2ME7QPLTkluIHdiSbe9Puf4tJegJ5vlp/sV56uxbFYgFWJEtO728qowe9Krtmp+PJ+6+4v0b7iEWYEW2Q1vynd0E6Imn9zCPkvtqxAKsyFacp2oVyW7DpS6zZ8/esWMHMpObN28+++yziB18gxwFAnRsp/UznPVlqyhWatSo+wBbG/1XrlxB5mPZXQ3H0VWYea0SWRvrN7dP7c0/vb/otWUhiB2OHTuWnJx8+fJlT0/PDh06vPHGG3ARGRnJ+Do5OR0+fBjy0NatW0+dOpWVlRUUFDRkyJDhw4czAWJjYydNmnTw4MFz586NGzdu/fr1jPtbb731wgsvIGvzv7VZ2WlVExcGIati/fG2vLsKGIhB7HDt2rVp06a9+uqrCxYsSEtL+/LLL+fPn79y5UrQsmfPnvPmzRs8eDAEW758OQiWmJgIwz23bt36+OOPfX19IQB4iUSiX375pWvXriBe586dIcDevXt3796N2MHDV5x51frGpPVlUyqQUMiWPXL+/HmpVPryyy+TJOnj49OmTZvU1NS6wZYsWVJeXt60qbYXFDLizp07jx8/zsgGOsnl8pkzZyKb4CQX0ZT1X2Lry0ZXj1KyQkRERFVV1fTp06OioqKjo/39/fXFY8000CkpKZAFMzIyGBc/v4cjmSA2shUC7RC99Z+G9U0SiQSGmNmSLSwsbMWKFV5eXlA8Dh069PXXX79w4UKtMDBmDQUpVGxTp049dOjQ6dOnoQo0DCAWi5GtKC9VEyzUGNaXzc1HpFaxOKrQo0cPqMN27doFtVpxcTHkPLW6RtsI6j8wWMDE6NOnj7Ozth1SWlqK7MS9LIVAhKyO9WUL7eisrGJLtjNnzkAtBReQ4aC9NWPGDJAkOzvbMExRURH89fb2Zj6m6UB24v4dhczB+jW99WVz95VBsXD+CCvDFlAkzpo1a/v27YWFhZcuXYIKDPQDK1EikYBOJ06cgCIxICBAKBSCZV9SUgJmZFJSUrdu3WpJqwcC5+fnQ5tBXwtal6ICjV+IFFkbVnpJ5F7Cq3+XIRYYO3YsVGmffPJJXFzc5MmTHR0dV69eDSKBF5iXUJ9B/gNDcdGiRRcvXoyJiYGicsqUKdBoA431TTdDnnrqKTBzwLDcs2cPsjaV5WqaQjGjrD9eysro9pWTxQdT8qZ+xlaLmyts+fx2UZ7qP4ut3NZGLOW2NlFyaHHv25CDnmxyMxXdn/FALMDWrOSuCa5/7S6MM9FbpFAo+vXrZ9RLqVRCRwZhzGqGbqo1a9Ygdlirw6gXdJiVlRkv86G3ZdmyZUa9dnx7WyJFbXvIEQuwOAVo7YJ0J1fh8GnGR91MGeWgKNgXRr1AS3iCiB3ge+GNMeoF7qaaegKBwMHBwajXyrdSJy0KkDqy0kZkd+bW1++kxoz2Cu3Eyhv3OLN6zs3mrWX9xrM1x5DdmVtj5/jv35CHnjDWfZjm7CZkTzNkg3mSVZWa7xPTn5/u1yRAhp4AvktMbRnh3Pt5duer2WJWcmmxat38jObhsoGTbL0yxZaUFSk2fnzHxUMwamYgYhnbLd2A4h6+Knq4Z+tIHlZ1W1fczs1QhHd37m2TeaE2XSi1Z3122j/l0KRrEe7Yd4wP4j43ThedOVRcmKNydBVMmGe7SaF2WJb4e3LO7evlKgUtEBBiGekkF8qcSZFYoDFYZsgsK62G1i5OpAx8detMIQwMEBGGIQnd8kVmlSkTpjo2svp2/dpGnS/NtA6rFztqVzsS+qWozO0QXPeIHsYmJKmqSrqqXF1WrFZUaCNz9RLFjva2cc1tB9kYFJWK47uLstKqqko1alCMJijNQ19CpwlzDX4Ckqwxhlf9ZAnm0et/AjxcUFHzQCGNRkNARMSDVcUGWhLMamNm+Wq1i+7SYO2wdkUwPB9Gygc3CkSkUEiLpISrpygkwrl1V/sU+HaTzQZAv3NiYmLr1q0R7+DzTgkwfMoMDvAPLBsnwbJxEj7LplKpYDAB8RGc2zgJlo2TYNk4CZaNk2DZOAmfZYPOLSwbx4CsJhDwdiEyn2Xja1ZDPJaNx21thHMbR8GycRIsGyfBdRsnwbmNk2DZOAmWjZNg2TgJNkk4Cc5tnIS3P4wgCHd3d8RTeCsbSZJ5ebxdWsffYkQorLU7EJ/AsnESLBsnwbJxEiwbJ8GycRIsGyfBsnESLBsnwbJxEiwbJ8GycRI+y6bRaBBPYXeHO/siEAj4muH4LBuPy0k+L5TisWw83AWoQ4cOUDwy+2lRFAXjpfB37NixM2bMQHyBh4VkWFgYSEXoYPTz9/cfPXo04hE8lG3EiBEyWY395rp3784cCsYbeCjbsGHDWrR4uLWjt7f3yJEjEb/gpyU5ZswY/T7vHTt2DAqy/rkX9oWfsiUkJAQGBsKFh4cHGCOId1jHksy8Uf7v2VJFlUG8BpunGl4DAhLpN2qt5VXzLpogSMpg988HO6rWuqV6G9ZaXvfu3bty5Yqbu2uH9hGoeuNQIzEYbgyKdFuC6vcMrfuN2l1BEa0P+SDQw8Mh66bZcHNZAUnLvUTd+lvhjGQryPbD+zcVFbRIQqgUBvE+2H5V+xsNtmLVeWl/Z/WDMPCqFVK37yqq8QhI7Ya6hg9a/0yZZ234xCG66j15DQNXBzP8lgcbu5K63Vup+mRjJKolW42XQGvAIkpD130OgEiCNBqa0qA23Zx7D2vUTtiNbW5/MzvVq5koflxzhGkYWeklBzbec/YQde5t+aTpRuW2795LDWgt6zGIz7v7s8TGJakdY+Rd472QRVhukhzdlUtRCGtmGX4tpRf+KEaWYrlsd25UObjwuUuTVdr28FBVIYuxXDZlBa3bMx9jCc5ussaMBlqeXcAoIiiEsQztEG4jTHhcynESLJudaNzZ95bLBkMiuG6zGALZSTZt3YZVsxSabpRdgAtJToJlsw92KyRJAcnnaV9sQzSqB9/yJw8d2xRvzxBjH9pOuY3HB7/ZABo16unhus0+2K1uwzSGRuY2y+s2gZAgBaw33IY81zd5/fdwsW17St/4KGRzDh3e1yc2sqiosP5g+nQ2EIJolD3XiOa2Gje3LQc3tzkJl+o2GK3YsnXDuuTVcN2mdbsXJ7zSrp12WlV6+s2du7aePXcqJycrsHnQgAFDBg8ajiwCCiuI9s6dzG3bN7m6unXv9vTUKTM/Wjrv2LEj/v7Nx455OT7+GSYkuEBKMjLT5XLXkJDQaW+826RJ9Qnu33z7xd59/3WQOcTGJjRr9nCajFqt/mHNqhMnj967l9O2bcTQwSO6dXsKWYTd6jYLWP3dlzt2bFm44JO57y328mry7pw3MjNvgftXq5afOvXXtDffXbpkBWj2xYqPT5w8hixCJBKlbF4XEBC457fjkyZO+e33nW+9PTk2JmHfnhN9esclLf+wtKwUgp0+c/L9+e+AhD+n/O+DeUtzc7M/X7GUiWHHzq07dm6BxKxalezr65e8/jt95Cu+XLZ128ahQ0Zu3LCrV3TsBwtmHfnjALIHjTBJBKRAaMbtxSXFP2/5adSoCV0iu/Xs2WvmjLmRnbsV3M8Hr3nzliQlrerUsUvHiEjIZ6GtWv996jiylJYhYYMGDhOLxb17xcHH8PD2IJhQKOzTOx6yS2ZGOjiu+fHr6Kdjhg8bA1kNArz+2tsnThy9dv0KeG3/JaVXdF9QxcXZJaHfQEgVE61Codizd/eY0S9C5HIX+YD+g+FtMBTVLEjCbiMAFEGZ8d230m8i7XKY8OovFgoXLkiq9qPp7dtTTv597PbtDMYBXnNkKZDVmAtHR0f4GxgYzHyUybTTy0tLS+BvWtq/IIz+ltBWbeDvtWuX4Y25e/d2/4RBeq9WrapPW79x46pSqewS2V3vFdGhM+RmeB1BRWQmFM2R5naZrnSSSqS13CmKmv3eNJVK+Z9JUyMiIp2dnN+YNhE1AqLmi0zW6TotKyuDrCMxSAmzYKCiohyACpgRmEEqlRmmv27aCu8XWCAbZ0wSR0cnpHs0tdxv/HsNXvNPklZ17tSVcYEH5OXpjVhDKtUKVlVVqXcp16XKw90TMqhAIFAYTIuvrKxgLjw8tXMaZ7yd6Ofnbxibt7cPMh/OdG6BtQYF44V/zrZu3RbpujTnJE7v0yvO1U07OVev061bafC/xYOSjQ0gGVAYXr78j96FuQ4Kbgk5tUkTX+3H56u9wG5kLpr5BUgkEriACphxKSy8D79Cv7THLBrZ5LXcJBGKSFJoxpc7OTnF9R0AliTUB+fOn/5yZdKZMydBQrD44Tlu/nl9SWkJGJbgDjZLTm42YhOwBo8eO7xt2yb4UkjMqq8/BdOjZUgoeIH98sefB6FzBK43pay7cuUicwvIA00LsEEuXjwPlRzYkDNnvf75F0uRRdD2KiTVKoow850Bqxp+5/JPF0P9ERLcauH8JMZ8SHxvETShBg+JgfIncc6HYF7Oe3/mhJeGr/txK2IHMP3z8u9t3rJ+5arl0FwDmxZqVsZr7AsToSsL3p6FH86BZiUYmYs/mssMd4waOT44uNXGlLVnz/4NZX54m/YzZsxFltE4k8TyNQA/zk8H2YZND0QY86kqo1KS0t74PARZBO7csg9P1ngb1CvvJU435fvT+l+h+YyeACyXTSgkbT8EAJXN6tUbTflySDO79ZKo1WabJFbB14cPW1VwppcEY0WwbJwEy2YfiMZ1Sloum25rGj7FHwAAEABJREFUATwrwUIImmxM5daI6a3aShVPlbQQCtlrLgmWzH7guo2TYNk4CZaNk1gum1gGNqwAYSxDgMhGPDzLLUlHF4GiXIkwFpF5ubQx08ktv7XPCM/KcmxNWsjVv0vcvcXIUiyXTe4h8wkUb1iaijBmcvK37LJC5ah3ApClNHY/yb/+l3fhSLFvsINfS5lUauT10e6b+aAzhX7YrULX7WGBkGSdtbHMLp81XIy1GJm9QGvHh+pzIZidJun6b6iRpDqJqb5Dn6RaaaudKkpdkKvMuFJWVaGZ/JGF49oPY0aN49TevIvHyhSVGo3KiC/duB6wRzx7c260ICnMHqBmxFBvAFKIhCLk6i0aMb05ahw8PL5Bz7hx4+bMmdOmTRvEO/jcblOr1UIhP38glo2TYNk4CZ9lU6lUIpEI8RGc2zgJlo2TYNk4Ca7bOAnObZyEz7JpNBosG8eArCYQ8HYUl8+y8TWrISwbR8GycRIsGyfBsnES3v4wHre1Ec5tHAXLxkmwbJwEy8ZJsEnCSXBu4yR8HgHw9/dHPIW3shEEkZmZiXgKf4sRoRDKScRTsGycBMvGSbBsnATLxkmwbJwEy8ZJsGycBMvGSbBsnATLxkmwbJwEy8ZJbHrIpS2BEQCSJDUaDeIjvJUN8TrDYdk4CQ93AYqIiKh1sCVFUXFxcUlJSYgv8DC3BQUFkTXx8fGZOLFR550+bvBQtvj4+FoLEtu2bRsWFoZ4BA9lGzt2bLNmzfQf5XL5+PHjEb/goWxOTk5Dhw7VZ7jQ0ND27dsjfsFPS3LMmDFNm2rPC3NwcOBfVkMN7CVJv1pCqWrUFoTpQzdoQvsP1YvB7dq9Xel6I3xwj24f2IYBUQ2Nf3XHzp0BAQFeDu1u/lNu4tuZj7T+XN5HJ8N0MIIwck6s4dNoSOQ0rfbxFzu5y+oP9ogGQEpS+v1cDSRI0+D2TyO3a7UqLKTFzCjNTQEh0L6fIilKeNHXv6WjyWD1yPbTsjRlOf30UG+fFs4IY0OO7cxOPVc+LjFA7mF893KTsq1dkCYQoyGvByGMnUhemDpypp+nr5EC07hJcvmvwqpyCmtmX5oESnd/l2PUy7hsV/8ukTrxubuSE4RFOZaXGB/BMK6NoooQ8HeVEVfwDnAxZXoa10atpGgKH2BpZ0gaUSaGC3GWenyhTJ+DiWV7fCFMnzll3IMkcQlpf2jTnSrGZaMofMKX/SFNd+YZLyQht/H36BvuQJjUwLhskNtoGpeTdoaizZSNIAh8gL3dIQmBKaPEuDNN8/h8MM5A0RraRBsANwAeX3QNAOO5x3huI3h9HB9XoLV5zXhdZSK3kfVZnxjbAHWbqfazibqNskVmGzw0Nnn99whjAqjbTLWf7Tk6M3LEuPbtOjLXQ4fFZWXfRRgD6uncMtUAIG1Qs40Z/SJzkZOTXVRUiDA1oWmTfcmC+fPn13U9f6QQTJg23VxRw3hueHxVVVVEh85wXVxc1P+ZpzIy0nr36sv4Dh+RoNFobty4Nu/9GX5+/i9NHFFSWhzVtQcUkiqVCoaIJr/yAgTbvj0l9eb1mD791Gr1d9+v/GrV8u++//Kfi+ecnZybNXvEyeLp6TchDV0iuy1eMndZ0sI9e3aJRGIHmcOb0yet/OqTv08dDw5u5enphXQbFpqKfMhzfaVS2f4Dv787580dO7dkZt7q2LHLgg9nf7go8eChPY4OThAJE/LYsSOLFieuXLV81+5t586fbhse4eTkBO4fzJ/1558Hr12/8s6sKfDxrRmvRHaO8vb2Ye5KTb0x7Pl+L06YjBqGRkVfOloU1d+9rpfJrmSSMMMkiYzsduXqReb67LlTTZr4XLx0nvl4N+tOQUE+BBCLxRUV5Tt3bp0ze+HQwSP093aMiFyy+HO42PDTjkULl8PFii+Xbd22ceiQkRs37OoVHfvBgllH/jhQfwKYHT9BoQnjJx/cfyq8bQdQ5fMvlr47a/6e345LxBKIkwlZT+QQScrmdQEBgXDLpIlTfvt951tvT46NSdi350Sf3nFJyz8sLSuFYKfPnHx//jvx8c/8nPK/D+Ytzc3N/nzFUn0Maemp8H/xh58OGfw8PIf9B37TJ/LIH/vl8obmBKSz55FZJonWIjGdQ+vSqWOXS5fOM2bMhQtneveKKysrBcHg48WL51xd3VqGhELPC+TIUaMm9I1NqCf3KBSKPXt3Q/k5aOAwuYt8QP/B8OCS13/XkGTExiZASuCLekf3LS8vHzRoeJvWbYVCYXR0bGrqdUjeIyNvGRIGXvCGwU+Aj+Hh7UEwiKFP73jIppkZ6eC45sevo5+OGT5sDGgAAV5/7e0TJ45CDkO63qWcnKwFHyzr0SMafvXAZ4cdPLhHvzTy0OF9/eKfRQ2mns4tE5WedszAjNzWuVNURUUFlFRwDfmsXduIsLDwSxe1Ge7ixfOdO3XVhwwLDa8/qhs3riqVyi6R3fUuUPampaUWlxSjR+HvH8hcOOqKrKAWIcxHmVQGpTFE+8jIIatVx+ConaMYGBhcHYPMAf6WlpbA37S0f+HX6WMIbaU9jv3atcvMx+YBLaRSKXP9zIAhZeVlJ08e092VevfubXhRUIMx2yQxd1aol5e3v3/zS5cveHh4gnhQJVy9dgn069fvWag/Ro18OJ0bXuT6oyrTFURvTKu9rqnwfgHkj/rvrbWsrdbHhkRO1KwajMVQBllWIpHqXRwctIpC+c98FEskei/IcD179Dpw8HfIfFBCtmoZ1rx5C9Rg6inwTMlm9nxeyFJQvUFCg4JC4Je0a9fx628+A/Pkzp3M7t2ebng8HjrDYcbbiWC8GLrrK/bG0PjImZxUVVWpdynXCebh7mk0PGQ4MGpKSkuOHjs8oP8QZCWsNt7WqVPXr7/+zMnRuYPOnoRyEiyx/ft/g2LH3d2j4fE08wuQ6F5YMFUYl8LC+1AtMS91I2l85FDPhbZqffnyP3oX5joouKXR8FFRPV1c5Js3J2dkpEOljswB8jphlklibt2GtA+iS05u9l9//dE2vAPSFR1ghmz/JaVz56hH3uuvq1EOH9535eoluPHFCa+AmQCVItRDYObNnPU62ITIGlglcrBCIets27YJ8hBY/6u+/hTsIPixRgNDqds/YdC27Zt6dI82y4xEurXLtFkT7ijzB26g4RIa2gZqZvgNjAtYWb/8+rP+Yz34NW2W0G/gj2u/Ack/+/RbqAuhhbQxZe3Zs387OjqFt2k/Y8ZcZCUaHzmY/nn59zZvWQ/tNjDxIzt3+8+kqfWE79Gj17rk7+LjnkHWw3hPf/KiWxSFhk0LRJhGk7I5GVqrP63/ta6BUz+KciolKW3qZyF1vUyPt+ERgEZz/vyZrOw765JXz/9gmbmaoXrLPBN9ko/flISNm9Zu2rTWqFfzwKCVK9agx49Zs6cKBIKJL78OPXnIAggzZ27pqsLHS7mBA4f16RNv1EsoeEzH6Pf+/hdqDIRJ3UyNANQ3t9IuQJ8v/EdPFKanq5oa3SZIM7okMbbGxAgA5DYCmyR2htTpYNTLeG7TqM0aAMCwAgU2hgkLw8TMLRJPb7U/9Whgom6jcG6zP/VYhSYsSdImk0kw9UIik9NbTbZ4sGp2B+o2ZFbdpm0wYN0eY+rpk0SYxxbjsolFhFqATUk7IxAg84ZJJU4EpebnVuwcIutWmcDkGg1jdIh2rijFstmZK38VOcjNMUmC27s5uQm3fZGGMPYj745q9CzjR4fXt47tl6/u5GdVRfT2COvqhjC2oqy48uTu/KybikmLWohlAqNhHrH88JdVt3MzlNBFSTWm06Tewbt6dmU15WVyG1SzRgmNBTYRc+2gRoLVic1I4huQPFKgDSJ1IkbPbCpzMrmHa4NWjVYWVpZV1pL9Ycp1V0Z2nCURQenCPPR7cGUYWh8MMatYmVuYX637Z8S95pfpk/LwQhfgw4ULxo4b16JFEPMEDe/SWmgUsx8uYdhEJXQbXdXSRbcpsG7r3Af3P0wYodtXt2aSmJsZI7CW48Ovp3VfTdfZxUKj8fJ/xI67qIFrt2VuMhkHi8n8knS5J+HVVIx4B5+X3KvVaiFPt1fEsnESLBsnwbJxEj7LplKpmFWm/APnNk6CZeMkWDZOgus2TsJb2TQajW41Jj8He3krG49LSIRl4yi8/WE8rtgQzm0cBcvGSbBsnITPsuG6jXvg3MZJsGycBMvGSaDdhmXjHji3cZXmzZsjnsJb2WiazszMRDyFv8WIUAjlJOIpWDZOgmXjJFg2ToJl4yRYNk6CZeMkWDZOgmXjJFg2ToJl4yRYNk6CZeMk9jwJmFWYUy4oip+b0PJWNqQ7KRTGuBEf4bNsPC4nCVscZ29bEhK0p9tB8VhQUCCRSOBCqVRGRESsWfM4noNjGTw0SQiCyMvLYy5AMLhwc3N77bXXEI/gYSHZs2fPWkVIy5Ytu3R59Ol/HIKHsr300kt+fn76j46OjmPGjEH8goeygWYxMTH6j4GBgdHR0Yhf8NOSHD9+fEBAANKdITtq1CjEO/gpm7u7e3x8PLS4Qbz+/fsj3mHnBsDx3XmZVyuKC9TaM6xo7U6ZtU6a0+2uabDbQZ0NUOG2WtshGN/zte7OqSb2Uq1ny1j4HoGQkDqRHr7ingM93X0kyE7YTbb1H6UX52ngYYhkQpmzyMFdJnUQCYTCR21LW2e/WyMb4NZyIqpP+XkQs1Znmja5BS6zrSoTxhANrVBWVRQrK4tVqgqVWkVJJER4d5ceA72QzbGDbJs/y8zLVAokpF9rTxdvR8RZbl/MLc2rFIpQ/5ea+Ld0QjbEprKVlSqSF9wmRWRYNH9m59++dK84u7xZS+mQ15ohW2E72QqyKzcl3fVs7uLTygPxjut/ZEgdiAnzWiCbYCPZcjMqt3x+t228jX6VXbh66FaTAOlzU/0Q+9hCtsK8yg0f8VwzhutHMyRS9OK8IMQytmi3bVx61zf8iTi3I/Sp5uUl1L71WYhlWJctedEtiZPIw88VPRmE9Qq4frYCsQy7sqWeLy4tVId0s52JZXcEAoHURfzj/HTEJuzKdmRbgYOr3boS7EVIN7/yYk32LRbzHIuyFd1XVJZTLSKboseVpC9Hb9u1DLGA2FF4ICUPsQaLsh3alCcQ83muSj14BMiL81mcfcTiY827o3SQS9ETiYe/C61B/54vQezA4lwSZRXlHeaA2EGjUf+2/5urN44VFeW0aN6hR9TzbUJ7gnt27s3lK8e8+cqag3+su3T1iNzFO6Jd3IC4KWApgG/OvbSUbQtz89JDgjr37fUyYhNCiG6cKmkZ4YJYgK3cVpSnnXvj6u2M2OGX3Z/8+demp6Kef2/Gr+3CY5JTZv9z6SC4CwXaXe227FjSsX2/pR8cHTN8wZFjGy5c3o+0G8yovk+e7ir3nvXm5mfipx4++lNpaT5iDXhR8nPYmu7Hlmw56ZVmHFxoJiqV4vT5/8Y8PaF71+ccHeRRnQeBSP1GcwAAAAP5SURBVPsO/6AP0CE8pkPbWKFQFNyik4eb352718Dx4pVDRcW5g/q/5ebq4+MdNPTZmZVVpYg1RFKBspKtY3nZkq28TMPegeu3s66q1cpWIVF6l+DATtm5qeUVxczHZk1b672kUmdGnvyC22KR1N3Nl3F3cfZ0lTdBrEHCiA7J1uNlq24Tikj2+jqrKsvg71ffT67lXlpWICC1v4ggjDyvisoSsaRGXSsSsmgxUbSGvUMI2JLNs6mEvUS7uHjC3+GD53i61zjf2E3uU2K6unKQuSgUNZrAVYpyxBq0SiORsvUI2JLNL1h7LmplmULmZP1eEi+PAJFIGy0YhIxLadl9GMqQQGYyXVu5ufqqVFVQlvo2CYGPd7NvlJSy2CLWKDVOPmwdi8piuw1suvt3WGm4gDzxff6z79APaRnnVWol2JCr176xffcj+jvCW0cLheItvy5RKquKS/J++nmug4McsYZGTfuFsFUIs9huc/UUlRZUInbo8/S4pr6tDv2Z/O/NU1KpU6B/u+cHv1f/LTKp08Sxn/5378q5i2PANoE2wNl/9rBUilWWKWmK7hrvidiBxWHSSycKj2wpCO/L/9HRuqSdzoJ24ssL2PrtLBaSbbu5CUVE9nUWm7SPLZXFivDubHU1ILYXSrXs5Hj9dJlvqMmyYu7iWKPuFAXWs8ljvGZP3+bkaLVx1x/Wv52eecGoFxif0Gww6rUo8QAywZ0reUIhikpgq4RENphL8u3sm46eDs3CvY363i+0ZPze3c2ag0ElJflqjdKol0JRKZHIkJlpuHwgvcdA94693BFrsC7bvTsVP3+a1TbuSanhUo/fljjQ4+aw+3tZHw/zbubQqpPj1UO30BNA9vUCjUrDtmbINjO34sf6NgmQXNrP7vQKu3P7Us792yWvLA1G7GO7WclHdxb8c7SwTR9+lpYZ53NL71VM/SwE2QSbrgH475q76Rcr5X4O/uEsdr3bnmuHb5ECYvJHrM9q1WPrFTc5mRXbVmTBd3oGcH4xgEajST+VXVWigk6soVNsOqnQPuvb9qy/8++5KhiQE0pJua+jd5AbM2mAExTnlhdmlUKDWqOk5F7CkTObisVsdRmbwp6rSc8cvH/hSFFFiW5bLALBGJl2LaDBgHDtpaRIv+KQ1q7s1CVct/aTqOFZfafOndAFYJaHPvSuXpFYfa/eXR95Dd/q1aUUxMKklERiKekbJHl2oi1WaRjlcdkF6NqZkuI8hbIS0QZ7mxmu93woVK3luwa66d21YtNUnU4Wvf9DfbQ3k1QNXZl1poR+OXB1YEJMujgTvsEyb38Zsjc83LzpSYDPJ0rxGCwbJ8GycRIsGyfBsnESLBsn+T8AAAD//5SDbMAAAAAGSURBVAMAS6gJ+Ti/SXsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# 聊天机器人系统指令\n",
    "# 定义机器人的角色和行为规范，并说明如何利用用户记忆来个性化回复。\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"你是一个有帮助的助手，具有记忆功能，可以提供关于用户的信息。\n",
    "如果你有这个用户的记忆，请使用它来个性化你的回复。\n",
    "以下是记忆（可能为空）：{memory}\"\"\"\n",
    "\n",
    "# 从聊天历史和任何现有记忆中创建/更新新记忆的指令\n",
    "# 这个指令用于指导模型如何根据当前对话和现有用户档案来生成或更新用户记忆。\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"根据用户的聊天历史创建或更新用户档案记忆。\n",
    "这将保存为长期记忆。如果存在现有记忆，只需更新它。\n",
    "以下是现有记忆（可能为空）：{memory}\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"\n",
    "    从存储中加载用户档案记忆并使用它来个性化聊天机器人的回复。\n",
    "\n",
    "    参数:\n",
    "        state: 消息状态，一个字典，包含当前对话的完整历史消息列表。\n",
    "        config: 运行配置，一个字典，通常包含用于标识用户或线程的信息，例如用户ID。\n",
    "        store: 存储接口，一个 BaseStore 的实例，用于访问和管理长期记忆数据。\n",
    "\n",
    "    返回:\n",
    "        包含AI回复消息的字典。返回的消息列表将追加到当前状态中。\n",
    "    \"\"\"\n",
    "\n",
    "    # 从配置字典中安全地获取用户ID。\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 定义用于存储用户记忆的命名空间。命名空间有助于组织和隔离不同用户的数据。\n",
    "    namespace = (\"memory\", user_id)\n",
    "    # 从存储中检索特定用户的记忆。使用 \"user_memory\" 作为键来获取用户档案。\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # 格式化现有记忆以便在系统提示中使用。如果记忆存在且有值，则提取并格式化姓名和兴趣。\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"姓名: {memory_dict.get('user_name', '未知')}\\n\" # 如果 user_name 不存在，则显示“未知”\n",
    "            f\"兴趣: {', '.join(memory_dict.get('interests', []))}\" # 将兴趣列表转换为逗号分隔的字符串\n",
    "        )\n",
    "    else:\n",
    "        # 如果没有找到现有记忆，则格式化记忆为 None。\n",
    "        formatted_memory = None\n",
    "\n",
    "    # 使用格式化后的记忆来构建完整的系统提示消息。\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # 调用语言模型生成回复。输入包括系统消息和当前的聊天历史。\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    # 返回包含AI回复消息的字典，以便更新状态。\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"\n",
    "    反思聊天历史，利用模型生成或更新用户档案记忆，并将其保存到存储中。\n",
    "\n",
    "    参数:\n",
    "        state: 消息状态，包含当前对话的完整历史消息列表。\n",
    "        config: 运行配置，通常包含用户ID等信息。\n",
    "        store: 存储接口，用于访问和管理长期记忆数据。\n",
    "    \"\"\"\n",
    "\n",
    "    # 从配置字典中获取用户ID。\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 定义用于存储用户记忆的命名空间。\n",
    "    namespace = (\"memory\", user_id)\n",
    "    # 从存储中检索现有用户的记忆。\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # 格式化现有记忆以便在指令中使用。如果记忆存在且有值，则提取并格式化姓名和兴趣。\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"姓名: {memory_dict.get('user_name', '未知')}\\n\" # 如果 user_name 不存在，则显示“未知”\n",
    "            f\"兴趣: {', '.join(memory_dict.get('interests', []))}\" # 将兴趣列表转换为逗号分隔的字符串\n",
    "        )\n",
    "    else:\n",
    "        # 如果没有找到现有记忆，则格式化记忆为 None。\n",
    "        formatted_memory = None\n",
    "\n",
    "    # 使用格式化后的现有记忆来构建指令消息，指导模型如何创建或更新记忆。\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=formatted_memory)\n",
    "\n",
    "    # 调用带有结构化输出的模型来生成符合 UserProfile 模式(Schema)的新记忆。\n",
    "    # 输入包括指令消息和当前的聊天历史。\n",
    "    new_memory = model_with_structure.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "\n",
    "    # 将新生成的或更新后的用户档案记忆保存到存储中。使用 \"user_memory\" 作为键。\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, new_memory)\n",
    "\n",
    "# 定义 LangGraph 的状态图。\n",
    "# StateGraph 定义了节点（图中的步骤）和边（节点之间的转换）。\n",
    "builder = StateGraph(MessagesState)\n",
    "# 添加 \"call_model\" 节点，对应上面定义的 call_model 函数。\n",
    "builder.add_node(\"call_model\", call_model)  # 调用语言模型生成回复\n",
    "# 添加 \"write_memory\" 节点，对应上面定义的 write_memory 函数。\n",
    "builder.add_node(\"write_memory\", write_memory)  # 写入/更新用户档案记忆\n",
    "# 定义从图的开始到 \"call_model\" 节点的边。\n",
    "builder.add_edge(START, \"call_model\")  # 对话开始时，首先调用模型生成回复\n",
    "# 定义从 \"call_model\" 节点到 \"write_memory\" 节点的边。\n",
    "builder.add_edge(\"call_model\", \"write_memory\")  # 模型生成回复后，根据对话更新记忆\n",
    "# 定义从 \"write_memory\" 节点到图的结束的边。\n",
    "builder.add_edge(\"write_memory\", END)  # 记忆更新完成后，流程结束\n",
    "\n",
    "# 初始化长期记忆存储（跨线程）。\n",
    "# InMemoryStore 是一个简单的内存存储实现，数据存储在内存中，适合开发和演示。\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# 初始化短期记忆检查点（线程内）。\n",
    "# MemorySaver 检查点将线程的状态保存在内存中，用于在同一线程内恢复对话。\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# 编译图。\n",
    "# 将构建器、短期记忆检查点和长期记忆存储结合起来，形成一个可执行的图。\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# 显示图结构。\n",
    "# 使用 mermaid 格式绘制图的可视化表示，帮助理解图的流程。\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3smT2MOxhVV",
    "outputId": "1304f97e-7c44-43ca-b024-17f2301fcfdc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "嗨，我是FLY，我喜欢在湖边骑自行车，并在咖啡店喝东西。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "嗨，FLY！很高兴认识你！湖边骑自行车听起来非常惬意，尤其是能感受到自然的气息。之后再去咖啡店喝点东西放松一下，真是完美的搭配！你有特别喜欢的咖啡或者饮品吗？\n"
     ]
    }
   ],
   "source": [
    "# 我们提供线程ID用于短期记忆（线程内）\n",
    "# 我们提供用户ID用于长期记忆（跨线程）\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# 用户输入\n",
    "# 用户介绍自己的姓名和兴趣\n",
    "input_messages = [HumanMessage(content=\"嗨，我是FLY，我喜欢在湖边骑自行车，并在咖啡店喝东西。\")]\n",
    "\n",
    "# 运行图\n",
    "# 流式处理用户输入并显示AI回复\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCw-7f4PxhVW"
   },
   "source": [
    "让我们检查存储中的记忆。\n",
    "\n",
    "我们可以看到记忆是一个符合我们模式(Schema)的字典。\n",
    "\n",
    "### 记忆存储验证\n",
    "- **数据结构**: 记忆以字典形式存储，包含用户档案信息\n",
    "- **模式(Schema)一致性**: 确保存储的数据符合预定义的 UserProfile 模式(Schema)\n",
    "- **持久性**: 记忆在会话间保持，支持个性化体验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ryC5Cf-xhVW",
    "outputId": "4b5912d8-8381-4ed4-bd27-39b6867993fc"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'interests': ['湖边骑自行车', '在咖啡店喝东西']}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 用于保存记忆的命名空间\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "# 从跨线程记忆中获取现有记忆\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.value  # 显示记忆的实际内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d9SHFSxxhVX"
   },
   "source": [
    "## 什么时候会失败？\n",
    "\n",
    "[`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) 非常有用，但如果我们在处理更复杂的模式(Schema)时会发生什么？\n",
    "\n",
    "[这里](https://github.com/hinthornw/trustcall?tab=readme-ov-file#complex-schema) 是一个更复杂模式(Schema)的示例，我们将在下面测试。\n",
    "\n",
    "这是一个 [Pydantic](https://docs.pydantic.dev/latest/) 模型，描述了用户在通信和信任跌落方面的偏好。\n",
    "\n",
    "### 复杂模式(Schema)的挑战\n",
    "- **嵌套结构**: 多层嵌套的对象和数组\n",
    "- **可选字段**: 许多字段可能为空或未定义\n",
    "- **类型验证**: 严格的类型检查可能导致解析失败\n",
    "- **模型容量**: 即使是 GPT-4o 这样的高级模型也可能遇到困难"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b93kzUM-xhVX"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class OutputFormat(BaseModel):\n",
    "    \"\"\"输出格式模式(Schema)，包含偏好和句子偏好显示\"\"\"\n",
    "    preference: str  # 用户偏好\n",
    "    sentence_preference_revealed: str  # 句子偏好显示\n",
    "\n",
    "class TelegramPreferences(BaseModel):\n",
    "    \"\"\"电报偏好模式(Schema)\"\"\"\n",
    "    preferred_encoding: Optional[List[OutputFormat]] = None  # 首选编码方式\n",
    "    favorite_telegram_operators: Optional[List[OutputFormat]] = None  # 喜欢的电报操作员\n",
    "    preferred_telegram_paper: Optional[List[OutputFormat]] = None  # 首选电报纸张\n",
    "\n",
    "class MorseCode(BaseModel):\n",
    "    \"\"\"摩尔斯电码偏好模式(Schema)\"\"\"\n",
    "    preferred_key_type: Optional[List[OutputFormat]] = None  # 首选键类型\n",
    "    favorite_morse_abbreviations: Optional[List[OutputFormat]] = None  # 喜欢的摩尔斯缩写\n",
    "\n",
    "class Semaphore(BaseModel):\n",
    "    \"\"\"旗语偏好模式(Schema)\"\"\"\n",
    "    preferred_flag_color: Optional[List[OutputFormat]] = None  # 首选旗帜颜色\n",
    "    semaphore_skill_level: Optional[List[OutputFormat]] = None  # 旗语技能水平\n",
    "\n",
    "class TrustFallPreferences(BaseModel):\n",
    "    \"\"\"信任跌落偏好模式(Schema)\"\"\"\n",
    "    preferred_fall_height: Optional[List[OutputFormat]] = None  # 首选跌落高度\n",
    "    trust_level: Optional[List[OutputFormat]] = None  # 信任水平\n",
    "    preferred_catching_technique: Optional[List[OutputFormat]] = None  # 首选接住技巧\n",
    "\n",
    "class CommunicationPreferences(BaseModel):\n",
    "    \"\"\"通信偏好模式(Schema)\"\"\"\n",
    "    telegram: TelegramPreferences  # 电报偏好\n",
    "    morse_code: MorseCode  # 摩尔斯电码偏好\n",
    "    semaphore: Semaphore  # 旗语偏好\n",
    "\n",
    "class UserPreferences(BaseModel):\n",
    "    \"\"\"用户偏好模式(Schema)\"\"\"\n",
    "    communication_preferences: CommunicationPreferences  # 通信偏好\n",
    "    trust_fall_preferences: TrustFallPreferences  # 信任跌落偏好\n",
    "\n",
    "class TelegramAndTrustFallPreferences(BaseModel):\n",
    "    \"\"\"电报和信任跌落偏好模式(Schema)\"\"\"\n",
    "    pertinent_user_preferences: UserPreferences  # 相关用户偏好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqidVSB-xhVa"
   },
   "source": [
    "## 使用 TrustCall 创建和更新档案模式(Schema)\n",
    "\n",
    "正如我们所看到的，处理模式(Schema)可能很棘手。\n",
    "\n",
    "复杂的模式(Schema)可能难以提取。\n",
    "\n",
    "此外，更新即使是简单的模式(Schema)也可能带来挑战。\n",
    "\n",
    "考虑我们上面的聊天机器人。\n",
    "\n",
    "我们每次选择保存新记忆时都*从头开始*重新生成档案模式(Schema)。\n",
    "\n",
    "这是低效的，如果模式(Schema)包含大量信息需要每次都重新生成，可能会浪费模型令牌。\n",
    "\n",
    "更糟糕的是，从头重新生成档案时我们可能会丢失信息。\n",
    "\n",
    "解决这些问题是 [TrustCall](https://github.com/hinthornw/trustcall) 的动机！\n",
    "\n",
    "这是一个开源库，用于更新 JSON 模式(Schema)，由 LangChain 团队的 [Will Fu-Hinthorn](https://github.com/hinthornw) 开发。\n",
    "\n",
    "它正是为了解决在处理记忆时遇到的这些挑战而创建的。\n",
    "\n",
    "让我们首先展示 TrustCall 在这个 [消息](https://python.langchain.com/docs/concepts/messages/) 列表上的简单提取用法。\n",
    "\n",
    "### TrustCall 的优势\n",
    "- **增量更新**: 只更新模式(Schema)中发生变化的部分\n",
    "- **效率提升**: 减少模型令牌使用和计算成本\n",
    "- **信息保持**: 避免在更新过程中丢失现有信息\n",
    "- **复杂模式(Schema)支持**: 更好地处理嵌套和复杂的模式(Schema)结构\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mZhemTWxhVa"
   },
   "outputs": [],
   "source": [
    "# 对话示例\n",
    "# 创建一个简单的对话历史用于演示 TrustCall 的提取功能\n",
    "conversation = [HumanMessage(content=\"嗨，我是FLY。\"),\n",
    "                AIMessage(content=\"很高兴认识你，FLY。\"),\n",
    "                HumanMessage(content=\"我真的很喜欢在湖边骑自行车。\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hGXSQOXxhVa"
   },
   "source": [
    "我们使用 `create_extractor`，传入模型以及我们的模式(Schema)作为 [工具](https://python.langchain.com/docs/concepts/tools/)。\n",
    "\n",
    "使用 TrustCall，可以以各种方式提供模式(Schema)。\n",
    "\n",
    "例如，我们可以传递 JSON 对象 / Python 字典或 Pydantic 模型。\n",
    "\n",
    "在底层，TrustCall 使用 [工具调用](https://python.langchain.com/docs/concepts/tool_calling/) 从 [消息](https://python.langchain.com/docs/concepts/messages/) 输入列表生成 [结构化输出](https://python.langchain.com/docs/concepts/structured_outputs/)。\n",
    "\n",
    "要强制 TrustCall 生成 [结构化输出](https://python.langchain.com/docs/concepts/structured_outputs/)，我们可以在 `tool_choice` 参数中包含模式(Schema)名称。\n",
    "\n",
    "我们可以使用上面的对话调用提取器。\n",
    "\n",
    "### TrustCall 工作原理\n",
    "- **工具调用机制**: 利用 LangChain 的工具调用功能\n",
    "- **模式(Schema)灵活性**: 支持多种模式(Schema)定义方式\n",
    "- **强制输出**: 通过 tool_choice 确保特定工具被使用\n",
    "- **消息处理**: 直接处理消息列表，无需额外格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uhF-YX32xhVb"
   },
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "\n",
    "# 用户档案模式(Schema)定义\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"用户档案模式(Schema)，包含类型化字段\"\"\"\n",
    "    user_name: str = Field(description=\"用户的首选名称\")\n",
    "    interests: List[str] = Field(description=\"用户兴趣列表\")\n",
    "\n",
    "# 初始化模型\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 创建提取器\n",
    "# 使用 TrustCall 创建结构化数据提取器\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],  # 将模式(Schema)作为工具传递\n",
    "    tool_choice=\"UserProfile\"  # 强制使用 UserProfile 工具\n",
    ")\n",
    "\n",
    "# 系统指令\n",
    "system_msg = \"从以下对话中提取用户档案\"\n",
    "\n",
    "# 调用提取器\n",
    "# 传入系统消息和对话历史\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+conversation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E19Uv-zVxhVc"
   },
   "source": [
    "当我们调用提取器时，我们会得到几个东西：\n",
    "\n",
    "* `messages`: 包含工具调用的 `AIMessages` 列表。\n",
    "* `responses`: 符合我们模式(Schema)的解析后工具调用结果。\n",
    "* `response_metadata`: 如果更新现有工具调用则适用。它说明哪些响应对应于哪些现有对象。\n",
    "\n",
    "### 提取结果解析\n",
    "- **messages**: 包含 AI 模型生成的原始工具调用消息\n",
    "- **responses**: 解析后的结构化数据，可直接使用\n",
    "- **response_metadata**: 用于跟踪更新操作的元数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NhUKGmzgxhVe",
    "outputId": "85baf14c-3f04-4057-d560-70c081171b11"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_1MPXHKhnINY1LapCneFetCIy)\n",
      " Call ID: call_1MPXHKhnINY1LapCneFetCIy\n",
      "  Args:\n",
      "    user_name: FLY\n",
      "    interests: ['骑自行车', '湖边活动']\n"
     ]
    }
   ],
   "source": [
    "# 显示提取器生成的原始消息\n",
    "# 这些消息包含工具调用的详细信息\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8i8uMix_xhVe",
    "outputId": "7dbea47e-ce58-479c-a6c5-aa949e0824c3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[UserProfile(user_name='FLY', interests=['骑自行车', '湖边活动'])]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# 获取解析后的结构化数据\n",
    "# 这是可以直接使用的用户档案对象\n",
    "schema = result[\"responses\"]\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yb62DH-xhVf",
    "outputId": "e1f1a3e4-bf3a-4bc2-f8bf-2a5e8e11e446"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'interests': ['骑自行车', '湖边活动']}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# 将 Pydantic 模型转换为字典格式\n",
    "# 便于查看和进一步处理\n",
    "schema[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCNi1yKxxhVf",
    "outputId": "bb2536ed-5f50-4ce6-ccb9-c6fb5c17e777"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'id': 'call_1MPXHKhnINY1LapCneFetCIy'}]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# 查看响应元数据\n",
    "# 包含工具调用的 ID 和相关信息\n",
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8vvadI4xhVg"
   },
   "source": [
    "让我们看看如何使用它来*更新*档案。\n",
    "\n",
    "对于更新，TrustCall 接受一组消息以及现有模式(Schema)。\n",
    "\n",
    "核心思想是它提示模型生成 [JSON Patch](https://jsonpatch.com/) 来仅更新模式(Schema)的相关部分。\n",
    "\n",
    "这比天真地覆盖整个模式(Schema)更不容易出错。\n",
    "\n",
    "它也更高效，因为模型只需要生成已更改的模式(Schema)部分。\n",
    "\n",
    "我们可以将现有模式(Schema)保存为字典。\n",
    "\n",
    "我们可以使用 `model_dump()` 将 Pydantic 模型实例序列化为字典。\n",
    "\n",
    "我们将其与模式(Schema)名称 `UserProfile` 一起传递给 `\"existing\"` 参数。\n",
    "\n",
    "### 增量更新机制\n",
    "- **JSON Patch**: 只修改发生变化的数据部分\n",
    "- **错误减少**: 避免完全重写可能导致的错误\n",
    "- **效率提升**: 减少模型需要处理的数据量\n",
    "- **信息保持**: 保留未更改的现有信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lOFzk9PxhVg"
   },
   "outputs": [],
   "source": [
    "# 更新对话\n",
    "# 添加更多关于用户兴趣的信息\n",
    "updated_conversation = [HumanMessage(content=\"嗨，我是FLY。\"),\n",
    "                AIMessage(content=\"很高兴认识你，FLY。\"),\n",
    "                HumanMessage(content=\"我真的很喜欢在湖边骑自行车。\"),\n",
    "                AIMessage(content=\"湖边骑自行车很棒！骑完自行车后你会去哪里\"),\n",
    "                HumanMessage(content=\"骑完自行车后我喜欢去一家咖啡店。\"),]\n",
    "\n",
    "# 更新指令\n",
    "# 指示模型更新现有记忆而不是创建新记忆\n",
    "system_msg = f\"\"\"更新记忆（JSON 文档）以整合以下对话中的新信息\"\"\"\n",
    "\n",
    "# 使用更新后的指令和现有档案调用提取器\n",
    "# 传入现有档案数据以进行增量更新\n",
    "result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=system_msg)]+updated_conversation},\n",
    "                                    {\"existing\": {\"UserProfile\": schema[0].model_dump()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcf5fjTUxhVh",
    "outputId": "16f4abb0-709f-4061-f63c-a9a6a03eb754"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UserProfile (call_NLG5rSbZoopxtJX36LsfQklG)\n",
      " Call ID: call_NLG5rSbZoopxtJX36LsfQklG\n",
      "  Args:\n",
      "    user_name: FLY\n",
      "    interests: ['湖边骑自行车', '去咖啡店']\n"
     ]
    }
   ],
   "source": [
    "# 显示更新后的工具调用消息\n",
    "# 查看模型如何更新现有档案\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-ebp9dxxhVw",
    "outputId": "035b57d8-2b8e-4336-a880-0684048446e3"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'id': 'call_NLG5rSbZoopxtJX36LsfQklG'}]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# 查看更新操作的元数据\n",
    "# 显示哪些响应对应哪些现有对象\n",
    "result[\"response_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVdn7DZcxhVw",
    "outputId": "e09f6116-9669-4b93-da56-a7d4f38452c4"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'interests': ['湖边骑自行车', '去咖啡店']}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# 获取更新后的档案\n",
    "# 这是包含新信息的完整用户档案\n",
    "updated_schema = result[\"responses\"][0]\n",
    "updated_schema.model_dump()  # 转换为字典格式查看"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQFUP0JSxhVz"
   },
   "source": [
    "## 基于档案模式(Schema)更新的聊天机器人\n",
    "\n",
    "现在，让我们将 TrustCall 集成到我们的聊天机器人中，以创建*和更新*记忆档案。\n",
    "\n",
    "### 集成 TrustCall 的优势\n",
    "- **智能更新**: 只更新发生变化的部分，保持现有信息\n",
    "- **效率提升**: 减少模型令牌使用和计算成本\n",
    "- **错误减少**: 避免完全重写可能导致的错误\n",
    "- **复杂模式(Schema)支持**: 更好地处理嵌套和复杂的用户档案结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "-hkA2XWYxhV0",
    "outputId": "c7ed6bb9-452a-413a-dd45-fb02b625752c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {}
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "# 初始化模型\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 用户档案模式(Schema)定义\n",
    "class UserProfile(BaseModel):\n",
    "    \"\"\"用户档案模式(Schema)\"\"\"\n",
    "    user_name: str = Field(description=\"用户的首选名称\")\n",
    "    user_location: str = Field(description=\"用户的位置\")\n",
    "    interests: list = Field(description=\"用户兴趣列表\")\n",
    "\n",
    "# 创建 TrustCall 提取器\n",
    "# 用于智能更新用户档案\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[UserProfile],\n",
    "    tool_choice=\"UserProfile\",  # 强制使用 UserProfile 工具\n",
    ")\n",
    "\n",
    "# 聊天机器人系统指令\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"你是一个有帮助的助手，具有记忆功能，可以提供关于用户的信息。\n",
    "如果你有这个用户的记忆，请使用它来个性化你的回复。\n",
    "以下是记忆（可能为空）：{memory}\"\"\"\n",
    "\n",
    "# TrustCall 提取指令\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"创建或更新记忆（JSON 文档）以整合以下对话中的信息：\"\"\"\n",
    "\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"\n",
    "    从存储中加载记忆并使用它来个性化聊天机器人的回复\n",
    "\n",
    "    参数:\n",
    "        state: 消息状态，包含当前对话历史\n",
    "        config: 运行配置，包含用户ID等信息\n",
    "        store: 存储接口，用于访问长期记忆\n",
    "\n",
    "    返回:\n",
    "        包含AI回复消息的字典\n",
    "    \"\"\"\n",
    "\n",
    "    # 从配置中获取用户ID\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 从存储中检索记忆\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # 为系统提示格式化记忆\n",
    "    if existing_memory and existing_memory.value:\n",
    "        memory_dict = existing_memory.value\n",
    "        formatted_memory = (\n",
    "            f\"姓名: {memory_dict.get('user_name', '未知')}\\n\"\n",
    "            f\"位置: {memory_dict.get('user_location', '未知')}\\n\"\n",
    "            f\"兴趣: {', '.join(memory_dict.get('interests', []))}\"\n",
    "        )\n",
    "    else:\n",
    "        formatted_memory = None\n",
    "\n",
    "    # 在系统提示中格式化记忆\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    "\n",
    "    # 使用记忆和聊天历史进行回复\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"\n",
    "    使用 TrustCall 智能更新记忆档案\n",
    "\n",
    "    参数:\n",
    "        state: 消息状态，包含当前对话历史\n",
    "        config: 运行配置，包含用户ID等信息\n",
    "        store: 存储接口，用于保存长期记忆\n",
    "    \"\"\"\n",
    "\n",
    "    # 从配置中获取用户ID\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # 从存储中检索现有记忆\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "\n",
    "    # 获取现有档案并转换为 JSON 文档格式\n",
    "    # 如果存在记忆，则用于增量更新；否则创建新档案\n",
    "    existing_profile = {\"UserProfile\": existing_memory.value} if existing_memory else None\n",
    "\n",
    "    # 调用 TrustCall 提取器进行智能更新\n",
    "    result = trustcall_extractor.invoke({\"messages\": [SystemMessage(content=TRUSTCALL_INSTRUCTION)]+state[\"messages\"], \"existing\": existing_profile})\n",
    "\n",
    "    # 获取更新后的档案作为 JSON 对象\n",
    "    updated_profile = result[\"responses\"][0].model_dump()\n",
    "\n",
    "    # 保存更新后的档案\n",
    "    key = \"user_memory\"\n",
    "    store.put(namespace, key, updated_profile)\n",
    "\n",
    "# 定义状态图\n",
    "# 创建集成 TrustCall 的聊天机器人工作流程\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)  # 调用模型节点\n",
    "builder.add_node(\"write_memory\", write_memory)  # 智能写入记忆节点\n",
    "builder.add_edge(START, \"call_model\")  # 从开始到调用模型\n",
    "builder.add_edge(\"call_model\", \"write_memory\")  # 从调用模型到写入记忆\n",
    "builder.add_edge(\"write_memory\", END)  # 从写入记忆到结束\n",
    "\n",
    "# 长期记忆存储（跨线程）\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "# 短期记忆检查点（线程内）\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "# 使用检查点和存储编译图\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "# 显示图结构\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tChc-AJxhV1",
    "outputId": "ba66abd5-4981-4a08-be2e-933ea2675873"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "嗨，我是FLY。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "嗨，FLY！很高兴认识你！有什么我可以帮忙的吗？\n"
     ]
    }
   ],
   "source": [
    "# 我们提供线程ID用于短期记忆（线程内）\n",
    "# 我们提供用户ID用于长期记忆（跨线程）\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# 用户输入\n",
    "# 用户介绍自己的姓名\n",
    "input_messages = [HumanMessage(content=\"嗨，我是FLY。\")]\n",
    "\n",
    "# 运行图\n",
    "# 流式处理用户输入并显示AI回复\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtz7cET_xhV2",
    "outputId": "07e5c641-210a-46d0-e4a3-e7e146715e6c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "我喜欢在湖边骑自行车。\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "听起来很棒，FLY！湖边骑自行车一定很惬意，既能享受自然风光，又能锻炼身体。如果你有特别喜欢的湖或者骑行路线，随时可以和我分享哦！我可以帮你规划路线或者提供一些骑行的小贴士。\n"
     ]
    }
   ],
   "source": [
    "# 用户输入\n",
    "# 用户分享自己的兴趣和位置信息\n",
    "input_messages = [HumanMessage(content=\"我喜欢在湖边骑自行车。\")]\n",
    "\n",
    "# 运行图\n",
    "# 流式处理用户输入并显示AI回复\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67X05D5IxhV3",
    "outputId": "5d9be8c4-94bf-4696-b6ba-49b6b149fcfa"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'namespace': ['memory', '1'],\n",
       " 'key': 'user_memory',\n",
       " 'value': {'user_name': 'FLY', 'user_location': '', 'interests': ['骑自行车']},\n",
       " 'created_at': '2025-09-17T08:59:34.648703+00:00',\n",
       " 'updated_at': '2025-09-17T08:59:34.648705+00:00'}"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# 用于保存记忆的命名空间\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "# 从跨线程记忆中获取现有记忆\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()  # 显示完整的记忆对象信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mc_8nA-JxhV3",
    "outputId": "00399961-9172-4fa2-bb28-a57646009838"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'user_name': 'FLY', 'user_location': '', 'interests': ['骑自行车']}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# 以 JSON 对象形式保存的用户档案\n",
    "# 显示实际存储的用户档案数据\n",
    "existing_memory.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMsYegzvxhV4",
    "outputId": "35b2b4de-b2e4-49eb-cbcf-2048667dcad6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "我也喜欢去咖啡店喝东西\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "那真是一个很棒的组合！骑完自行车后去咖啡店坐下来喝杯咖啡或者其他饮品，既能放松身心，又能享受片刻的宁静。你更喜欢哪种饮品呢？比如咖啡、茶还是其他特别的饮品？或者有没有特别喜欢的咖啡店风格？我可以帮你推荐一些适合骑行后放松的饮品或者咖啡店哦！\n"
     ]
    }
   ],
   "source": [
    "# 用户输入\n",
    "# 用户添加新的兴趣信息\n",
    "input_messages = [HumanMessage(content=\"我也喜欢去咖啡店喝东西\")]\n",
    "\n",
    "# 运行图\n",
    "# 流式处理用户输入并显示AI回复\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urdUBoxhxhV5"
   },
   "source": [
    "在新线程中继续对话。\n",
    "\n",
    "### 跨线程记忆测试\n",
    "- **目的**: 验证长期记忆在不同会话间的持久性\n",
    "- **测试场景**: 使用新的线程ID但相同的用户ID\n",
    "- **预期结果**: 机器人应该记住之前会话中的用户信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDs8ngMZxhV5",
    "outputId": "50df1ea5-01e7-4d07-94fb-c299de75d096"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "您推荐哪些湖边咖啡店给我？\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "FLY，你喜欢在湖边骑自行车和去咖啡店喝东西，这听起来很棒！虽然我不知道你具体的位置，但我可以推荐一些湖边咖啡店的类型，或者如果你告诉我你所在的城市，我可以提供更具体的建议。\n",
      "\n",
      "### 一些湖边咖啡店的推荐类型：\n",
      "1. **风景优美的湖边咖啡店**：选择那些有露台或大窗户的咖啡店，可以一边享受咖啡，一边欣赏湖景。\n",
      "2. **骑行友好的咖啡店**：寻找那些提供自行车停车设施的咖啡店，方便你骑行后休息。\n",
      "3. **特色饮品的咖啡店**：尝试一些提供独特饮品或手工咖啡的地方，让你的湖边体验更特别。\n",
      "4. **湖边小众咖啡店**：如果你喜欢安静，可以选择一些人流较少的小众咖啡店，享受宁静的湖边时光。\n",
      "\n",
      "如果你告诉我你所在的城市或湖泊的名字，我可以帮你查找具体的推荐哦！\n"
     ]
    }
   ],
   "source": [
    "# 我们提供线程ID用于短期记忆（线程内）\n",
    "# 我们提供用户ID用于长期记忆（跨线程）\n",
    "# 注意：使用新的线程ID \"2\"，但相同的用户ID \"1\"\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# 用户输入\n",
    "# 用户询问面包店推荐，测试长期记忆是否有效\n",
    "input_messages = [HumanMessage(content=\"您推荐哪些湖边咖啡店给我？\")]\n",
    "\n",
    "# 运行图\n",
    "# 流式处理用户输入并显示AI回复\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deSNlTf7xhV7"
   },
   "source": [
    "追踪：\n",
    "\n",
    "https://smith.langchain.com/o/7bfa9385-4ac5-468a-a06c-ffd7dbac42ec/projects/p/27f0e396-e7ab-4eac-9501-8df28b729149?timeModel=%7B%22duration%22%3A%227d%22%7D\n",
    "\n",
    "![TrustCall 集成](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509171738029.png)\n",
    "\n",
    "### 项目总结\n",
    "\n",
    "通过这个教程，我们学习了：\n",
    "\n",
    "1. **结构化记忆**: 使用 TypedDict 和 Pydantic 定义用户档案模式(Schema)\n",
    "2. **LangGraph 存储**: 使用 InMemoryStore 保存和检索长期记忆\n",
    "3. **结构化输出**: 使用 `with_structured_output` 确保模型输出符合模式(Schema)\n",
    "4. **TrustCall 集成**: 使用 TrustCall 进行智能的增量更新\n",
    "5. **跨线程记忆**: 实现不同会话间的记忆持久化\n",
    "\n",
    "### 技术要点\n",
    "- **模式(Schema)设计**: 合理设计数据结构以支持用户档案管理\n",
    "- **记忆管理**: 区分短期和长期记忆的不同用途\n",
    "- **智能更新**: 使用 TrustCall 避免完全重写，提高效率\n",
    "- **个性化体验**: 基于用户档案提供个性化的聊天体验"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}