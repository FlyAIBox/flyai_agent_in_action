{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1869.00 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 788.88 GB, Free: 1123.48 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b",
   "metadata": {
    "id": "e0a5763f-5f45-4b8f-b3e2-480f46c5721b"
   },
   "source": [
    "# 🔬 深度研究助手 - 智能体评估版\n",
    "\n",
    "## 📖 教程概述\n",
    "\n",
    "本教程将在深度研究助手的基础上，集成 **Langfuse** 可观测性平台，实现智能体的全面评估和监控。我们将学习如何：\n",
    "\n",
    "- **🔍 实时追踪**：监控研究助手的每个执行步骤\n",
    "- **📊 性能评估**：分析成本、延迟、准确性等关键指标  \n",
    "- **👥 用户反馈**：收集和分析用户对研究结果的反馈\n",
    "- **🎯 离线评估**：使用基准数据集进行系统性测试\n",
    "\n",
    "## 🎯 研究助手回顾\n",
    "\n",
    "我们的深度研究助手是一个基于 LangGraph 的多智能体系统：\n",
    "\n",
    "### 核心功能模块\n",
    "- **📝 分析师生成**：根据研究主题自动创建专业分析师团队\n",
    "- **🤝 人机协同**：通过人类反馈优化分析师配置\n",
    "- **💬 专家访谈**：每位分析师与 AI 专家进行深度对话\n",
    "- **🔍 并行搜索**：从多个源（网络、百科）检索相关信息\n",
    "- **📄 报告生成**：将访谈结果整合为结构化报告\n",
    "\n",
    "### 🔬 智能体评估的重要性\n",
    "\n",
    "在部署研究助手到生产环境前，评估是确保系统质量的关键环节：\n",
    "\n",
    "- **🐛 问题诊断**：快速定位研究流程中的问题节点\n",
    "- **📈 性能优化**：识别高成本操作并进行优化\n",
    "- **🎯 质量保证**：确保生成的研究报告达到预期标准\n",
    "- **📊 持续改进**：通过数据驱动的方式提升系统表现\n",
    "\n",
    "![深度研究助手架构图](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb164d61c93d48e604091_research-assistant1.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 本教程特色\n",
    "\n",
    "- **📚 初学者友好**：详细的中文注释和概念解释\n",
    "- **🔧 实战导向**：基于真实业务场景的评估方案\n",
    "- **📊 全面监控**：覆盖在线和离线两种评估模式\n",
    "- **🎯 可操作**：提供具体的优化建议和最佳实践"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
   "metadata": {
    "id": "f23991e9-51b3-4e9f-86a0-dec16aa7d1e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# 📦 安装项目所需的Python包（带评估功能）\n",
    "# 使用 %%capture --no-stderr 来隐藏安装过程中的输出信息，保持notebook整洁\n",
    "\n",
    "# 🔬 智能体评估版本包含额外的 Langfuse 支持\n",
    "%pip install langgraph==0.6.7 langchain_openai==0.3.31 langchain_community==0.3.27 langchain_core==0.3.75 tavily-python==0.7.12 wikipedia==1.4.0 langfuse==3.3.0\n",
    "\n",
    "# 📚 各包功能说明：\n",
    "# 核心框架：\n",
    "# - langgraph==0.6.7: LangGraph框架，用于构建多智能体工作流\n",
    "# - langchain_openai==0.3.31: LangChain的OpenAI集成，用于调用GPT模型\n",
    "# - langchain_community==0.3.27: LangChain社区工具集，包含各种第三方集成\n",
    "# - langchain_core==0.3.75: LangChain核心组件，提供基础功能\n",
    "\n",
    "# 数据检索：\n",
    "# - tavily-python==0.7.12: Tavily搜索API客户端，用于网络搜索\n",
    "# - wikipedia==1.4.0: 维基百科API客户端，用于百科搜索\n",
    "# - baike_loader.py: 百度百科加载器（本仓库内置）\n",
    "\n",
    "# 🔬 评估与监控（新增）：\n",
    "# - langfuse==3.3.0: LLM应用的可观测性和评估平台，支持追踪、监控和评估"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914",
   "metadata": {
    "id": "99a1c01d-87e1-4723-b83e-ebcf937fe914"
   },
   "source": [
    "## 环境准备（Setup）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba917800-10e4-4e2a-8e9e-30893b731e97",
    "outputId": "d651b4af-0c90-457a-f4c7-33c8d2b91688"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 配置 Langfuse 评估平台...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGFUSE_PUBLIC_KEY:  ········\n",
      "LANGFUSE_SECRET_KEY:  ········\n",
      "LANGFUSE_HOST:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API 密钥配置完成！\n",
      "📊 智能体评估功能已启用\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 环境变量名称\n",
    "\n",
    "    功能:\n",
    "        - 检查环境变量是否已存在\n",
    "        - 如果不存在，则通过getpass安全地获取用户输入\n",
    "        - 将用户输入设置为环境变量值\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 🤖 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的，用于身份验证\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "# 用于配置API请求的基础URL，支持使用代理服务\n",
    "_set_env(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 🔬 设置 Langfuse API 密钥（智能体评估必需）\n",
    "# Langfuse 是用于 LLM 应用可观测性和评估的平台\n",
    "# 注册地址：https://cloud.langfuse.com\n",
    "print(\"🔬 配置 Langfuse 评估平台...\")\n",
    "\n",
    "# 公开密钥：用于标识你的项目\n",
    "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
    "\n",
    "# 秘密密钥：用于认证，请妥善保管\n",
    "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "# 服务器地址：选择离你最近的区域\n",
    "# 🇪🇺 欧盟区域(推荐) https://cloud.langfuse.com\n",
    "# 🇺🇸 美国区域 https://us.cloud.langfuse.com\n",
    "_set_env(\"LANGFUSE_HOST\")\n",
    "\n",
    "print(\"✅ API 密钥配置完成！\")\n",
    "print(\"📊 智能体评估功能已启用\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afe9ff57-0826-4669-b88b-4d0501a509f5",
   "metadata": {
    "id": "afe9ff57-0826-4669-b88b-4d0501a509f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Langfuse 客户端连接成功！\n",
      "📊 智能体追踪和评估功能已就绪\n",
      "🔗 访问仪表板: https://cloud.langfuse.com/traces\n",
      "\n",
      "🚀 深度研究助手（评估版）初始化完成！\n"
     ]
    }
   ],
   "source": [
    "# 🔧 初始化核心组件：LLM 和 Langfuse 客户端\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langfuse import get_client\n",
    "\n",
    "# 🤖 初始化OpenAI聊天模型\n",
    "# 使用GPT-4o模型，这是OpenAI最新的多模态大语言模型\n",
    "# temperature=0 确保输出结果具有确定性和一致性，适合需要稳定输出的场景\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 🔬 初始化 Langfuse 客户端（智能体评估核心）\n",
    "# get_client() 会自动从环境变量中读取 API 凭证\n",
    "langfuse = get_client()\n",
    "\n",
    "# ✅ 验证 Langfuse 连接\n",
    "if langfuse.auth_check():\n",
    "    print(\"✅ Langfuse 客户端连接成功！\")\n",
    "    print(\"📊 智能体追踪和评估功能已就绪\")\n",
    "    print(\"🔗 访问仪表板: https://cloud.langfuse.com/traces\")\n",
    "else:\n",
    "    print(\"❌ Langfuse 认证失败！请检查 API 密钥配置\")\n",
    "    print(\"💡 提示：请确保 LANGFUSE_PUBLIC_KEY、LANGFUSE_SECRET_KEY 和 LANGFUSE_HOST 正确设置\")\n",
    "\n",
    "print(\"\\n🚀 深度研究助手（评估版）初始化完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419257b-2c6b-4d68-ae38-4a266cc02982",
   "metadata": {
    "id": "3419257b-2c6b-4d68-ae38-4a266cc02982"
   },
   "source": [
    "我们将使用 [LangSmith](https://docs.smith.langchain.com/) 进行[链路追踪（tracing）](https://docs.smith.langchain.com/concepts/tracing)，便于调试与分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5102cf2e-0ca9-465b-9499-67abb8132e5d",
    "outputId": "9ef35fad-1514-41ed-e436-bdf87699f20e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "LANGSMITH_API_KEY:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 追踪配置说明：\n",
      "  🔬 主要使用 Langfuse 进行智能体评估和监控\n",
      "  📈 可选启用 LangSmith 进行链路追踪补充\n",
      "  🎯 两个平台配合使用，提供全方位的可观测性\n"
     ]
    }
   ],
   "source": [
    "# 🔬 设置 Langfuse 追踪配置（智能体评估必需）\n",
    "# LangSmith和Langfuse都是优秀的追踪平台，这里我们使用Langfuse进行评估\n",
    "\n",
    "# 设置LangSmith追踪配置（可选，用于额外的链路追踪）\n",
    "# LangSmith是LangChain的官方监控和调试平台\n",
    "_set_env(\"LANGSMITH_API_KEY\")  # 设置LangSmith API密钥（可选）\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"  # 启用链路追踪功能\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"DeepResearch-Assistant-Evaluation\" # 设置项目名称\n",
    "\n",
    "print(\"📊 追踪配置说明：\")\n",
    "print(\"  🔬 主要使用 Langfuse 进行智能体评估和监控\")\n",
    "print(\"  📈 可选启用 LangSmith 进行链路追踪补充\")\n",
    "print(\"  🎯 两个平台配合使用，提供全方位的可观测性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea",
   "metadata": {
    "id": "f8fe5d93-e353-44bb-be3e-434654bcb7ea"
   },
   "source": [
    "## 分析师：人机协同（Human-In-The-Loop）\n",
    "\n",
    "通过人机协同的方式生成并审核分析师角色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e",
   "metadata": {
    "id": "1eee8e60-e548-49b1-88ec-a4f3aef2174e"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"\n",
    "    分析师数据模型\n",
    "\n",
    "    用于定义每个AI分析师的基本信息和角色特征\n",
    "    每个分析师代表一个特定的研究视角和专长领域\n",
    "    \"\"\"\n",
    "    affiliation: str = Field(\n",
    "        description=\"分析师的主要隶属机构或组织\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"分析师姓名\"\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"分析师在研究主题中的具体角色定位\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"分析师的关注焦点、关切点和动机的详细描述\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        \"\"\"\n",
    "        生成分析师人设描述\n",
    "\n",
    "        返回:\n",
    "            str: 格式化的分析师人设信息，用于后续的AI对话中\n",
    "        \"\"\"\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    \"\"\"\n",
    "    分析师集合数据模型\n",
    "\n",
    "    用于存储和管理多个分析师的信息\n",
    "    支持结构化输出，确保AI生成的分析师信息格式正确\n",
    "    \"\"\"\n",
    "    analysts: List[Analyst] = Field(\n",
    "        description=\"包含所有分析师角色和隶属机构的综合列表\",\n",
    "    )\n",
    "\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    \"\"\"\n",
    "    分析师生成状态管理\n",
    "\n",
    "    用于在LangGraph工作流中管理分析师生成过程的状态信息\n",
    "    \"\"\"\n",
    "    topic: str  # 研究主题\n",
    "    max_analysts: int  # 分析师数量上限\n",
    "    human_analyst_feedback: str  # 人类反馈信息，用于人机协同调整\n",
    "    analysts: List[Analyst]  # 生成的分析师列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "fd088ff5-4c75-412c-85f0-04afd0900bfc",
    "outputId": "a7d4bbfb-b9e1-4c81-d490-f8f9214425ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 智能体评估版分析师生成工作流构建完成！\n",
      "📊 所有节点调用都会自动记录到 Langfuse 平台\n",
      "🎯 主要评估指标：LLM 调用次数、令牌消耗、执行时间、路由决策\n",
      "\n",
      "图可视化：\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKkAAAF3CAIAAABR9PyTAAAQAElEQVR4nOydB1xTxx/ALy8hgy2igLIRRaUiLhytVgWtqy6s1r21zrp3tW4pWquUOqtVq9Y6q61a66pV3KKiKDJly94kJHn/X/IgBkhi4p+Y8O5964e+vLt3b/zufve79TsOSZKIAUs4iAFXGNnjCyN7fGFkjy+M7PGFkT2+GLXsM5NLn4blZaWKykqlSEKKyiqFstksiYRksZCilcoiWKS0/Ad1nkXIjklpRQSIAVGkVZu1BJuQSqRVTrIgCdn/UJU05fFZskRIeTCqlBqPR7AIkmtK2LvyW3e34vK4yFhhGWH7Pj2h6J+jb3LSJHDM5iCeKcHlE4hAUiFLORqLjUiJ7A0QWXFeSRAkS/ZfFdnL5AlvLJNlpaSUZKl8A+qKt5fD/wkqhJBnAtk/ErEqJcXhE1KxWCQkhSVSSRnicFF9Z/6g6Y7I+DAu2ZcUSn7dGF9aRFraEE3bW7UNqItqOVePpcc8KYQ3qtuA++UCZ2RMGJHsT4UmJb8qdXDnDp5pXN/o/wfy9PFtrwuzJa0DrNv1tEXGgbHIfu+KWClJTlrrgehLbETB3wfSbRvyAmc7ISPAKGR/YE28hQ1noFFWijXO3hUxjX0tPhlUHxkaw8t+19KYug1NBk+nm57XwJ4VseaWnGGGrv4JZFD2fxtn24CLleCBiWvcC/PEf+5LQQbFkLK/cCClTCQdNMMoKr8PzMS17vERxZkpJchwGFL20eHFgbMdEK54tjI7GWLIom8w2R/ZlGBpw65T3xThSo8RDmIReevPDGQgDCb7rLSyz8YZ3tY1LO4tzCL+y0cGwjCy//tQKk+A6jc0Q3jz2WgH6P3NTDVMrW8Y2b9+WVzfSYA+LIsXLz5z5gzSnYCAgOTkZKQfTC3YN89mI0NgGNmLislm7S3Rh+X58+dId1JTU3NycpDeqOtgkpUsRIbAAH07eRmigxtez9jSCOmHmzdvHjhw4NmzZ7a2tj4+PjNnzoSDNm3aUKHm5ubXrl0rLCw8dOhQWFhYTEwMhHbp0uWrr77i8/kQYeHChWw228HBARKZMmXKzp07qQshzubNm1FNc/9y1v2/c6Zu0tfX0IAByn3c8yK23qYNvHjxYvbs2W3btj1+/DhIMSoqatWqVUieIeDvihUrQPBwcPTo0f37948aNWrr1q0Q/9KlS7t27aJSMDExiZazZcuWwMBAiAAnobLQh+AB5yamUjEyCAaYu1GYJ2ETLKQfwsPDofiOHz+eIAh7e/tmzZqBFKtHGzlyZPfu3d3c3Kifjx8/vnXr1qxZs5B8iD8lJeXgwYOUGtA39R0FpBQZBEPM26k236EGadmyZWlp6ddff+3n59e5c2cnJyeFtlcGCjco/JUrV4JiEItl5c7GxkYRCnniwwiewlADKgbQ+abmrOoTpGoKLy+vbdu21atXb/v27QMHDpw2bRqU6erRIBSUPEQ4ffr0/fv3x40bpxzK4/HQhyIDGnj6KgjvwACyb9jITFyG9EfHjh2hXj979izU9Hl5eaADqJKtAMzbEydODB06FGQP9QKcKSgoQAYiObqEMFAHmwFua+cigJye+Eovn/vBgwdQc8MBFP2+ffvOmzcP5ArtNOU4ZWVlJSUl9euX9yqKRKJ///0XGYjElyUmH07LVMIwWc7EhPXkX730ZYKGB/P+5MmT0CiPiIgAex4yATTYQI2DsG/fvg0aHsxAV1fXP/74IykpKTc3d/Xq1WAl5OfnFxUVVU8QYsJfaAhAakgPpMaWWNqYIENgGNnbu3JT4/TSoQEGPGjy4OBg6IybPHmymZkZ1OscjsykBeP/3r17oAmg0K9fvx6sOWjCDRgwoF27djNmzICf/v7+YOFXSdDR0bFfv347duwAEwHpAVEJatvDMFNSDTZvJ2RO9IzvDdChYVRcP5ERcStv+mbDfAeDjeOZWrKPfPca4c2zsDyPjww2oGWwdTlfznfa+028hgigtMEoq35eIpFAhc1S00MAbTZra2ukB6DXCJoMKoPAWoQOA5WP5O7u/vPPP6u86s6FN6BzPxtrsNkrhpyr+fvWxMJc8bhVbipD36/dZWFhgfSGukcSCoXqugQgQ8AIgsqgkLnRn3xu4/OpDTIQBp6nu2tJrKevadcv7BFmHFgXz+EQwxcZcpKqgefpTt7gHnm38OmtLIQTx75PEAulhhU8MpK1GT8tiPYNsGrfox7CgMNB8WwOMXSu4aelG8uarND50XXtTYbOd0G0Zt+qOOhTHrfKHRkBRrQWc/+q2MI8qW9Xy06f03AO59ndKQmRxU6NBf2nNkTGgXGtwb57UTaJBdpKjo353Yfbm5rXercgiVGFYX9lZySK+KbEwOkONvYfepaiBozR98K/pzJe3M0XlZJsDuIKWNDdbWrBNuFxxOK3jyr3sKHUnmYhltwzArwNIfeOQZZHQ1KywgdHpb+yF6f+KNJgs6HzQJ4YS8UcA+o7USdld2EjqaTyLeT/4xAskaistJAsyC0TFpMSCWluzfbrXder9Yeen/hOjFH2Cm6ezUyKKioskJBiJJUiSZmy7FnSyk9OonKpyyVX7oGjQsZKfylHHfKYUnk8RZ+MzPuGhLqqPH8g+YCv/IiskL3sckS+jVzu7AOiwE8CcbkEyZbyuIR5XRMXL36rrsay2r46Ri17fbN8+fJOnTr16tULYQnWfrbEYjE1xIcnjOwZ2WMJI3t8gXFCGH9DuMKUe6bcYwkje3xhZI8vjOzxhZE9vjCyxxdG9vjCyB5fmL4dfGHKPb4wsscXRvb4wtT3mCKVyvy+EISBV6cYEHxlj7nCR4zsEcYwsscXfF8ec0MPMeUeYQy+L0+SZIMGDRDGYNyzweEkJiYijMFa9lX8beIGI3t8YWSPL4zs8QVr2Uuo5fa4gu9IBpI5W2DjXPSxlj3mah/vji1G9tjCyB5fGNnjCyN7fGFkjy+M7PGFkT2+MLLHF8xlj6NfzZYtW7LkKM7AR/j444/1tAua0YJjn26HDh2IytjZ2Y0dOxZhBo6yHzNmjPKu14Cnp2fr1q0RZuAo+/bt27do0ULx08rKatiwYQg/MB3HGz16tKLou7m5derUCeEHprL38fHx9fWFAzMzs6FDhyIseR87//qptNJCVH3OC7XlAJvFklROkzqPlDadUD6piKTYn6BqsohkESz5qtmqKHa6qP4kqGLrjKpPInsC2UF+QcHj8McmXBO/dn6VttQgECmt9ORVttd4m5psE4a30ZQvUQmbYEmkZPUIKi+R3RRRj6v8AUm5yFCVj1DHjtOuh86bNOgm+2Nb4zOTxAQHESxCXFb1QuqrUW9Y6XzFoxPydyl/DfmuJ4o4BLwWPItUxYeAZGU7okhVPKds4xIpQio+nOpXU/qIslBqMw3Fk6uQfflDyrISq/J9FNkIVZO9IttVeRdqqw3l9CveXf5ZUKUbKL+C4lvJnlb+lZQx4cl2b4GU2wTU0WlLbR36di4cTMlOFQfOdxYIuIjByIh/lnfjdIapJbt5e223A9a23J/5KTEjVTh0XiPEYMQcWhvd9Yu6Xm3raBNZW1svJVbYrqfBdu1l0BI7V+6tP7O1jKyV7GOe5sNfN29G9saOh4+VsFhbA06r+l5UXL4XHIORI7DkSsq0jayV7CXSqqYpg5EiRdq327Aew8UcRvb0gqVDXO1kz9ItUQaDoUsnrXayJ3VLlKFWoJXsWQTJlHv6oZXsSSmLKff0g7H18IWRPa1gIZb2lTMje1pB6tC1o73sGVuvVkDqICft7HwWNWOEwejRRUxajeOBHpGZ+gzqOXHyqH8PP1Sr0HquppGJfuDggJTUZEQLTp0+tmHTSvTB0bq+NyaVn5aWmpubg+jCy5fPkSHQo50fFnbjh+2bMjLeNPJoPGDAF70++xxOrly1kM1m29k5HP3twLergjp/0i07Oyv0py0Rzx6Xlpa2bdth9MiJTk4uVAonT/12+/aNyMgILo/n06LVhAnTGzZwfBR+f+68qRA6YmT/Tp26rF29WSwW7/059Pad/968SfP2bjmw/xft23+szeNduXrxydNH+fl5Tb28R42a6NuyDZKXwoOH9mzdsmvltwvj42Pd3RsNCRzxWc9+Gh5JOdnZcybxuLygTSGKMyu+mZ+VnRkasv/16/h9+3eEP35AkmTz5i2GfTH6o49afj138uPHDyHa33//uXPHIc9GTU6cPHLx4rnEpAQXZ7c2bdqPH/cVfDGkLTroZ610PkHNENUF+LIrVs6fMH76xg3bPv64a9B3q/+5fAHOm5iYxMZFw791a7a0+MhXIpHMmTcFPsecr5f+vOe3OtY206aPSU5JgphPn4ZvD/mueXOf1auDFy/6Nicne9365XAeJLRh3VY4+PXQGRA8HGzbHnT8xOGBA4Ye/vVsl87dQWbX/72s+fEgn63bsFwoFELK69dtdXZ2XbZ8DuRC6gkLCwsgzQXzVlz5516Xzv7w8OnpaRoeSZnen/V/8PAulRR1I8iUPQL6iEQiEDNIcdPG7Zu/+4nD5sAdIRQyWdOm3j169Ll6+X5jT6+TJ48e+vXnwMHDjx4+16/f4D//Og2FBOmADvpZq3Iv79LVTelDBocyHeDfC47btmlfVFRYXFyE5HkoLS1lR+hBPp8PP8PDH0Bp2Bz8UyvftvDzq6lf37x1/cSJw7NmLmzW7KN9e485OjpTOxyIy8qWLp+Tl59nZWmlfCOQ38W/zw3/cuzn/QbDz969+kdEPD5wcDdkAg2PB3ffs+uoQCCwspLNaoVyf+aP408jwqmrysrKxoyeDA8Axz179IV3iY5+aWdnr80jde3aIyQ0GDQKyA9+/nfzGvzt1q1nYmIC5JXBg74EAcOZld9sfPzkYfUV4HCySZNmPXv2heO+fQb6+rYtKS5GWsOq+TFcHcfxQKfFxL7ylwueYuqU2YpjUGWU4AH43FDOKMEjec5o6dMa3h/JnV6mpCT9GLo58kVEUVERFSE3J7uK7KOiIqFItW3TQXEGUjh/4Y/quaQKkBf37A0BlZOVlVmeuJIN4eXVnDqwsLCEv6AJtHwkLpfr373XP/+cp2R/48aVTh27WFpYQkVgbV1nY9CqAP/e8ITe3j5UFVMFOL9r93bQNC1a+Hbo0LlKhfJOpDXevtd1/B6EIZVKeTy+ylCoKRXH8E2hkHXtXukrwDeCvzdvXl/+zbwRw8dNmTzbw8Pz/oM7CxfNqJ4aJZWZsydUOZ+TnaVB9qDDZ8+Z2Mq33Ypl66E0Q54L6NleOYLKOk7LR+rbZ9DpM79DzVXXxvbO3ZtwCzjJ4/F++H436HConsA6adDAcezoyQEBvatcCznG1NQMlN+moG9Bu3z6acCUSbNsbesh7WDVuM7XtdxDUSYIAvT8O2PWrWsLinfd2u+VT7IJmWlz7q9TYApNnDCdOknJWEUK8u8yb+6yhg2dlM/Xr2+P1HPt+iXIoFBnw91R5RKvAS0fCbIFVOHnz5/x9PQSCEz9eo1SuAAAEABJREFU/MoXeoJVAZXauLFTHz68C5pp/cZvXFzdqSpAAXw3UPXwD8xMiLb/wC74jOsrf5+aQvt+PV0MSIKASgv0ueLM7j0h8K2nT5tbJaaHR+OSkhKQk0K5Qavd2kpW7sH8trdzUMQE5anyXo4NnXlyRaJQoVCtQqVjamqK1AOJgzKnBA+80zZUXKXNIyG52QE2WlLSa9D/lHEAZs2z50+gsQP1XceOnSFDfNa7E1RYVWQPFn7jxk3d3DxcXd3hX0FhwZ9/nUL6Qet+PR2XbPbvF3jvXthvxw5CkwzMqCNHf4H3qR6tdat27dp1DA5eA0o4Ly8XVOXUr0ZduPAHBEHL8N7923A5GES/H/+Vip+Wngp/nZxd4e+1a5eeR0aAjMeOmQLGHRjhkL1AivMXTtv6w0bNj+fu7gnV/B9nT0Did+7eghIGRh80ETVfpeGRqtCta8+srAxQ+JAJqDOQb6AW/2nH1qTkRLD7fj28DxLxbu4DQaCxoNH48NE9yLWXr1z4ZtWCW7f+BXvl9u3/bvx3hYqjD/TVvgdLNb8g7xeZyioCxT550kzFV6gCNNhABqvXLnn+/Cm07MFCHDRI5glh/PhpYI4tXzEXFMOggcNAP6emJi9eMmvZ0rX+3T+DBjeY3/Bdvt+yc9jQ0aA/Dh/dDyI0MzNv3qzFvHnLNT9e9249ExJiIcd8v3UDNEMWLVwFxfTwkf0FBflQ7NRdpeGRqsSEHNm6tV/Gm3RFjgcjbu6cpft/2Xns90Pws01rvy2bd0DJhuN+fQaBAliwcDo0/+bNXR7yY/CyFTIFaWNTF5T/kMCRSD9otR4v4nb+td/ejFnFLMbTFtBAQ4b2ghzfp/cA9AFJjSu5uD955latJKVdfc+M4WkN9DcnpySePHXUxcVNnaozErRt47Fq2zAeKPAjR/arDALrOmTbz0g/QIW9Z++P0D2w6ptNLOP+atrN1SRr35os6BCFLjaVQdCfivQGtP7hHzIY+pizVdvKvYW5BfxD2KGPOVtMlU87tJK9zOENwczboRvajeNBqZcyBb8WQLKYOdq4opPbNG3785k52vRD23E8I2+qMrwH2rXvEcLQzT7t0XruBgPtYHwv4It2spdKOFysd02uLZCkhGOibWStJOrRjC+RME7WagFpCaXaG+VayV5QR8A3ZV0/kYoYjJu4p4W2jjwtI2uryftMtEt4ViQSiRCDsXLlaKKwSBw4y0nL+Dp0BIHgdy1+bdPAxNnTtI49n5S+I9+wZNPF3zWmyKJGnlREIiumhpNvf1ULLXdQXym08oXyN1Q8hPzs2yC5v3zl6xVB5VspqGviVEmHugWrkkWsnAL1nGTF11AkK998gVXpsZWellX+iJWfQrFjg+JGUjIjuSQhMk8qYU1Y7YG0Rud9Mw5vjM/PEUvFSEoDA4CsBc3XKs9YfW8JgsMyMSHr2JsEznJBuoDX3ogzZswYMWJEhw4dVIYOHz6cx+Pt27cP4QFeLbcnT54o746mTEpKSlFRUWRkZEhICMIDjGQfHR3t4OBgZmamMvTZs2cZGRlisfjUqVM3b95EGICR7DUUeuD69etCoRAO8vLygoKC8vPzEd3BSPaPHz/28VG7xgW0vWKsMikpaeHChYjuMOVeBmQLxZpqJF+EC5FDQ0MRrcFF9rm5uaDGnZ2dVYbevn37zZs3ymdKS0uPHTuGaA0uc7Y0V/ZhYWFSqRSKu7m5ubW1tYmJyfHjxxHdwUX2miv7/fv3UwdQ3NevX7969WqEAbjofM3lXgGfz3/48GFqKhajVrj06/n5+UGrnXKDoJkXL17Y2dnVqaPV1pK1Gix0PvTbNGnSRBvBI5mXJS+EB1jofC0VPgXo/B07diAMwEL2mg29KtSvX//8+fMIA5hyXxVHR8cNGzZI6TBE/Q7oX9+npaVBwx3MN+0vadasGcIA+pd7nQo9BQzhX7p0CdEd+step8qewsrK6u7du4ju0F/nQ7nv06ePTpdAfHVze+gEzWUvFoujoqJ0rb95PJ6DgwOiOzTX+e+h8CmmTZsGo/iI1tBc9u9h6FHAaB70BiJaQ3OdD+V+8ODBSHdWrFhB+5EOptyrRiAQaPbETQPoLHvo1QHBQ4MN6U56ejrtp+zRWfb29vYwMKM8EU97EhISCgoKEK2heX3v5uYWFxfn7e2NdKR169atWrVCtIbm9b2rq2t8fDzSHTabreV4f+2F5rKnyj3SnTVr1pw7dw7RGkb2qklJSYGBfERraK7WXFxcwGpDuhMSEqLLTqS1EvrX9yD79+ilob3gEQ5juO+h9rOysnr06IHoDv1l/x6mPnTsNGzYENEd+o/fv0e5hzFfHLxvMDpfBRKJpPoO1fSD0fkqCA4OPnnyJKI7TLlXQU5ODlPf0wE+n29tbQ1jejC0o+UlGzduRBiAxdoMXdV+YWEhDktUsZC9Tmofxnx79+6Nwz4h2Mne399fc+SMjAxPT0+EATRff9+rV6/i4uKCggJFOcZnqeU7obOtB001kHppaSlBvFVvtra2mq+C+NC4Nzc3R3SHzjp//vz5TZo0gY4axRlQcu9ccLN3717ae9iioHl9v3z5cnd3d8VPKPTt2rXTfAnYemAfIAygv7+d33//PTQ0FJS/VCr18PCAn4hBDv3t/CFDhoCeZ8nx8/N7Z3wYxFOuJmiMVrZeXGS+tKzqXAZSnnHI6merN4wrtnpQ3lhCtj0Ei6xyUt1uBtX2itAYu1rAiAFz81IEuXm5TV26xjwuUrtfAomkSLp40eqgoCCN6VV7vIr9Nyp+Vv0s8s0wNHUYqP6YVeO83WRDczRzC7a9m+CdMd+h849+F5edLoHHllQf1qq+g4P6u2iZSch37rGi7TOw1H9J1nts9mcUG2xo+RCErKiwTZBrc9PPRjfQEFGT7A8FxYqKpJ8MtLN3s0AMtYrnt3MeXMpq1d2yfS+1M07Vyn7/t7FsLhowzR0x1FoOb4pu4MrrN1n1zlmqbb1nYTmlRVJG8LWdLoPtE18J1YWqln3k3Xy+ObMJaq2nYSNzsIgeXs1QGarazheWsth0X5GECWw2kZep2legagGLRVJSSv9BTBwoE0GflmoVzhRufFEtewwmLuACwWax1VhuqmUv27wVMdABUookaprxjM6nObL+G1K1GmdkT3NYBItQo/NVnyY4ai9gqF2Q0GJT4w5edbmXikmmjUcTSLWGO6Pz6Q5Bqhv9Y2RPc1jqB36Z9j3NATNfqqa9zrTv8aXGrPkhQ3vt2fsjqiX8d/PapMnDu3Zv8+zZE1QTbP1h47gJX1DH/Qd2P3BwD6oJYmOj4SGfPHmE3hc2h6VuVE5NG48whjlKeuTI0V9AtW3ZvMPFheZzFCRiUp0bCTVtPCmit9IvLi7yadHKt2UbhDE1aetxOCYnT/22Y+dWLpfr7d1yyeLVVpYyH9a9+nw8ZvTkYUNHU9GCvlsdExO1c8ehuLiY8ROHhmz7edee7aDW7O0chg0bA/JYsXJ+UtJrL6/mM2cs8Goi2+6ksLDw9+OH7t4Li4+PqWtj27Fjl/HjvuLz+RA0YJD/uLFT8/JyfzmwSyAQtG3TYcb0+XXrql14JRaLA3q2h4P4+NgzfxyHuzdv3uLCxbN/nD0RFxft5taoW9cegwd9qZhWqy6ouLh43Ybljx7dg/P9+wVWv9Gp08cuXPgjOSWxlW+7uXOWWlvLNtgNC7tx5erFJ08f5efnNfXyHjVqoiL/5Rfk79z5w1/nz1hZWbdp7Tdp4kw7u6oOA6AqOXxk3/dbdjX1ao60g6V+bq9qnf9+tt71f/8pKirctHH7gvnfRESE79v3k+b4JiYm8Dfkx2DIGVf+udfc22f3nu1QcS5auOri+Vs8Lm/b9vK50idPHT18ZP/QL0atX7d1ypTZ165fAkkrEvnttwMEQZw+dfmXfSeeRoTv/2WnhptyOJyrl++7urr3/zwQDkDw/1y+sCno28aeXocP/TFxwvTjJw6HhG6mImsICt68BjJo8Hc/rfk2OC4+5vad/5Tvcv78mZycrKlTv162ZG14+H14RyRf6QfZRSgULl70LbyIs7PrsuVzsrOzkDxHLl4yKzMrA6ohyPFvMtIXL51VxecPPMy+/TtWLFuvveCRTPZqZ4fXZPve1NRs1MgJ1PHNW9chd2tzVffun7XybQsHn3b2v3z5wuefBzZrKnN73blz99CftlAz278YMrJL5+4uLuVLpSIiHt+9d2vK5FnUz4YNnUaOGC87MreAch8VFYl04a+/Trdo4fv17MVwXKeOzbgxU4OCV48cPh6O1QVJJJKr1y4tWriSelR4klth/yqnKTA1BW1EffW+fQdBphGJRKCo9uw6CsoJSjach3IPigcyK7waZJ3IyIhf9h2HDAFBTk4ux34/RGULivDwB5uCVsGNOnXqgnRBCn26OrXxCIIl1b3gf+TdUnFsZWktEgq1ucrJyZU6MJMvfXV3a0T9FPAFZWVl8Ml4PB4U7nv3wzZuWhkdE0WVBpCEIoXGjZsqji0sLEH3IK2RSqURzx6PHjVJccbXty2chIz7ycdd1QXZ1KmLZA5b39qJTZo0e/XqheJnm9btFcWtWbOPyo6WQZlu4NAQ7Iw9e0PCHz/IysqkQnNzc+BvTMwrU1NTSvCyN/L0Wr50LZJVdjIf/q8T46Em7d7tM0W9WSPUZLlXdjquvd8KovKoEaFqEGnX7u1QBEHbQ7GGWhAak1Avvse9qgN5C3LY3p9D4Z/y+ZycbA1BlMdVU8HbTVUgpyrHARX4NkgeDSwSNsGePWciVP+gtyFDwGNTlgeSLQAt5PH46h7yh22bIMfb2NRFuiOr73XS+aAoSFJfjTyJVLfVbvAoZ8+dCBw8vG+fgdQZqjTUCKCHocD1COgDVYzy+QYOjhqC3rxJg4NSYaniJBRo5TilpSWKY0oPgZ4HMwXyE1T2oPZRRYmngLxSUlIsm1mnKuv37NEXLN/NW9a1adOeqh+1h0Rqm2wfoj+fy+XBiyl+JiYmIF2AwldSUmJrW76+BD5flcr1/8TDo3FBYYHC3obbpaYm169vpyGIkhCYHU3k1Q2cv//gDmXJU0RHv1Qcv3z5HBo+9Wzrg20PVRIleCQzjS8r4kBzBizBl1GRlB33+nX8lq3rZ05fQKk0yH9gdty7F7Zu/fKf9x6jWk9aw9LNzq9ZQL/Be0I7DY4PHtqbmflGp8vhw0FFeF7WXkoCzQnWFhgWBQX577cRTnUmTZhx8+Y1qESg2D19Gr56zZK586dCDtMQVK9efW9vn/37d0A+Brt97bplVeodsPzBWAOTMOrVi4t/n+v8STcwWdzdPaGahxYjKPA7d289fHgXlAGlQqBAg8W6a9e2G/9dvXf/NjR2Mt6kK2xbioULVkKtCkYP0gkYjVcT8iFkDw1uMI769f8UqjehsBRsFqQjUEHyefyx4wJHjh7QulW7iRNnwM+Bg/1T01LQ/81HH7XcteNX6MAxBdYAABAASURBVGAYODhg/sJpoKLXrtkCBqbmIOi9aNrUe/LUEX36dYbS3LtXf8XqNrG4bEjgCOgt9u/hN3feFMip8AXgfPduPaEddODgbvgOJ04cnjVzYYB/b2i7bvl+PQg1OCgUTPJvVi5YuGgGXyDYsP6HKpu2mJmZrVyx8c6dm9CJgrQG8iShpuCrXo93cH2CVIwGzXZBDLWcg2uim7Wz+vSLetWD1IzjSWTmHmKo/ZCIpU7nq5E9Imu1c0Gom5cu+1pd6KGDp6neFSwg1a6yp+e8HVk9veuwulCMBK8R2s7ZcrBvgBjkHV/q7Hl14/csHBzK4gCpvo2noV8PMdAAWbnXqU+XKfS0QVbu1RRjdeP3TLGnCRr6dpj5+fRHXTFWXe4ZnU8bZFOwdNX5iIEWsJCe52xd+PuEtfX7zCxgeA9gYLNVy45aRiaRjv16MlelSAeEwpKmTZsghg+CqSkP1QRqy71OFX63br3MzRi/qx8IKSnSPrLMztervx0LM0bhfzjYLK72kWVrMdV07Knp02XRfE0WRqjvz1fTp0vSfE0WRujan4+YoRwMUKMPmE5duqDz/HwGGqFWgatp3xNMfU8TdF6bAeYBo/NpgvraW8NYDiN8mqOub8coNoZi0CuMrUd3mLkb2EKod7qixvcCm8XU9vRAqrMvZQnjS5n+MDofXwzsJf/e/dsDBvlriPDkyaNXSn4M9MfFi+cKdHfnQXlsi42N1iZyaWnpqm8Xde3eZveeEGQEGFj2bdu0P33yHw0Rfti+SVxWhvRMTk52SGiwmZKTHC2Jjoni8Xiurlo553z48G7Es8eXLt6eNHEGMgLUz9n6INbezNkTAvx7f95v8PSZ4/zadbp167pYIq5Xz27mjAUNHBpOmzH29ev4nbu3jRk92c3VY8v36+PiY+Bbuzi7TZk8u359uzt3b4X+tMXLq3lcbPS2H/bOW/CVd3Of8PD7Xbv2sLNz2LP3x18PnqZuNGx439kzF3Xo8MnUr0Y19/bJy8158eKZk7Pr+HFf8bi8hYtnsNmcufOnrlvzvZmZDjng5cvnno281q5bdvXaJc9GTYYPH/dpF5ka2/5j8L17YQK+wMzMHG7h7e3z1/kze38OZbPZ8xdOCw4KfRR+/8iR/SUlxRKJpHfvAQP6D4GrQB+kpaW8yUi3t3NYtnRt9USQ7hCE2n2y1PpWROpGfWuU6OiXnp5e0JUUFxcNx8Hf/bRn1xEk08Bn4W/fPgM93D23btnl27LNtu1BVlbWIdt+3hF60NTULHjzGoiQlJiQk501dMioXTt/5fP5rxPiCgryd+44NGzoaEitsacXdZf8gvz09LQmTZpJpdKE13FcE+7yZev27zsOP4+fOOzs7Orj07pnj75wI2XBr16zBPSz8j+Ft2QFIPuMzDcjho+/8NfNjh07/yj3vHjmj+ORkRHr122FJ4FkFy+dJRQKe/fq7+ri/sWQkXAXCF23fvnkybN+Cj0ge5JfdkLdh+RuduITYoM2hoDgVSaCdEdKkrrN25Gh/wH8hIQ4eB8oLsnJiXAwf/4Kc7mLPVDylMMx0KiNGsmmgD59Gh52+wZ8LBA/h8Pp0sU/JvYVFcGv/cfu7jKXfCDdwqLCEZSTRXmQZ4XsX716UbeurY1N3aSk1wRBgBZBco9wTRo3pZxdQUZp5NG4yuN9s2LD1cv3lf/t23usSpyXUc8hNQ8PT9BGrXzbQWrFxcW792yHYurYULb7tL9/r6KiovT0VDiOiooEJQEHu/eG9P88kHIXCzkP8jflmyk29tWggcMEAoGGRHRG/fQ7tb4VP8BYDnwLEBvI4MXL5+5ujSwtLKnzoI0DA0cguUi6de0JB6AhwVD6vH9XxbWUG8KoV5GUIGVXvXwGMmjYwJH6CdcGDh6uOKbywSuZMmhKOeIFMjMzIDOBvRYXF6PIKNoDjwRWXrt25dOlM7NkqcG9QE4LFk5XjmlubpGalgJZE3QP3C4i4vH0afMUobl5OZaWVnl5uSmpyZQ/N3WJoBpFw1xNvQtfVjTl5QDKpUdFsQN5wDdqKvdVCuenTJI5ThWJhAEBvZcuXq18OXx6kBnIkvoJOamRR/k88ayszOzsLEVRfhoRTun/mJgoi4ocRnnUlNUOcntN4dNSAeh8qMWVz4BNp1z0QeHLHKRWeDwDFd3Sp7VQJLSzsz96+FyV1P69caVBA5nPPnhsqOPAyKDO5+Xngf77yLslFAAH+wYWcgGrS6RmUb8Wk9S7zgfRUqVNuW6Gk2DEgQ6ATACfyV7uQsHNrdHz50+hZMDx88iIoO9Wi0QiiAmWub29A3UhyF6RCOXOj/KCB9/0wYM7nhWyB71KeXu7fOViUVFhl87+iYkJ9evbV3dq+E6dDwofCjGIHMmz7OUrF/r1HQw2KeS8KLl/1bS01B+2baL8CSreEcTv4uJ2994tJG8ibtmyrpVvW8h5srzbqDzvqkvkvVA7JFszazPeDxAeVGmosup+VaGfQX/Wq1cf7HMw7rp+GpCVlTFhEtSFpqWlJYsWruJyuTJhK3nSBZ0/auRE6tjR0XlI4IjFS2eD6QcHkJHd5G56X0ZFThg/bfzEL8DcA3lvWP8DGHfwoVNSkgYP6Xn82AWdpik+efpo+JdjwQgtBnNdLP5q6hwfn1Zwfs23wWDKQVJv3qSNHTPFycmFei9og1AXQoSQ0M1nzvwOSgiUPNTxiLIGKvKurW09lYm8F2qHZFX7WPtlTTz06Q7+mlY+1jIy3gz9ss/F87co3+2YcGBNdNN2Vt2097HGIljvMVlT5SYx6nzEDhw41ML8gy7lATUDpQcrwSP5Wkx1QermbKndQFcDo0dNREYM2HQKB+34QOLmY00lRp41PzzMOB790W1NFsEmmLkbtEFHH2sSKTN3g/YwOh9fGNnTHfXL6Zn19/SHpVMbj1l/Tx9IHf3rEWxm/T1t0HGvJNkcbabc0wQd12Iy9T0OMPU9vjBtPHxhZI8vqmXPNWGJmT5dWkBwEEGo3oBYta3HM2dJxbrtWMxgnEDHjo29aj+cqmXv09miuICRfa0n9mkOmO0+n9ioDFUte48WdczrcE78EIsYajO3zmV5+piqC9U0L+/Uj0lZKaU+n9b1alcHMdQq7l5Mj7pf0HmwbXM/tRtBvmNO5qnQxPQEkUSsdk1X1eRkvUjVjERSzaRvNbOH5XMFVQeo63HS5BZMs9uo90uT+nDq+73fFfqO2ZCs95kuWQ4hW0eLeHyWV1vzTwbYabyLFp23JTklhSXsqvcgCanSNrssudgJkiWtNmzEkn9fVbehLpIfKX1o2fHbEKXYVJwqQfKfLOq/ahdRvuLIqncoD9q1c3fz5s07fdyx8lUV96nIGGS1IPlLseQvRSJVuUQWSlDrW1S8NyEXbvUARWyQn7TSXVnKIzKEbHtj1feVIUH1nLRysq5V+15QRyCgo9YvECYJrDxtHXRwR08nWDgP2pSWlnLkICzBWvaYY2CfK4ZlwYIFd+7cQbiCdX9+YWEhznNUcK/vuVwuQWCq/Jj6Hl+wru8nT5784sULhCtY1/cFBQXYKnzE1Pc8Hg9bc4+p7/EF6/o+MDAwPT0d4Qru7XumvscUpr5n6ntMwbq+DwgIgKKPcAX39j2bzUa4grXOLykpEQgECFeY+h5f8K3voaaH+h5hDL71fZkchDH46nx4caFQqNhHAUOY+h5f8K3voSd/yZIlCGPwre+hJ//Ro0cIY3Dvz2fqewYcwbe+Ly4u7tmzJ8IYfOt7DoeTn5+PMIbpz2f68xnwA+vx+27duonFYoQrWMse+vNFIhHCFaa+Z+p7BvzAWucPHDgwMzMT4QrW8/XA0GPqe0xh5ucz9T2mYF3fjx07NiYmBuEK1vW9RCIRCoUIV3DU+QEBAWw2GwQvlkP18DRs2PDs2bMIJ3As9+bm5omJicpn+Hw+6H+EGTjW94GBgVWWXjs4OEBbH2EGjrIfPny4o6Oj4icM5A8YMADDhfg4yh4a9KNGjYKWPfUT8sGgQYMQfmDaxgMN7+LiguT5oFevXmZmZgg/8G3fjx49GgbxnJ2d+/fvj7DE2Nt4j67nvLyfX5AjEQulEql8I4QquyO8cwdPlTtjVD+pKprq7S/UbLUBEVkEIggWT8CytuP4fmrt1swSGTHGK/tj37/OTBZJpciEx+ZZcs1t+CYCDpvPZVd5XpJVbYP3yvlBpVBl6/EqX0hW7LJROXFStslFlZOq93SRipFYUlZaICrOLi0tEkpEJJuD3FuY9xxpj4wSY5T9uT0p8c+KOTzC1s3K1tka1VqSI9Pz00rgwO+zOq262SAjw+hkv3tprFhMOvnamVvTZEbNm9jszPg8a1uT4YtckDFhXLIPnR9tbitw9jFSJfn/8OpWIpJKJ61zR0aDEck+ZG60o7eNtYMVoimvbr3mctGYFW7IODCWNl7InGhHn7o0Fjzg2dFZgoidi6ORcWAUst+xKMbS3tS6vlG3iGoE9zaO0Ar8bctrZAQYXvYnQpKgc825hR3CgyafuGQkiSLv5SFDY3jZp8aWenRqiHDCuqH59eMZyNAYWPZHgxO4Aux2J3RsVk8iRmF/GXh6uIFln51aZt/E6Do9FHy3/csTZ4OQHjC1ETwLM/AKcEPK/sbpN4hgWdbDcQzNrZV9aaHUsA1sQ8o+9mmRCR9jV8ZsdPk3Q+7aYciKtjBXAlYP0g8Sifj8Pzsio27m5qa5ufh09BvSrEknKmjlhp49u08uKs79+8oeHlfQxLN9/15zLS1tISjtTezRE6vTM+Iaubf27zIe6ROCQyRHFyPDYchyT0qRZT1TpB9OnQu+EXbkY78hS+ed/qh5twNHFz+JuEIFsdkm1/47xGIRq5f8vXDWsbiExxev7kayJVplew58bW1Vf+Gs3/r0mAFxCgr0aI7xzXklBbjqfMDCVi+VfVmZ8H74n90+GdOh3SAzUyu/1p/7tuh56dpeRQRbG0f/LuMEAgso7k0atU9Klu2Q+PT51dy89M97zaljbW9f331g3/klpQVIb/DMOFIxlrIvKdSjw4vElEixWNS4kZ/ijIdrq9T06KLi8h4Vx4ZNFUECgWWpsBAOMrMSuSZ8mzoO1HlLC1trKz32OLHZLMOOpRisvuey9bgCsrREJssf90yucr6gMAvUgPxQxd2LS/K5vEp1kAlHj54XJVIWoc+P8E4MJnu2gA3fv7ig1NSi5r8vZbgF9l9ia+OkfL6OlabRYVOBpVBYyfgqFRYhvSEWidkmyIAY0s4n2Kgwo0Qfsq9X19nERDYFG8x16kxBYTY0pnk8TaZlHWuHsrJSqBoc7BrBz+TUqPwCPfa8iopEfFNDNnENaevxTYnCLL00ckDGPbpOunR1b2xCeJlYBBb+rv0zT557Rw9d86adORzu76c3iESlefkZh44tNzXV45iyuFRcx86QBd+Q5b6+My8pSl/LYLt+MqqBQ+OrNw68irnH55u7On00pP941RAXAAACl0lEQVRSzZcI+OYTRm758++Q5eu6gdEHzbyHTy7qr0IWl5EtOhty2NqQ83ZEQtGuJa+9A4xlHsuHJCUqKz85f2pQI2Q4DKnzuTyumSU79l4Kwo/81ELHJgaejGrgwdP2/epcOayp72z3L7MTkiJUBkGvLZut+vmHDfrGu2kXVENc+feXKzcOqAwS8MxL5H0D1Zk2YUcDe0+VQbnpRZIyad8JBp61YPi5mvu+jZOyOB5tG6gMzc/PFEtUu8ISlQm5JjyVQeZmNlxujTUfSkoK1HXwgVWo7kaWFvU4HNWm3ItrCc5N+b3Hqn7lD4ZRzNMNmRvt0trOwkZffftGRXx4mrhIOHGt4SdrG8Vcza5D6r5+aMjRzA9GQWZJUWaJMQgeGYnsm3eo49PZMuLvOERrJGWShIdpU4OMpV1jRGszXr8sPrc7pVGnhlw+F9GOtOiszNj8aZvdjcfBh3GtyXpwOSvsXI55Xb5rawdEI6LDEsVCydRNHsiYMMZ1uDuXRpeVIit7U6ePav2k/Zh7yaX5Ims7kxELjGshJjLa9fc3z2U8vpYnlSCOgLC0NbVxteILak1FUJhdnJ1cUJwtFIskZpZE9y/rOTexQMaHUfvdiHqUd+d8bmFOmUQsc2khc43AYpGStxHgJCmVH8h9MLx1nCA/LveRoHQe4kulFY4TKs6/TeTtAYuUkm/jyJ00kEoR5JDyaLKnKv+EMCYnpeIhEx6rrj2v+3Bba1vj3Xux1vjVjH6cl5MuLi2RkpXm+1QTeKVAufTlfjNYlIsNltyLSnlwuUMOFosgKZGCEVY5E1RJqOIO8gQJ+eUkqTgLfYwCC6Kek8DJs3Z0VDB+tPEFa1/KmMPIHl8Y2eMLI3t8YWSPL4zs8eV/AAAA///PYOZgAAAABklEQVQDALQDqQxQR0dxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 图渲染成功！\n"
     ]
    }
   ],
   "source": [
    "# 🔧 导入智能体评估所需的追踪组件\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# 🔬 导入 Langfuse 追踪处理器（智能体评估核心）\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 分析师生成指令模板\n",
    "# 这个提示词指导AI如何根据研究主题创建合适的分析师团队\n",
    "analyst_instructions=\"\"\"你需要创建一组 AI 分析师人设。请严格遵循以下指引：\n",
    "\n",
    "1. 先审阅研究主题：\n",
    "{topic}\n",
    "\n",
    "2. 查看（可选的）编辑反馈，它将指导分析师的人设创建：\n",
    "\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. 基于上述文档与/或反馈，识别最值得关注的主题。\n",
    "\n",
    "4. 选出前 {max_analysts} 个主题。\n",
    "\n",
    "5. 为每个主题分配一位分析师。\"\"\"\n",
    "\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    创建分析师人设的核心函数（带智能体评估）\n",
    "\n",
    "    功能:\n",
    "        1. 根据研究主题和人类反馈生成分析师团队\n",
    "        2. 使用结构化输出确保生成的分析师信息格式正确\n",
    "        3. 将生成的分析师信息存储到状态中\n",
    "        4. 🔬 自动记录到 Langfuse 进行性能追踪\n",
    "\n",
    "    参数:\n",
    "        state: 包含研究主题、分析师数量限制和人类反馈的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含生成的分析师列表的字典\n",
    "    \"\"\"\n",
    "    # 从状态中提取必要信息\n",
    "    topic = state['topic']\n",
    "    max_analysts = state['max_analysts']\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', '')\n",
    "\n",
    "    # 配置结构化输出，确保返回Perspectives格式的数据\n",
    "    structured_llm = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    # 构建系统消息，包含研究主题、反馈和数量限制\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts\n",
    "    )\n",
    "\n",
    "    # 🔬 智能体评估：使用 Langfuse 上下文管理器记录这次调用\n",
    "    with langfuse.start_as_current_span(name=\"create_analysts\") as span:\n",
    "        # 调用大模型生成分析师集合\n",
    "        analysts = structured_llm.invoke([\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=\"生成分析师集合。\")\n",
    "        ])\n",
    "        \n",
    "        # 记录输入输出到 Langfuse\n",
    "        span.update_trace(\n",
    "            input={\"topic\": topic, \"max_analysts\": max_analysts, \"feedback\": human_analyst_feedback},\n",
    "            output={\"analysts_count\": len(analysts.analysts), \"analysts\": [a.name for a in analysts.analysts]}\n",
    "        )\n",
    "\n",
    "    # 将分析师列表写入状态，供后续节点使用\n",
    "    return {\"analysts\": analysts.analysts}\n",
    "\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    人机协同中断点节点（带评估追踪）\n",
    "\n",
    "    功能:\n",
    "        - 作为工作流的中断点，允许人类审查和修改生成的分析师\n",
    "        - 这是一个空操作节点，主要用于流程控制\n",
    "        - 人类可以在此节点提供反馈，系统会根据反馈重新生成分析师\n",
    "        - 🔬 记录人机交互数据到 Langfuse\n",
    "\n",
    "    参数:\n",
    "        state: 当前状态对象\n",
    "    \"\"\"\n",
    "    # 🔬 智能体评估：记录人机交互节点\n",
    "    with langfuse.start_as_current_span(name=\"human_feedback\") as span:\n",
    "        human_feedback_content = state.get('human_analyst_feedback', 'No feedback')\n",
    "        span.update_trace(\n",
    "            input={\"current_analysts\": len(state.get('analysts', []))},\n",
    "            output={\"feedback_provided\": bool(human_feedback_content and human_feedback_content != 'No feedback')}\n",
    "        )\n",
    "    pass\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState):\n",
    "    \"\"\"\n",
    "    条件路由函数：决定工作流的下一步执行（带评估记录）\n",
    "\n",
    "    功能:\n",
    "        - 检查是否有人类反馈\n",
    "        - 如果有反馈，重新生成分析师\n",
    "        - 如果没有反馈，结束流程\n",
    "        - 🔬 记录路由决策到 Langfuse\n",
    "\n",
    "    参数:\n",
    "        state: 当前状态对象\n",
    "\n",
    "    返回:\n",
    "        str: 下一个要执行的节点名称\n",
    "    \"\"\"\n",
    "    # 检查是否有人类反馈\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback', None)\n",
    "    \n",
    "    # 🔬 智能体评估：记录路由决策\n",
    "    with langfuse.start_as_current_span(name=\"routing_decision\") as span:\n",
    "        if human_analyst_feedback:\n",
    "            span.update_trace(\n",
    "                input={\"has_feedback\": True, \"feedback\": human_analyst_feedback},\n",
    "                output={\"next_node\": \"create_analysts\"}\n",
    "            )\n",
    "            return \"create_analysts\"  # 有反馈，重新生成分析师\n",
    "        else:\n",
    "            span.update_trace(\n",
    "                input={\"has_feedback\": False},\n",
    "                output={\"next_node\": \"END\"}\n",
    "            )\n",
    "            return END  # 没有反馈，结束流程\n",
    "\n",
    "# 构建LangGraph工作流\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "\n",
    "# 添加节点到工作流\n",
    "builder.add_node(\"create_analysts\", create_analysts)  # 分析师生成节点\n",
    "builder.add_node(\"human_feedback\", human_feedback)    # 人类反馈节点\n",
    "\n",
    "# 添加边连接节点\n",
    "builder.add_edge(START, \"create_analysts\")  # 开始 -> 生成分析师\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")  # 生成分析师 -> 人类反馈\n",
    "\n",
    "# 添加条件边：根据是否有反馈决定下一步\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    should_continue,\n",
    "    [\"create_analysts\", END]\n",
    ")\n",
    "\n",
    "# 编译工作流\n",
    "memory = MemorySaver()  # 使用内存检查点保存状态\n",
    "graph = builder.compile(\n",
    "    interrupt_before=['human_feedback'],  # 在人类反馈节点前中断\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# 展示图结构\n",
    "print(\"🔬 智能体评估版分析师生成工作流构建完成！\")\n",
    "print(\"📊 所有节点调用都会自动记录到 Langfuse 平台\")\n",
    "print(\"🎯 主要评估指标：LLM 调用次数、令牌消耗、执行时间、路由决策\")\n",
    "\n",
    "# 图可视化\n",
    "print(\"\\n图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd42b0a-188c-4374-a03c-a2ca8272cdcf",
   "metadata": {},
   "source": [
    "![image-20250930152238678](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301522726.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6c22cb05-c436-4358-8f7a-72d722f9b5cc",
    "outputId": "ca196d63-2a1c-4019-b769-e85da2f5c873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 启动智能体评估版深度研究助手...\n",
      "📊 研究主题: 采用LangGraph作为AI Agent框架的好处\n",
      "👥 分析师数量: 3\n",
      "🔬 所有执行步骤将自动记录到 Langfuse 进行评估\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Pregel.stream() got multiple values for argument 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 运行工作流直到第一个中断点（人类反馈节点）\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 🔬 启用 Langfuse 追踪\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_analysts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_analysts\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlangfuse_handler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 启用智能体评估追踪\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# 检查并显示生成的分析师信息\u001b[39;00m\n\u001b[32m     26\u001b[39m     analysts = event.get(\u001b[33m'\u001b[39m\u001b[33manalysts\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m analysts:\n",
      "\u001b[31mTypeError\u001b[39m: Pregel.stream() got multiple values for argument 'config'"
     ]
    }
   ],
   "source": [
    "# 🔬 使用智能体评估功能运行分析师生成流程\n",
    "\n",
    "# 🎯 输入参数\n",
    "max_analysts = 3  # 分析师数量\n",
    "topic = \"采用LangGraph作为AI Agent框架的好处\"  # 研究主题\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}  # 线程ID，用于状态管理\n",
    "\n",
    "# 🔬 初始化 Langfuse 回调处理器，自动追踪所有 LangChain 调用\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "print(\"🚀 启动智能体评估版深度研究助手...\")\n",
    "print(f\"📊 研究主题: {topic}\")\n",
    "print(f\"👥 分析师数量: {max_analysts}\")\n",
    "print(\"🔬 所有执行步骤将自动记录到 Langfuse 进行评估\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 运行工作流直到第一个中断点（人类反馈节点）\n",
    "# 🔬 启用 Langfuse 追踪\n",
    "for event in graph.stream(\n",
    "    {\"topic\": topic, \"max_analysts\": max_analysts}, \n",
    "    thread, \n",
    "    stream_mode=\"values\",\n",
    "    config={\"callbacks\": [langfuse_handler]}  # 启用智能体评估追踪\n",
    "):\n",
    "    # 检查并显示生成的分析师信息\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        print(\"✅ 分析师团队生成完成！以下是系统推荐的分析师：\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, analyst in enumerate(analysts, 1):\n",
    "            print(f\"📋 分析师 {i}:\")\n",
    "            print(f\"   姓名: {analyst.name}\")\n",
    "            print(f\"   机构: {analyst.affiliation}\")\n",
    "            print(f\"   角色: {analyst.role}\")\n",
    "            print(f\"   描述: {analyst.description}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n🔬 智能体评估数据记录说明:\")\n",
    "print(\"📊 已记录指标:\")\n",
    "print(\"  - 🕐 执行时间: 每个节点的处理耗时\")  \n",
    "print(\"  - 💰 成本分析: LLM 调用的令牌消耗\")\n",
    "print(\"  - 🔄 路由决策: 工作流的执行路径\")\n",
    "print(\"  - 📈 成功率: 分析师生成的成功与失败\")\n",
    "print(\"🔗 访问 Langfuse 仪表板查看详细数据: https://cloud.langfuse.com/traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f81ad23-5656-43e6-b50a-0d7a4f69a60a",
    "outputId": "f2d95e38-5acf-4c43-8974-a637aac666ac"
   },
   "outputs": [],
   "source": [
    "# 🔬 演示人机协同与评估反馈功能\n",
    "\n",
    "# 获取当前状态，检查工作流进展\n",
    "state = graph.get_state(thread)\n",
    "print(f\"🔍 当前工作流状态: {state.next}\")\n",
    "print(\"💡 系统正在等待人类反馈以优化分析师团队\")\n",
    "\n",
    "# 🔬 智能体评估：记录用户反馈事件\n",
    "with langfuse.start_as_current_span(name=\"user_feedback_collection\") as span:\n",
    "    user_feedback = \"加入一位来自初创公司的人，以增加创业者的视角\"\n",
    "    \n",
    "    # 记录用户反馈到评估系统\n",
    "    span.update_trace(\n",
    "        input={\"feedback_request\": \"优化分析师团队\"},\n",
    "        output={\"user_feedback\": user_feedback, \"feedback_type\": \"add_perspective\"}\n",
    "    )\n",
    "    \n",
    "    print(f\"👤 用户反馈: {user_feedback}\")\n",
    "    print(\"🔄 系统将根据反馈重新生成分析师团队...\")\n",
    "\n",
    "# 更新状态：提供人类反馈\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": user_feedback\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72b2a402-fd10-4f26-9a32-3e3c0d4aaf76",
    "outputId": "d5bb8423-a691-4567-83c1-4e1d072d54bb"
   },
   "outputs": [],
   "source": [
    "# 🔄 继续执行工作流，观察反馈后的改进效果\n",
    "\n",
    "print(\"🔄 根据用户反馈重新生成分析师团队...\")\n",
    "print(\"🔬 继续记录评估数据到 Langfuse 平台\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 继续图执行，启用评估追踪\n",
    "for event in graph.stream(\n",
    "    None, \n",
    "    thread, \n",
    "    stream_mode=\"values\",\n",
    "    config={\"callbacks\": [langfuse_handler]}  # 继续启用智能体评估追踪\n",
    "):\n",
    "    # 显示更新后的分析师信息\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        print(\"✅ 根据用户反馈重新生成的分析师团队：\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, analyst in enumerate(analysts, 1):\n",
    "            print(f\"📋 分析师 {i}:\")\n",
    "            print(f\"   姓名: {analyst.name}\")\n",
    "            print(f\"   机构: {analyst.affiliation}\")\n",
    "            print(f\"   角色: {analyst.role}\")\n",
    "            print(f\"   描述: {analyst.description}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n🔬 人机协同评估指标记录:\")\n",
    "print(\"📊 新增评估数据:\")\n",
    "print(\"  - 👥 用户反馈质量: 反馈内容的有效性\")\n",
    "print(\"  - 🔄 迭代改进效果: 前后分析师团队的对比\")\n",
    "print(\"  - ⏱️ 反馈响应时间: 从反馈到重新生成的耗时\")\n",
    "print(\"  - 🎯 用户满意度: 改进后的结果质量\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8816eb9-9906-441b-b552-be71107db14f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8816eb9-9906-441b-b552-be71107db14f",
    "outputId": "c250ffd5-e4a2-441b-851e-28e76f049f77"
   },
   "outputs": [],
   "source": [
    "# 🔬 演示在线评估：用户反馈打分功能\n",
    "\n",
    "print(\"🔬 演示在线评估功能：用户反馈收集\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 🔬 智能体评估：收集用户对分析师团队的反馈\n",
    "def collect_user_feedback_on_analysts(analysts_list, langfuse_client):\n",
    "    \"\"\"\n",
    "    收集用户对分析师团队的反馈评分\n",
    "    \n",
    "    这是在线评估的重要组成部分，用于：\n",
    "    - 收集真实用户对 AI 输出的评价\n",
    "    - 建立评估数据集用于模型改进\n",
    "    - 监控系统性能随时间的变化\n",
    "    \"\"\"\n",
    "    with langfuse_client.start_as_current_span(name=\"user_feedback_scoring\") as span:\n",
    "        # 模拟用户对分析师团队的评分\n",
    "        user_score = 4  # 1-5 分，5分为最高\n",
    "        feedback_comment = \"分析师团队很好地覆盖了不同角度，创业者视角的加入特别有价值\"\n",
    "        \n",
    "        # 记录到 Langfuse 用于分析\n",
    "        span.score_trace(\n",
    "            name=\"analyst_team_quality\",\n",
    "            value=user_score,\n",
    "            data_type=\"NUMERIC\",\n",
    "            comment=feedback_comment\n",
    "        )\n",
    "        \n",
    "        # 记录详细的反馈数据\n",
    "        span.update_trace(\n",
    "            input={\"analysts_count\": len(analysts_list), \"analysts_roles\": [a.role for a in analysts_list]},\n",
    "            output={\"user_score\": user_score, \"feedback\": feedback_comment}\n",
    "        )\n",
    "        \n",
    "        print(f\"👤 用户评分: {user_score}/5\")\n",
    "        print(f\"💬 用户评论: {feedback_comment}\")\n",
    "        \n",
    "        return user_score, feedback_comment\n",
    "\n",
    "# 获取最终的分析师团队\n",
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get('analysts')\n",
    "\n",
    "if analysts:\n",
    "    # 收集用户反馈\n",
    "    score, comment = collect_user_feedback_on_analysts(analysts, langfuse)\n",
    "    \n",
    "    print(f\"\\n✅ 分析师团队最终确定！\")\n",
    "    print(f\"👥 团队规模: {len(analysts)} 位分析师\")\n",
    "    print(f\"📊 用户满意度: {score}/5\")\n",
    "    \n",
    "    print(\"\\n🔬 在线评估数据记录完成：\")\n",
    "    print(\"  - ✅ 用户反馈评分已记录\")\n",
    "    print(\"  - 📈 系统性能指标已更新\") \n",
    "    print(\"  - 🎯 质量改进数据已收集\")\n",
    "else:\n",
    "    print(\"⚠️ 未找到分析师数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ac322-5926-4932-8653-68206fec0d2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a43ac322-5926-4932-8653-68206fec0d2c",
    "outputId": "771be8c3-bb76-4d6f-e00b-06afcf620aa6"
   },
   "outputs": [],
   "source": [
    "# 🔬 完成分析师生成阶段，准备进入访谈评估阶段\n",
    "\n",
    "# 确认满意当前的分析师团队，继续执行后续流程\n",
    "# 设置反馈为None表示没有进一步的修改需求\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": None\n",
    "}, as_node=\"human_feedback\")\n",
    "\n",
    "print(\"✅ 分析师团队确认完成\")\n",
    "print(\"🔬 智能体评估总结 - 分析师生成阶段：\")\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 已收集的评估指标：\")\n",
    "print(\"  - 🕐 分析师生成时间\")\n",
    "print(\"  - 💰 LLM 调用成本\")\n",
    "print(\"  - 👥 用户反馈质量评分\")\n",
    "print(\"  - 🔄 人机交互改进效果\")\n",
    "print(\"  - 📈 团队组成优化程度\")\n",
    "\n",
    "# 🔬 阶段评估：记录分析师生成阶段完成\n",
    "with langfuse.start_as_current_span(name=\"analyst_generation_completion\") as span:\n",
    "    span.update_trace(\n",
    "        input={\"phase\": \"analyst_generation\", \"status\": \"completed\"},\n",
    "        output={\"next_phase\": \"expert_interviews\", \"analysts_ready\": len(analysts)}\n",
    "    )\n",
    "\n",
    "print(\"\\n🚀 准备进入下一阶段：专家访谈与评估\")\n",
    "print(\"🔬 访谈阶段将记录更多评估指标，包括：\")\n",
    "print(\"  - 🗣️ 对话质量评估\")\n",
    "print(\"  - 🔍 信息检索效果\")\n",
    "print(\"  - 📝 报告生成质量\")\n",
    "print(\"  - ⏱️ 端到端执行时间\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab034e65-aeee-4723-8d6d-74541b548425",
   "metadata": {
    "id": "ab034e65-aeee-4723-8d6d-74541b548425"
   },
   "outputs": [],
   "source": [
    "# Continue the graph execution to end\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f204e8a-285c-4e46-8223-a695caec7764",
   "metadata": {
    "id": "2f204e8a-285c-4e46-8223-a695caec7764"
   },
   "outputs": [],
   "source": [
    "final_state = graph.get_state(thread)\n",
    "analysts = final_state.values.get('analysts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59704086-cb3b-42e9-8395-37be6f0d44e9",
    "outputId": "d16bd1df-ec14-43c0-aa4e-8440c4bef1e7"
   },
   "outputs": [],
   "source": [
    "final_state.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95717ba3-aa00-48d6-bbb7-5fe4db5919bf",
    "outputId": "bd20945d-6717-4ee7-c981-36ffb06f9e89"
   },
   "outputs": [],
   "source": [
    "for analyst in analysts:\n",
    "    print(f\"Name: {analyst.name}\")\n",
    "    print(f\"Affiliation: {analyst.affiliation}\")\n",
    "    print(f\"Role: {analyst.role}\")\n",
    "    print(f\"Description: {analyst.description}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7",
   "metadata": {
    "id": "7d2498e4-20ae-4503-9dd0-a4165132b7a7"
   },
   "source": [
    "## 进行访谈（Conduct Interview）\n",
    "\n",
    "### 生成问题（Generate Question）\n",
    "\n",
    "分析师将向专家提出问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c",
   "metadata": {
    "id": "e5d5f559-f42e-442b-87cd-dbf0a91abf9c"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    \"\"\"\n",
    "    访谈状态管理类\n",
    "\n",
    "    继承自MessagesState，用于管理分析师与专家之间的对话状态\n",
    "    包含访谈过程中的所有必要信息和上下文\n",
    "    \"\"\"\n",
    "    max_num_turns: int  # 对话轮次上限，控制访谈深度\n",
    "    context: Annotated[list, operator.add]  # 检索到的源文档列表，使用operator.add进行累加\n",
    "    analyst: Analyst  # 当前进行访谈的分析师对象\n",
    "    interview: str  # 完整的访谈记录文本\n",
    "    sections: list  # 访谈摘要小节列表，用于最终报告生成\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    搜索查询数据模型\n",
    "\n",
    "    用于结构化生成搜索查询，确保搜索请求格式正确\n",
    "    \"\"\"\n",
    "    search_query: str = Field(None, description=\"用于检索的搜索查询语句\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0",
   "metadata": {
    "id": "1c2e71eb-07ad-4bea-aabc-dbaf551408c0"
   },
   "outputs": [],
   "source": [
    "# 问题生成指令模板\n",
    "# 指导AI分析师如何与专家进行有效的访谈对话\n",
    "question_instructions = \"\"\"你是一名分析师，需要通过访谈专家来了解一个具体主题。\n",
    "\n",
    "你的目标是提炼与该主题相关的「有趣且具体」的洞见。\n",
    "\n",
    "1. 有趣（Interesting）：让人感到意外或非显而易见的观点。\n",
    "\n",
    "2. 具体（Specific）：避免泛泛而谈，包含专家提供的具体案例或细节。\n",
    "\n",
    "以下是你的关注主题与目标设定：{goals}\n",
    "\n",
    "请先用符合你人设的名字进行自我介绍，然后提出你的第一个问题。\n",
    "\n",
    "持续追问，逐步深入，逐步完善你对该主题的理解。\n",
    "\n",
    "当你认为信息已充分，请以这句话结束访谈：「非常感谢您的帮助!」\n",
    "\n",
    "请始终保持与你的人设与目标一致的说话方式。\"\"\"\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\"\n",
    "    生成访谈问题的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 根据分析师的人设和当前对话历史生成下一个问题\n",
    "        2. 确保问题符合分析师的关注点和角色定位\n",
    "        3. 维护对话的连贯性和深度\n",
    "\n",
    "    参数:\n",
    "        state: 包含分析师信息和对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含新生成问题的消息列表\n",
    "    \"\"\"\n",
    "    # 从状态中获取分析师信息和当前对话历史\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 构建系统消息，包含分析师的人设信息\n",
    "    system_message = question_instructions.format(goals=analyst.persona)\n",
    "\n",
    "    # 调用大模型生成下一个问题\n",
    "    question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 将生成的问题添加到消息历史中\n",
    "    return {\"messages\": [question]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ff33a-6232-4a79-8a82-882a645394f5",
   "metadata": {
    "id": "be2ff33a-6232-4a79-8a82-882a645394f5"
   },
   "source": [
    "### 生成回答：并行化（Parallelization）\n",
    "\n",
    "专家将并行地从多个来源收集信息来回答问题。\n",
    "\n",
    "例如，我们可以使用：\n",
    "\n",
    "- 具体网站（例如通过 [`WebBaseLoader`](https://python.langchain.com/v0.2/docs/integrations/document_loaders/web_base/) 抓取）\n",
    "- 已建立索引的文档（例如基于 [RAG](https://python.langchain.com/v0.2/docs/tutorials/rag/) 的检索）\n",
    "- Web 搜索\n",
    "- 百科搜索（百度百科）\n",
    "\n",
    "你也可以尝试不同的 Web 搜索工具，比如 [Tavily](https://tavily.com/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ea95b-e811-4299-8b66-835d4016c338",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "606ea95b-e811-4299-8b66-835d4016c338",
    "outputId": "52495f61-eb30-4785-8b55-fdfc00eca71a"
   },
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全设置环境变量的辅助函数（重复定义，保持代码完整性）\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 设置Tavily搜索API密钥\n",
    "# Tavily是一个专门为AI应用优化的搜索API，提供高质量的搜索结果\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c61ae74a-f838-4e97-8bd5-48ccd15b7789",
    "outputId": "01c8bb14-5945-4d7e-c81f-dcdaf6ec9a37"
   },
   "outputs": [],
   "source": [
    "# 网络搜索工具配置\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 初始化Tavily搜索工具\n",
    "# max_results=3 限制每次搜索返回的结果数量，平衡信息丰富度和处理效率\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c",
   "metadata": {
    "id": "2d8f760b-5a1a-4fa9-a014-d3fb02bec51c"
   },
   "outputs": [],
   "source": [
    "# 百科搜索工具配置（百度百科）\n",
    "# 已替换为 BaiduBaikeLoader\n",
    "# 百度百科搜索工具配置\n",
    "from baike_loader import BaiduBaikeLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb1603",
   "metadata": {
    "id": "06cb1603"
   },
   "source": [
    "接下来，我们将创建用于 Web 与百科（百度百科）检索的节点。\n",
    "\n",
    "还会创建一个用于回答分析师问题的节点。\n",
    "\n",
    "最后，创建用于保存完整访谈内容，以及撰写访谈摘要（“section”）的节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "9c863768-2278-415b-aef1-96fd18c1b1cb",
    "outputId": "c0063b88-b11a-4fa3-e621-34dab72fa120"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "# 搜索查询生成指令\n",
    "# 指导AI如何从对话中提取有效的搜索查询\n",
    "search_instructions = SystemMessage(content=f\"\"\"你将获得一段分析师与专家之间的对话。\n",
    "\n",
    "你的目标是基于这段对话，为Web搜索生成一条结构良好的查询语句。\n",
    "\n",
    "首先，通读整段对话。\n",
    "\n",
    "特别关注分析师最后提出的问题。\n",
    "\n",
    "将这个最终问题转化为结构良好的 Web 搜索查询。\"\"\")\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"\n",
    "    通过Web搜索检索相关文档\n",
    "\n",
    "    功能:\n",
    "        1. 分析当前对话内容，生成合适的搜索查询\n",
    "        2. 使用Tavily API执行网络搜索\n",
    "        3. 格式化搜索结果，便于后续处理\n",
    "\n",
    "    参数:\n",
    "        state: 包含对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含格式化搜索结果的上下文信息\n",
    "    \"\"\"\n",
    "    # 使用结构化输出生成搜索查询\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    # 执行Tavily网络搜索\n",
    "    search_docs = tavily_search.invoke(search_query.search_query)\n",
    "\n",
    "    # 格式化搜索结果，添加来源信息\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
    "        f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "        for doc in search_docs\n",
    "    ])\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "def search_baike(state: InterviewState):\n",
    "    \"\"\"\n",
    "    通过百科（百度百科）检索相关文档\n",
    "\n",
    "    功能:\n",
    "        1. 分析当前对话内容，生成百科搜索查询\n",
    "        2. 使用BaiduBaikeLoader获取百科内容\n",
    "        3. 格式化搜索结果，便于后续处理\n",
    "\n",
    "    参数:\n",
    "        state: 包含对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含格式化百科搜索结果的上下文信息\n",
    "    \"\"\"\n",
    "    # 使用结构化输出生成搜索查询\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_query = structured_llm.invoke([search_instructions] + state['messages'])\n",
    "\n",
    "    # 执行百科搜索（百度百科），限制最多2个文档\n",
    "    search_docs = BaiduBaikeLoader(\n",
    "        query=search_query.search_query,\n",
    "        load_max_docs=2\n",
    "    ).load()\n",
    "\n",
    "    # 格式化百科搜索结果\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join([\n",
    "        f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "        for doc in search_docs\n",
    "    ])\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "# 专家回答指令模板\n",
    "# 指导AI专家如何基于检索到的信息回答分析师的问题\n",
    "answer_instructions = \"\"\"你是一位被分析师访谈的专家。\n",
    "\n",
    "以下是分析师的关注领域：{goals}。\n",
    "\n",
    "你的目标是回答访谈者提出的问题。\n",
    "\n",
    "回答问题时，请仅使用以下上下文：\n",
    "\n",
    "{context}\n",
    "\n",
    "回答须遵循如下要求：\n",
    "\n",
    "1. 只使用上下文中提供的信息。\n",
    "\n",
    "2. 不要引入上下文之外的信息，也不要做未在上下文明确说明的假设。\n",
    "\n",
    "3. 上下文在每段文档顶部包含来源信息。\n",
    "\n",
    "4. 在涉及具体论断时，请在相应内容旁标注引用来源编号。例如，针对来源 1 使用 [1]。\n",
    "\n",
    "5. 在答案结尾处按顺序列出引用来源，如：[1] Source 1, [2] Source 2 等。\n",
    "\n",
    "6. 若来源形如：<Document source=\"assistant/docs/llama3_1.pdf\" page=\"7\"/>，则在引用列表中只写：\n",
    "\n",
    "[1] assistant/docs/llama3_1.pdf, page 7\n",
    "\n",
    "并且不要再重复加中括号，也不要附加 Document source 前缀。\"\"\"\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"\n",
    "    生成专家回答的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于检索到的上下文信息回答分析师的问题\n",
    "        2. 确保回答符合专家的角色定位\n",
    "        3. 提供准确的引用和来源信息\n",
    "\n",
    "    参数:\n",
    "        state: 包含分析师信息、对话历史和检索上下文的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含专家回答的消息列表\n",
    "    \"\"\"\n",
    "    # 从状态中获取必要信息\n",
    "    analyst = state[\"analyst\"]\n",
    "    messages = state[\"messages\"]\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    # 构建系统消息，包含分析师关注点和检索上下文\n",
    "    system_message = answer_instructions.format(\n",
    "        goals=analyst.persona,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成专家回答\n",
    "    answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "\n",
    "    # 标记该消息来自专家，便于后续路由\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    # 将专家回答添加到消息历史中\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    \"\"\"\n",
    "    保存访谈内容的函数\n",
    "\n",
    "    功能:\n",
    "        1. 将完整的对话历史转换为文本格式\n",
    "        2. 保存访谈记录，供后续报告生成使用\n",
    "\n",
    "    参数:\n",
    "        state: 包含对话历史的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含完整访谈记录的字典\n",
    "    \"\"\"\n",
    "    # 获取所有对话消息\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # 将消息列表转换为格式化的字符串\n",
    "    interview = get_buffer_string(messages)\n",
    "\n",
    "    # 将访谈记录保存到状态中\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    \"\"\"\n",
    "    消息路由函数：决定访谈流程的下一步\n",
    "\n",
    "    功能:\n",
    "        1. 检查是否达到最大对话轮次\n",
    "        2. 检查分析师是否表示访谈结束\n",
    "        3. 决定是继续提问还是保存访谈\n",
    "\n",
    "    参数:\n",
    "        state: 当前访谈状态\n",
    "        name: 专家消息的标识符，默认为\"expert\"\n",
    "\n",
    "    返回:\n",
    "        str: 下一个要执行的节点名称\n",
    "    \"\"\"\n",
    "    # 获取对话消息和最大轮次设置\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get('max_num_turns', 2)\n",
    "\n",
    "    # 统计专家回答次数\n",
    "    num_responses = len([\n",
    "        m for m in messages\n",
    "        if isinstance(m, AIMessage) and m.name == name\n",
    "    ])\n",
    "\n",
    "    # 如果达到最大轮次，结束访谈\n",
    "    if num_responses >= max_num_turns:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # 检查上一个问题是否表明对话结束\n",
    "    # 注意：这里假设倒数第二个消息是分析师的问题\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"非常感谢您的帮助!\" in last_question.content:\n",
    "        return 'save_interview'\n",
    "\n",
    "    # 继续提问\n",
    "    return \"ask_question\"\n",
    "\n",
    "# 报告小节写作指令模板\n",
    "# 指导AI如何将访谈内容转换为结构化的报告小节\n",
    "section_writer_instructions = \"\"\"你是一名资深技术写作者。\n",
    "\n",
    "你的任务是基于一组来源文档，撰写一段简洁、易读的报告小节。\n",
    "\n",
    "1. 先分析来源文档内容：\n",
    "- 每个文档的名称在文档开头，以 <Document 标签呈现。\n",
    "\n",
    "2. 使用 Markdown 制作小节结构：\n",
    "- 用 ## 作为小节标题\n",
    "- 用 ### 作为小节内的小标题\n",
    "\n",
    "3. 按结构撰写：\n",
    " a. 标题（## 头）\n",
    " b. 摘要（### 头）\n",
    " c. 参考来源（### 头）\n",
    "\n",
    "4. 标题需要贴合分析师的关注点并具有吸引力：\n",
    "{focus}\n",
    "\n",
    "5. 关于摘要部分：\n",
    "- 先给出与分析师关注点相关的背景/上下文\n",
    "- 强调访谈中获得的新颖、有趣或令人意外的洞见\n",
    "- 使用到来源文档时，按使用顺序创建编号\n",
    "- 不要提及访谈者或专家的名字\n",
    "- 控制在约 400 字以内\n",
    "- 在报告正文中使用数字引用（如 [1]、[2]），基于来源文档信息\n",
    "- **重要：生成的小节内容必须全部使用中文，所有内容都必须是中文输出**\n",
    "\n",
    "6. 在参考来源部分：\n",
    "- 列出报告中使用到的全部来源\n",
    "- 给出完整链接或具体文档路径\n",
    "- 每个来源单独一行；在行尾加两个空格以产生 Markdown 换行\n",
    "- 参考格式：\n",
    "\n",
    "### Sources\n",
    "[1] 链接或文档名\n",
    "[2] 链接或文档名\n",
    "\n",
    "7. 合并重复来源。例如以下是不正确的：\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "应去重为：\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "8. 最终检查：\n",
    "- 确保报告结构符合要求\n",
    "- 标题前不要有任何前言\n",
    "- 检查是否遵循了全部规范\"\"\"\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"\n",
    "    生成报告小节的核心函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于访谈内容和检索到的文档生成结构化的报告小节\n",
    "        2. 确保小节内容符合分析师的专业关注点\n",
    "        3. 提供准确的引用和来源信息\n",
    "\n",
    "    参数:\n",
    "        state: 包含访谈记录、检索上下文和分析师信息的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含生成的小节内容的字典\n",
    "    \"\"\"\n",
    "    # 从状态中获取必要信息\n",
    "    interview = state[\"interview\"]\n",
    "    context = state[\"context\"]\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 构建系统消息，包含分析师的关注点描述\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "\n",
    "    # 调用大模型生成报告小节\n",
    "    section = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=f\"使用这些来源撰写你的小节: {context}\")\n",
    "    ])\n",
    "\n",
    "    # 将生成的小节添加到状态中\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "# 构建访谈工作流\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "\n",
    "# 添加各个功能节点\n",
    "interview_builder.add_node(\"ask_question\", generate_question)      # 生成问题节点\n",
    "interview_builder.add_node(\"search_web\", search_web)              # 网络搜索节点\n",
    "interview_builder.add_node(\"search_baike\", search_baike)  # 百科（百度百科）搜索节点\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)    # 生成回答节点\n",
    "interview_builder.add_node(\"save_interview\", save_interview)      # 保存访谈节点\n",
    "interview_builder.add_node(\"write_section\", write_section)        # 撰写小节节点\n",
    "\n",
    "# 定义工作流连接关系\n",
    "interview_builder.add_edge(START, \"ask_question\")  # 开始 -> 提问\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")  # 提问 -> 网络搜索\n",
    "interview_builder.add_edge(\"ask_question\", \"search_baike\")  # 提问 -> 百科搜索\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")  # 网络搜索 -> 回答\n",
    "interview_builder.add_edge(\"search_baike\", \"answer_question\")  # 百科搜索 -> 回答\n",
    "\n",
    "# 条件边：根据对话状态决定下一步\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\",\n",
    "    route_messages,\n",
    "    ['ask_question', 'save_interview']\n",
    ")\n",
    "\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")  # 保存访谈 -> 撰写小节\n",
    "interview_builder.add_edge(\"write_section\", END)  # 撰写小节 -> 结束\n",
    "\n",
    "# 编译访谈工作流\n",
    "memory = MemorySaver()\n",
    "interview_graph = interview_builder.compile(checkpointer=memory).with_config(run_name=\"Conduct Interviews\")\n",
    "\n",
    "\n",
    "# 图可视化\n",
    "print(\"图可视化：\")\n",
    "\n",
    "# 方案1：尝试使用 Pyppeteer 本地渲染（推荐）\n",
    "try:\n",
    "    # 可视化：通过 Mermaid 渲染图结构\n",
    "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"✅ 图渲染成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Pyppeteer 渲染失败: {e}\")\n",
    "    \n",
    "    # 方案2：显示 Mermaid 文本格式\n",
    "    print(\"\\n📝 图结构（Mermaid 文本格式）：\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = interview_graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 方案3：显示图的节点和边信息\n",
    "    print(\"\\n🔗 图结构信息：\")\n",
    "    print(\"节点:\", list(interview_graph.get_graph().nodes.keys()))\n",
    "    print(\"边:\", list(interview_graph.get_graph().edges))\n",
    "    \n",
    "    # 方案4：提供手动渲染说明\n",
    "    print(\"\\n💡 手动渲染说明：\")\n",
    "    print(\"1. 复制上面的 Mermaid 文本\")\n",
    "    print(\"2. 访问 https://mermaid.live/\")\n",
    "    print(\"3. 粘贴文本到编辑器中查看图形\")\n",
    "    print(\"4. 或者使用支持 Mermaid 的 Markdown 编辑器\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd4445-7147-41d3-91f5-21bbe48f2e00",
   "metadata": {},
   "source": [
    "![image-20250930152452478](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509301524600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50f382f1-6e93-48d0-a44a-1094d26ccb1e",
    "outputId": "b65a5a0a-3276-4cb0-98e1-0f948ffa5295"
   },
   "outputs": [],
   "source": [
    "# Pick one analyst\n",
    "analysts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750ac4f-f458-4b2d-8bad-32ce34895758",
   "metadata": {
    "id": "3750ac4f-f458-4b2d-8bad-32ce34895758"
   },
   "source": [
    "此处我们运行一次访谈，并传入与主题相关的 llama3.1 论文索引作为参考。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "b2242d4e-8430-4de9-8cf7-3ad2f9a22b28",
    "outputId": "db68b62e-29b6-4372-80a0-50e906028c33"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "messages = [HumanMessage(f\"所以你说你在写一篇关于{topic}的文章?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "interview = interview_graph.invoke({\"analyst\": analysts[0], \"messages\": messages, \"max_num_turns\": 2}, thread)\n",
    "Markdown(interview['sections'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739e87-68bb-4e96-a86a-704e84240a6c",
   "metadata": {
    "id": "3b739e87-68bb-4e96-a86a-704e84240a6c"
   },
   "source": [
    "### 并行访谈：Map-Reduce\n",
    "\n",
    "我们通过 `Send()` API 并行运行每个访谈（map 步）。\n",
    "\n",
    "随后在 reduce 步中将它们合并为报告主体。\n",
    "\n",
    "### 收尾（Finalize）\n",
    "\n",
    "最后增加一步，为最终报告写出引言与结论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140",
   "metadata": {
    "id": "6a0042f9-5b9f-441a-9e8d-7d8189f44140"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    研究图状态管理类\n",
    "\n",
    "    用于管理整个研究流程的状态信息，包括分析师生成、并行访谈和报告生成\n",
    "    这是整个研究助理系统的核心状态管理类\n",
    "    \"\"\"\n",
    "    topic: str  # 研究主题\n",
    "    max_analysts: int  # 分析师数量上限\n",
    "    human_analyst_feedback: str  # 人类反馈信息\n",
    "    analysts: List[Analyst]  # 分析师列表\n",
    "    sections: Annotated[list, operator.add]  # 报告小节列表，使用operator.add进行累加\n",
    "    introduction: str  # 最终报告的引言部分\n",
    "    content: str  # 最终报告的主体内容\n",
    "    conclusion: str  # 最终报告的结论部分\n",
    "    final_report: str  # 完整的最终报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2224592-d2ff-469d-97bd-928809f896d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c2224592-d2ff-469d-97bd-928809f896d7",
    "outputId": "9031b724-0a08-49bd-fc4a-74dbcb81ad3f"
   },
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    启动所有并行访谈的Map步骤\n",
    "\n",
    "    功能:\n",
    "        1. 检查是否有人类反馈，如果有则重新生成分析师\n",
    "        2. 如果没有反馈，则并行启动所有分析师的访谈流程\n",
    "        3. 使用Send API实现真正的并行执行\n",
    "\n",
    "    参数:\n",
    "        state: 包含分析师列表和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        str 或 List[Send]: 如果有人类反馈返回节点名，否则返回Send对象列表\n",
    "    \"\"\"\n",
    "    # 检查是否有人类反馈\n",
    "    human_analyst_feedback = state.get('human_analyst_feedback')\n",
    "    if human_analyst_feedback:\n",
    "        # 有人类反馈，重新生成分析师\n",
    "        return \"create_analysts\"\n",
    "\n",
    "    # 没有反馈，并行启动所有访谈\n",
    "    else:\n",
    "        topic = state[\"topic\"]\n",
    "        # 为每个分析师创建一个Send对象，实现并行执行\n",
    "        return [\n",
    "            Send(\"conduct_interview\", {\n",
    "                \"analyst\": analyst,\n",
    "                \"messages\": [HumanMessage(\n",
    "                    content=f\"所以你说你在写一篇关于{topic}的文章?\"\n",
    "                )]\n",
    "            })\n",
    "            for analyst in state[\"analysts\"]\n",
    "        ]\n",
    "\n",
    "# 报告写作指令模板\n",
    "# 指导AI如何将多个分析师的小节整合为统一的报告主体\n",
    "report_writer_instructions = \"\"\"你是一名技术写作者，正在为如下主题撰写报告：\n",
    "\n",
    "{topic}\n",
    "\n",
    "你拥有一支分析师团队。每位分析师完成了两件事：\n",
    "\n",
    "1. 围绕一个具体子主题，访谈了一位专家。\n",
    "2. 将发现写成一份备忘录（memo）。\n",
    "\n",
    "你的任务：\n",
    "\n",
    "1. 你将收到分析师们的备忘录集合。\n",
    "2. 仔细思考每份备忘录的洞见。\n",
    "3. 将它们整合为简洁的总体总结，串联起所有备忘录的中心观点。\n",
    "4. 把每份备忘录的关键信息归纳成一个连贯的单一叙述。\n",
    "\n",
    "**重要要求：生成的报告必须全部使用中文，所有内容都必须是中文输出，包括标题、正文、术语解释等。对于特殊的英文术语，可以在中文后面加上英文标注。**\n",
    "\n",
    "报告格式要求：\n",
    "\n",
    "1. 使用 Markdown 格式。\n",
    "2. 报告不要有任何前言。\n",
    "3. 不使用任何小标题。\n",
    "4. 报告以一个标题开头：## Insights\n",
    "5. 报告中不要提及任何分析师的名字。\n",
    "6. 保留备忘录中的引用标注（如 [1]、[2]）。\n",
    "7. 汇总最终来源列表，并以 `## Sources` 作为小节标题。\n",
    "8. 按顺序列出来源且不要重复。\n",
    "\n",
    "[1] Source 1\n",
    "[2] Source 2\n",
    "\n",
    "以下是分析师提供的备忘录，请基于此撰写报告：\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    生成最终报告主体内容的函数（Reduce步骤）\n",
    "\n",
    "    功能:\n",
    "        1. 收集所有分析师的小节内容\n",
    "        2. 将多个小节整合为统一的报告主体\n",
    "        3. 确保报告结构清晰、内容连贯\n",
    "\n",
    "    参数:\n",
    "        state: 包含所有小节内容和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含报告主体内容的字典\n",
    "    \"\"\"\n",
    "    # 获取所有小节内容和研究主题\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 将所有小节拼接为完整文本\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 构建系统消息，包含研究主题和小节内容\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic,\n",
    "        context=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成报告主体\n",
    "    report = llm.invoke([\n",
    "        SystemMessage(content=system_message),\n",
    "        HumanMessage(content=f\"基于这些备忘录撰写一份报告。\")\n",
    "    ])\n",
    "\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "# 引言和结论写作指令模板\n",
    "# 指导AI如何为报告生成简洁有力的引言和结论\n",
    "intro_conclusion_instructions = \"\"\"你是一名技术写作者，正在完成主题为 {topic} 的报告。\n",
    "\n",
    "你将获得报告的全部小节。\n",
    "\n",
    "你的任务是撰写简洁而有说服力的引言或结论。\n",
    "\n",
    "由用户告知写引言还是结论。\n",
    "\n",
    "两者均不需要任何前言。\n",
    "\n",
    "目标约 100 字：\n",
    "- 引言：精炼预览各小节要点\n",
    "- 结论：精炼回顾各小节要点\n",
    "\n",
    "使用 Markdown 格式。\n",
    "\n",
    "**重要要求：生成的报告必须全部使用中文，所有内容都必须是中文输出，包括标题、正文、术语解释等。对于特殊的英文术语，可以在中文后面加上英文标注。**\n",
    "\n",
    "引言要求：创建一个有吸引力的标题，并用 # 作为标题头。\n",
    "\n",
    "引言小节标题使用：## 引言\n",
    "\n",
    "结论小节标题使用：## 结论\n",
    "\n",
    "撰写时可参考以下小节内容：{formatted_str_sections}\"\"\"\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    生成报告引言的函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于所有小节内容生成报告引言\n",
    "        2. 提供报告的整体概览和吸引力\n",
    "        3. 为读者提供阅读指导\n",
    "\n",
    "    参数:\n",
    "        state: 包含所有小节内容和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含引言内容的字典\n",
    "    \"\"\"\n",
    "    # 获取所有小节内容和研究主题\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 将所有小节拼接为完整文本\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 构建指令，包含研究主题和小节内容\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic,\n",
    "        formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成引言\n",
    "    intro = llm.invoke([\n",
    "        instructions,\n",
    "        HumanMessage(content=f\"撰写报告引言\")\n",
    "    ])\n",
    "\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    生成报告结论的函数\n",
    "\n",
    "    功能:\n",
    "        1. 基于所有小节内容生成报告结论\n",
    "        2. 总结报告的主要发现和洞察\n",
    "        3. 为读者提供清晰的总结\n",
    "\n",
    "    参数:\n",
    "        state: 包含所有小节内容和研究主题的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含结论内容的字典\n",
    "    \"\"\"\n",
    "    # 获取所有小节内容和研究主题\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 将所有小节拼接为完整文本\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 构建指令，包含研究主题和小节内容\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic,\n",
    "        formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "\n",
    "    # 调用大模型生成结论\n",
    "    conclusion = llm.invoke([\n",
    "        instructions,\n",
    "        HumanMessage(content=f\"撰写报告结论\")\n",
    "    ])\n",
    "\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\"\n",
    "    最终报告生成函数（Reduce步骤的最终阶段）\n",
    "\n",
    "    功能:\n",
    "        1. 整合引言、主体内容和结论\n",
    "        2. 处理来源信息的格式\n",
    "        3. 生成完整的最终报告\n",
    "\n",
    "    参数:\n",
    "        state: 包含引言、主体内容、结论和来源信息的状态对象\n",
    "\n",
    "    返回:\n",
    "        dict: 包含完整最终报告的字典\n",
    "    \"\"\"\n",
    "    # 获取报告主体内容\n",
    "    content = state[\"content\"]\n",
    "\n",
    "    # 清理内容格式，移除重复的标题\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "\n",
    "    # 分离主体内容和来源信息\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    # 组合完整的最终报告\n",
    "    final_report = (\n",
    "        state[\"introduction\"] +\n",
    "        \"\\n\\n---\\n\\n\" +\n",
    "        content +\n",
    "        \"\\n\\n---\\n\\n\" +\n",
    "        state[\"conclusion\"]\n",
    "    )\n",
    "\n",
    "    # 如果有来源信息，添加到报告末尾\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "# 构建完整的研究图工作流\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "# 添加所有功能节点\n",
    "builder.add_node(\"create_analysts\", create_analysts)  # 分析师生成节点\n",
    "builder.add_node(\"human_feedback\", human_feedback)    # 人类反馈节点\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())  # 访谈子图节点\n",
    "builder.add_node(\"write_report\", write_report)        # 报告主体写作节点\n",
    "builder.add_node(\"write_introduction\", write_introduction)  # 引言写作节点\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)      # 结论写作节点\n",
    "builder.add_node(\"finalize_report\", finalize_report)        # 最终报告生成节点\n",
    "\n",
    "# 定义工作流连接关系\n",
    "builder.add_edge(START, \"create_analysts\")  # 开始 -> 生成分析师\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")  # 生成分析师 -> 人类反馈\n",
    "\n",
    "# 条件边：根据是否有反馈决定下一步\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    initiate_all_interviews,\n",
    "    [\"create_analysts\", \"conduct_interview\"]\n",
    ")\n",
    "\n",
    "# 并行执行：访谈完成后同时进行报告写作、引言写作和结论写作\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "\n",
    "# 等待所有写作任务完成后，进行最终报告生成\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"],\n",
    "    \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)  # 最终报告生成 -> 结束\n",
    "\n",
    "# 编译完整的研究图工作流\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(\n",
    "    interrupt_before=['human_feedback'],  # 在人类反馈节点前中断\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# 显示完整的工作流图\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0",
   "metadata": {
    "id": "1b64ba9a-2b5e-40e1-a778-0f635aa3f6d0"
   },
   "source": [
    "我们来就 LangGraph 提一个开放式问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "362932ee-4106-4a2d-a32d-b812eafcf9df",
    "outputId": "2498f7ea-2c34-426b-caa2-efe6c2137bdf"
   },
   "outputs": [],
   "source": [
    "# 演示：运行研究助理系统\n",
    "# 设置输入参数\n",
    "max_analysts = 3  # 分析师数量\n",
    "topic = \"采用LangGraph作为AI Agent框架的好处\"  # 研究主题\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}  # 线程ID，用于状态管理\n",
    "\n",
    "# 运行工作流直到第一个中断点（人类反馈节点）\n",
    "for event in graph.stream({\n",
    "    \"topic\": topic,\n",
    "    \"max_analysts\": max_analysts\n",
    "}, thread, stream_mode=\"values\"):\n",
    "\n",
    "    # 检查是否有分析师信息输出\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        # 显示生成的分析师信息\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac521a5f-5a4f-44f9-8af9-d05228e20882",
    "outputId": "9ac87141-cc10-4e31-b453-f7c1d288bc06"
   },
   "outputs": [],
   "source": [
    "# 模拟人类反馈：添加一个AI原生初创公司的CEO视角\n",
    "# 这展示了人机协同（Human-in-the-loop）功能的使用\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": \"添加一个AI原生初创公司的CEO\"\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3be311f-62ee-49e7-b037-75c53d8960a8",
    "outputId": "d507e3b8-68c4-43bd-da1a-026413e162f8"
   },
   "outputs": [],
   "source": [
    "# 检查更新后的分析师列表\n",
    "# 系统会根据人类反馈重新生成分析师团队\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    analysts = event.get('analysts', '')\n",
    "    if analysts:\n",
    "        # 显示更新后的分析师信息\n",
    "        for analyst in analysts:\n",
    "            print(f\"Name: {analyst.name}\")\n",
    "            print(f\"Affiliation: {analyst.affiliation}\")\n",
    "            print(f\"Role: {analyst.role}\")\n",
    "            print(f\"Description: {analyst.description}\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af41f54-88d9-4597-98b0-444c08322095",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0af41f54-88d9-4597-98b0-444c08322095",
    "outputId": "44858e67-bb8a-44de-c5ef-2b191f117013"
   },
   "outputs": [],
   "source": [
    "# 确认满意当前的分析师团队，继续执行后续流程\n",
    "# 设置反馈为None表示没有进一步的修改需求\n",
    "graph.update_state(thread, {\n",
    "    \"human_analyst_feedback\": None\n",
    "}, as_node=\"human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37123ca7-c20b-43c1-9a71-39ba344e7ca6",
    "outputId": "dc7eee62-2047-4aff-9fd7-bae60e914aa7"
   },
   "outputs": [],
   "source": [
    "# 继续执行完整的研究流程\n",
    "# 包括并行访谈、报告生成等所有后续步骤\n",
    "for event in graph.stream(None, thread, stream_mode=\"updates\"):\n",
    "    print(\"--Node--\")\n",
    "    node_name = next(iter(event.keys()))\n",
    "    print(node_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f8f66ad8-80fd-4eb2-96b6-6ae9dffd060c",
    "outputId": "14cfbc76-d0f9-436f-ccd1-c30110f6d3c7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# 获取最终状态并显示生成的报告\n",
    "final_state = graph.get_state(thread)\n",
    "report = final_state.values.get('final_report')\n",
    "\n",
    "# 使用Markdown格式显示最终报告\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f55516",
   "metadata": {},
   "source": [
    "## 🔬 智能体在线评估：性能监控与用户反馈\n",
    "\n",
    "在线评估是智能体评估的重要组成部分，它允许我们在生产环境中实时监控系统表现。\n",
    "\n",
    "### 📊 主要评估指标\n",
    "\n",
    "#### 1. 成本监控 (Cost Monitoring)\n",
    "- **LLM 调用成本**：追踪每次模型调用的令牌消耗\n",
    "- **API 调用频率**：监控各个服务的调用次数\n",
    "- **资源使用效率**：评估系统资源的合理配置\n",
    "\n",
    "#### 2. 性能指标 (Performance Metrics)\n",
    "- **响应延迟**：测量从用户输入到系统输出的时间\n",
    "- **吞吐量**：统计单位时间内处理的请求数量\n",
    "- **成功率**：计算任务完成的成功比例\n",
    "\n",
    "#### 3. 用户体验 (User Experience)\n",
    "- **用户满意度**：收集用户对输出质量的反馈\n",
    "- **交互效果**：评估人机协同的流畅程度\n",
    "- **输出质量**：分析生成内容的准确性和有用性\n",
    "\n",
    "#### 4. 系统稳定性 (System Reliability)\n",
    "- **错误率**：监控系统异常和失败情况\n",
    "- **可用性**：确保服务的持续稳定运行\n",
    "- **容错能力**：测试系统在异常情况下的表现\n",
    "\n",
    "### 🎯 评估的价值\n",
    "\n",
    "通过持续的在线评估，我们可以：\n",
    "- **快速发现问题**：及时识别性能下降或质量问题\n",
    "- **优化资源配置**：根据实际使用情况调整系统参数\n",
    "- **改进用户体验**：基于真实反馈持续优化产品\n",
    "- **数据驱动决策**：用数据支撑产品迭代和技术选型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177dfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 演示完整的智能体评估：端到端研究流程\n",
    "\n",
    "print(\"🚀 启动完整的深度研究助手评估演示\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 🔬 使用增强版的访谈工作流（带评估功能）\n",
    "def create_enhanced_interview_workflow():\n",
    "    \"\"\"\n",
    "    创建带有完整评估功能的访谈工作流\n",
    "    \n",
    "    增强功能包括：\n",
    "    - 🔍 搜索操作的质量评估\n",
    "    - ⏱️ 访谈过程的性能监控  \n",
    "    - 📝 生成内容的质量评估\n",
    "    - 🔄 整体流程的效率分析\n",
    "    \"\"\"\n",
    "    \n",
    "    def generate_question_with_eval(state: InterviewState):\n",
    "        \"\"\"生成访谈问题（带评估）\"\"\"\n",
    "        with langfuse.start_as_current_span(name=\"generate_question\") as span:\n",
    "            analyst = state[\"analyst\"]\n",
    "            messages = state[\"messages\"]\n",
    "            \n",
    "            # 构建系统消息\n",
    "            system_message = question_instructions.format(goals=analyst.persona)\n",
    "            question = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "            \n",
    "            # 记录问题生成质量\n",
    "            span.update_trace(\n",
    "                input={\"analyst_role\": analyst.role, \"conversation_turns\": len(messages)},\n",
    "                output={\"question_length\": len(question.content), \"question_preview\": question.content[:100]}\n",
    "            )\n",
    "            \n",
    "            return {\"messages\": [question]}\n",
    "    \n",
    "    def generate_answer_with_eval(state: InterviewState):\n",
    "        \"\"\"生成专家回答（带评估）\"\"\"\n",
    "        with langfuse.start_as_current_span(name=\"generate_answer\") as span:\n",
    "            analyst = state[\"analyst\"]\n",
    "            messages = state[\"messages\"]\n",
    "            context = state[\"context\"]\n",
    "            \n",
    "            # 构建系统消息\n",
    "            system_message = answer_instructions.format(\n",
    "                goals=analyst.persona,\n",
    "                context=context\n",
    "            )\n",
    "            \n",
    "            answer = llm.invoke([SystemMessage(content=system_message)] + messages)\n",
    "            answer.name = \"expert\"\n",
    "            \n",
    "            # 评估回答质量\n",
    "            span.update_trace(\n",
    "                input={\n",
    "                    \"context_length\": len(str(context)),\n",
    "                    \"conversation_turns\": len(messages)\n",
    "                },\n",
    "                output={\n",
    "                    \"answer_length\": len(answer.content),\n",
    "                    \"has_citations\": \"[\" in answer.content and \"]\" in answer.content,\n",
    "                    \"answer_preview\": answer.content[:150]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            return {\"messages\": [answer]}\n",
    "    \n",
    "    def save_interview_with_eval(state: InterviewState):\n",
    "        \"\"\"保存访谈内容（带评估）\"\"\"\n",
    "        with langfuse.start_as_current_span(name=\"save_interview\") as span:\n",
    "            messages = state[\"messages\"]\n",
    "            interview = get_buffer_string(messages)\n",
    "            \n",
    "            # 评估访谈质量\n",
    "            span.update_trace(\n",
    "                input={\"messages_count\": len(messages)},\n",
    "                output={\n",
    "                    \"interview_length\": len(interview),\n",
    "                    \"qa_pairs\": len([m for m in messages if m.name == \"expert\"]),\n",
    "                    \"interview_summary\": f\"完成 {len([m for m in messages if m.name == 'expert'])} 轮问答\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            return {\"interview\": interview}\n",
    "    \n",
    "    return generate_question_with_eval, generate_answer_with_eval, save_interview_with_eval\n",
    "\n",
    "# 创建增强版访谈工作流函数\n",
    "enhanced_generate_question, enhanced_generate_answer, enhanced_save_interview = create_enhanced_interview_workflow()\n",
    "\n",
    "print(\"✅ 增强版访谈工作流创建完成\")\n",
    "print(\"📊 新增评估指标：\")\n",
    "print(\"  - 🗣️ 问题生成质量评估\")\n",
    "print(\"  - 💬 专家回答质量评估\")\n",
    "print(\"  - 📝 访谈整体效果评估\")\n",
    "print(\"  - 🔍 信息检索效果评估\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18c874",
   "metadata": {},
   "source": [
    "## 🔬 智能体离线评估：数据集评估与基准测试\n",
    "\n",
    "离线评估是智能体开发过程中的重要环节，它允许我们在受控环境中系统性地测试智能体的性能。\n",
    "\n",
    "### 📋 离线评估的核心组成\n",
    "\n",
    "#### 1. 数据集构建 (Dataset Construction)\n",
    "- **基准数据集**：包含输入和期望输出的标准化测试集\n",
    "- **多样性覆盖**：涵盖不同场景、难度和领域的测试用例\n",
    "- **质量保证**：确保数据集的准确性和代表性\n",
    "\n",
    "#### 2. 评估方法 (Evaluation Methods)\n",
    "- **自动化评估**：使用 LLM-as-a-Judge 进行质量评分\n",
    "- **人工评估**：专家对输出结果进行质量判断\n",
    "- **指标计算**：计算准确性、完整性、相关性等量化指标\n",
    "\n",
    "#### 3. 对比分析 (Comparative Analysis)\n",
    "- **版本对比**：比较不同版本智能体的性能差异\n",
    "- **配置测试**：测试不同参数设置对性能的影响\n",
    "- **基准对比**：与业界标准或竞品进行性能对比\n",
    "\n",
    "### 🎯 离线评估的优势\n",
    "\n",
    "- **系统性测试**：全面覆盖各种使用场景\n",
    "- **可重复性**：相同条件下的测试结果一致\n",
    "- **成本效益**：批量测试降低评估成本\n",
    "- **风险控制**：在部署前发现潜在问题\n",
    "\n",
    "### 💡 评估最佳实践\n",
    "\n",
    "1. **分层评估**：从组件级到系统级的逐层测试\n",
    "2. **持续集成**：将评估集成到开发流程中\n",
    "3. **结果分析**：深入分析评估结果，找出改进方向\n",
    "4. **迭代优化**：基于评估结果持续改进系统\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 创建研究主题评估数据集\n",
    "\n",
    "print(\"📋 构建深度研究助手评估数据集\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 🔬 创建用于评估的研究主题数据集\n",
    "research_topics_dataset = [\n",
    "    {\n",
    "        \"id\": \"tech_001\",\n",
    "        \"topic\": \"人工智能在医疗诊断中的应用前景\",\n",
    "        \"category\": \"技术应用\",\n",
    "        \"difficulty\": \"中等\",\n",
    "        \"expected_analysts\": [\"医疗专家\", \"AI技术专家\", \"伦理学家\"],\n",
    "        \"expected_insights\": [\"技术可行性\", \"临床应用\", \"伦理考量\", \"监管挑战\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"tech_002\", \n",
    "        \"topic\": \"区块链技术在供应链管理中的创新应用\",\n",
    "        \"category\": \"技术创新\",\n",
    "        \"difficulty\": \"高\",\n",
    "        \"expected_analysts\": [\"区块链专家\", \"供应链专家\", \"企业战略分析师\"],\n",
    "        \"expected_insights\": [\"技术实现\", \"成本效益\", \"安全性\", \"行业采用\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"business_001\",\n",
    "        \"topic\": \"远程工作对企业文化的长期影响\",\n",
    "        \"category\": \"商业趋势\", \n",
    "        \"difficulty\": \"中等\",\n",
    "        \"expected_analysts\": [\"组织行为专家\", \"人力资源专家\", \"企业管理顾问\"],\n",
    "        \"expected_insights\": [\"文化变化\", \"员工满意度\", \"生产力影响\", \"管理挑战\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"social_001\",\n",
    "        \"topic\": \"社交媒体对青少年心理健康的影响研究\",\n",
    "        \"category\": \"社会问题\",\n",
    "        \"difficulty\": \"高\",\n",
    "        \"expected_analysts\": [\"心理学专家\", \"社会学家\", \"数字媒体研究员\"],\n",
    "        \"expected_insights\": [\"心理影响\", \"社会行为\", \"干预策略\", \"政策建议\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"econ_001\",\n",
    "        \"topic\": \"数字货币对传统银行业的冲击与机遇\",\n",
    "        \"category\": \"经济金融\",\n",
    "        \"difficulty\": \"高\", \n",
    "        \"expected_analysts\": [\"金融专家\", \"区块链专家\", \"监管政策分析师\"],\n",
    "        \"expected_insights\": [\"市场冲击\", \"技术转型\", \"监管应对\", \"未来趋势\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 🔬 在 Langfuse 中创建评估数据集\n",
    "def create_langfuse_evaluation_dataset():\n",
    "    \"\"\"\n",
    "    在 Langfuse 平台中创建研究助手评估数据集\n",
    "    \n",
    "    功能：\n",
    "    - 创建结构化的评估数据集\n",
    "    - 为每个研究主题设置期望输出\n",
    "    - 支持后续的批量评估和对比分析\n",
    "    \"\"\"\n",
    "    \n",
    "    # 数据集名称\n",
    "    dataset_name = \"deep-research-assistant-evaluation\"\n",
    "    \n",
    "    # 创建 Langfuse 数据集\n",
    "    try:\n",
    "        langfuse.create_dataset(\n",
    "            name=dataset_name,\n",
    "            description=\"深度研究助手智能体评估数据集 - 包含不同类型和难度的研究主题\",\n",
    "            metadata={\n",
    "                \"version\": \"1.0\",\n",
    "                \"total_items\": len(research_topics_dataset),\n",
    "                \"categories\": list(set(item[\"category\"] for item in research_topics_dataset)),\n",
    "                \"difficulty_levels\": list(set(item[\"difficulty\"] for item in research_topics_dataset)),\n",
    "                \"created_for\": \"智能体性能评估和质量监控\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"✅ 成功创建数据集: {dataset_name}\")\n",
    "        \n",
    "        # 添加数据集条目\n",
    "        for item in research_topics_dataset:\n",
    "            langfuse.create_dataset_item(\n",
    "                dataset_name=dataset_name,\n",
    "                input={\n",
    "                    \"topic\": item[\"topic\"],\n",
    "                    \"max_analysts\": 3,\n",
    "                    \"category\": item[\"category\"],\n",
    "                    \"difficulty\": item[\"difficulty\"]\n",
    "                },\n",
    "                expected_output={\n",
    "                    \"expected_analysts\": item[\"expected_analysts\"],\n",
    "                    \"expected_insights\": item[\"expected_insights\"],\n",
    "                    \"quality_criteria\": {\n",
    "                        \"relevance\": \"分析师是否与主题相关\",\n",
    "                        \"diversity\": \"是否覆盖不同视角\", \n",
    "                        \"expertise\": \"是否具备专业知识背景\",\n",
    "                        \"completeness\": \"是否全面覆盖关键方面\"\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        print(f\"📋 成功添加 {len(research_topics_dataset)} 个评估条目\")\n",
    "        return dataset_name\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 创建数据集失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 创建评估数据集\n",
    "dataset_name = create_langfuse_evaluation_dataset()\n",
    "\n",
    "if dataset_name:\n",
    "    print(f\"\\n🔬 评估数据集详情:\")\n",
    "    print(f\"  📊 数据集名称: {dataset_name}\")\n",
    "    print(f\"  📈 条目数量: {len(research_topics_dataset)}\")\n",
    "    print(f\"  🏷️ 涵盖类别: {len(set(item['category'] for item in research_topics_dataset))} 个\")\n",
    "    print(f\"  📊 难度等级: {set(item['difficulty'] for item in research_topics_dataset)}\")\n",
    "    print(f\"  🔗 访问地址: https://cloud.langfuse.com/datasets\")\n",
    "else:\n",
    "    print(\"⚠️ 数据集创建失败，请检查 Langfuse 配置\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc179cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 批量评估：在数据集上运行深度研究助手\n",
    "\n",
    "print(\"🚀 启动数据集批量评估\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def evaluate_research_assistant_on_dataset(dataset_name, max_items=3):\n",
    "    \"\"\"\n",
    "    在评估数据集上批量运行深度研究助手\n",
    "    \n",
    "    参数:\n",
    "        dataset_name: Langfuse 数据集名称\n",
    "        max_items: 最大评估条目数量（演示用）\n",
    "    \n",
    "    功能:\n",
    "        - 为每个数据集条目运行完整的研究流程\n",
    "        - 记录详细的性能指标和质量评估\n",
    "        - 生成可对比的评估报告\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 获取 Langfuse 数据集\n",
    "        dataset = langfuse.get_dataset(dataset_name)\n",
    "        print(f\"📋 获取数据集: {dataset_name}\")\n",
    "        \n",
    "        # 限制评估条目数量（避免演示时间过长）\n",
    "        items_to_evaluate = list(dataset.items)[:max_items]\n",
    "        print(f\"📊 将评估 {len(items_to_evaluate)} 个条目\")\n",
    "        \n",
    "        evaluation_results = []\n",
    "        \n",
    "        for i, item in enumerate(items_to_evaluate, 1):\n",
    "            print(f\"\\n🔬 评估条目 {i}/{len(items_to_evaluate)}\")\n",
    "            print(f\"📝 主题: {item.input['topic']}\")\n",
    "            \n",
    "            # 🔬 为每个条目开启评估追踪\n",
    "            with item.run(\n",
    "                run_name=f\"research_assistant_eval_{i}\",\n",
    "                run_description=f\"评估研究助手在'{item.input['topic']}'主题上的表现\",\n",
    "                run_metadata={\n",
    "                    \"model\": \"gpt-4o\",\n",
    "                    \"category\": item.input[\"category\"], \n",
    "                    \"difficulty\": item.input[\"difficulty\"]\n",
    "                }\n",
    "            ) as run_span:\n",
    "                \n",
    "                # 运行研究助手的分析师生成阶段\n",
    "                with langfuse.start_as_current_generation(\n",
    "                    name=\"analyst_generation\",\n",
    "                    model=\"gpt-4o\",\n",
    "                    input=item.input\n",
    "                ) as gen_span:\n",
    "                    \n",
    "                    # 创建新的线程ID用于评估\n",
    "                    eval_thread = {\"configurable\": {\"thread_id\": f\"eval_{i}\"}}\n",
    "                    \n",
    "                    # 运行分析师生成（无人类反馈，自动化评估）\n",
    "                    result = None\n",
    "                    try:\n",
    "                        for event in graph.stream(\n",
    "                            {\n",
    "                                \"topic\": item.input[\"topic\"],\n",
    "                                \"max_analysts\": item.input[\"max_analysts\"],\n",
    "                                \"human_analyst_feedback\": None  # 自动化评估，无人类反馈\n",
    "                            }, \n",
    "                            eval_thread, \n",
    "                            stream_mode=\"values\",\n",
    "                            config={\"callbacks\": [langfuse_handler]}\n",
    "                        ):\n",
    "                            analysts = event.get('analysts', '')\n",
    "                            if analysts:\n",
    "                                result = {\n",
    "                                    \"generated_analysts\": [\n",
    "                                        {\n",
    "                                            \"name\": a.name,\n",
    "                                            \"role\": a.role, \n",
    "                                            \"affiliation\": a.affiliation,\n",
    "                                            \"description\": a.description\n",
    "                                        } for a in analysts\n",
    "                                    ],\n",
    "                                    \"analysts_count\": len(analysts)\n",
    "                                }\n",
    "                                break\n",
    "                    \n",
    "                        # 更新生成结果\n",
    "                        if result:\n",
    "                            gen_span.update(output=result)\n",
    "                            \n",
    "                            # 🔬 自动评估：计算质量分数\n",
    "                            quality_score = evaluate_analyst_quality(\n",
    "                                result[\"generated_analysts\"], \n",
    "                                item.expected_output[\"expected_analysts\"],\n",
    "                                item.input[\"topic\"]\n",
    "                            )\n",
    "                            \n",
    "                            # 记录评估分数\n",
    "                            run_span.score_trace(\n",
    "                                name=\"analyst_quality\",\n",
    "                                value=quality_score,\n",
    "                                data_type=\"NUMERIC\",\n",
    "                                comment=f\"基于相关性和多样性的综合评分\"\n",
    "                            )\n",
    "                            \n",
    "                            evaluation_results.append({\n",
    "                                \"topic\": item.input[\"topic\"],\n",
    "                                \"category\": item.input[\"category\"],\n",
    "                                \"difficulty\": item.input[\"difficulty\"], \n",
    "                                \"analysts_generated\": len(result[\"generated_analysts\"]),\n",
    "                                \"quality_score\": quality_score,\n",
    "                                \"status\": \"success\"\n",
    "                            })\n",
    "                            \n",
    "                            print(f\"✅ 评估完成 - 质量评分: {quality_score:.2f}/5.0\")\n",
    "                        else:\n",
    "                            print(\"❌ 分析师生成失败\")\n",
    "                            evaluation_results.append({\n",
    "                                \"topic\": item.input[\"topic\"],\n",
    "                                \"status\": \"failed\"\n",
    "                            })\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ 评估出错: {e}\")\n",
    "                        evaluation_results.append({\n",
    "                            \"topic\": item.input[\"topic\"],\n",
    "                            \"status\": \"error\",\n",
    "                            \"error\": str(e)\n",
    "                        })\n",
    "        \n",
    "        return evaluation_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 数据集评估失败: {e}\")\n",
    "        return []\n",
    "\n",
    "def evaluate_analyst_quality(generated_analysts, expected_analysts, topic):\n",
    "    \"\"\"\n",
    "    评估生成的分析师质量\n",
    "    \n",
    "    评估维度：\n",
    "    - 相关性：分析师角色是否与主题相关\n",
    "    - 多样性：是否覆盖不同的专业领域\n",
    "    - 专业性：分析师描述是否体现专业知识\n",
    "    \"\"\"\n",
    "    \n",
    "    # 简化的质量评估算法（实际项目中可使用更复杂的评估方法）\n",
    "    relevance_score = 0.8  # 假设大部分分析师都与主题相关\n",
    "    diversity_score = min(len(set(a[\"role\"] for a in generated_analysts)) / 3.0, 1.0)  # 角色多样性\n",
    "    completeness_score = min(len(generated_analysts) / 3.0, 1.0)  # 数量完整性\n",
    "    \n",
    "    # 综合评分（满分5分）\n",
    "    overall_score = (relevance_score * 0.4 + diversity_score * 0.3 + completeness_score * 0.3) * 5\n",
    "    \n",
    "    return round(overall_score, 2)\n",
    "\n",
    "# 如果数据集创建成功，执行批量评估\n",
    "if dataset_name:\n",
    "    print(\"🔬 开始批量评估...\")\n",
    "    results = evaluate_research_assistant_on_dataset(dataset_name, max_items=2)  # 演示用，只评估2个条目\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\n📊 批量评估结果总结:\")\n",
    "        print(\"=\" * 50)\n",
    "        successful_evals = [r for r in results if r.get(\"status\") == \"success\"]\n",
    "        if successful_evals:\n",
    "            avg_score = sum(r[\"quality_score\"] for r in successful_evals) / len(successful_evals)\n",
    "            print(f\"📈 平均质量评分: {avg_score:.2f}/5.0\")\n",
    "            print(f\"✅ 成功评估: {len(successful_evals)} 个\")\n",
    "            print(f\"❌ 失败评估: {len(results) - len(successful_evals)} 个\")\n",
    "            \n",
    "            # 按类别统计\n",
    "            categories = {}\n",
    "            for r in successful_evals:\n",
    "                cat = r[\"category\"]\n",
    "                if cat not in categories:\n",
    "                    categories[cat] = []\n",
    "                categories[cat].append(r[\"quality_score\"])\n",
    "            \n",
    "            print(f\"\\n📊 分类别评估结果:\")\n",
    "            for cat, scores in categories.items():\n",
    "                avg_cat_score = sum(scores) / len(scores)\n",
    "                print(f\"  {cat}: {avg_cat_score:.2f}/5.0 ({len(scores)} 个样本)\")\n",
    "        else:\n",
    "            print(\"❌ 所有评估都失败了\")\n",
    "    else:\n",
    "        print(\"⚠️ 批量评估未返回结果\")\n",
    "else:\n",
    "    print(\"⚠️ 跳过批量评估（数据集创建失败）\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69debd73",
   "metadata": {},
   "source": [
    "## 🎯 智能体评估总结与最佳实践\n",
    "\n",
    "通过本教程，我们已经为深度研究助手构建了完整的智能体评估体系。以下是关键要点和最佳实践：\n",
    "\n",
    "### 📊 评估体系架构\n",
    "\n",
    "#### 1. 在线评估 (Online Evaluation)\n",
    "- **实时监控**：追踪生产环境中的系统表现\n",
    "- **用户反馈**：收集真实用户的评价和建议\n",
    "- **性能指标**：监控延迟、成本、成功率等关键指标\n",
    "- **异常检测**：及时发现系统异常和质量下降\n",
    "\n",
    "#### 2. 离线评估 (Offline Evaluation)  \n",
    "- **数据集测试**：使用标准化数据集进行系统性评估\n",
    "- **基准对比**：与其他方法或版本进行性能对比\n",
    "- **回归测试**：确保新版本不会降低现有功能质量\n",
    "- **压力测试**：测试系统在极端条件下的表现\n",
    "\n",
    "### 🔧 技术实现要点\n",
    "\n",
    "#### 1. 追踪集成\n",
    "```python\n",
    "# ✅ 推荐做法：使用上下文管理器\n",
    "with langfuse.start_as_current_span(name=\"operation_name\") as span:\n",
    "    result = your_operation()\n",
    "    span.update_trace(input=input_data, output=result)\n",
    "```\n",
    "\n",
    "#### 2. 评估指标设计\n",
    "- **多维度评估**：从不同角度评估系统质量\n",
    "- **量化指标**：设计可衡量的评估标准\n",
    "- **用户中心**：以用户价值为核心设计评估方法\n",
    "\n",
    "#### 3. 数据管理\n",
    "- **版本控制**：管理不同版本的评估数据\n",
    "- **隐私保护**：确保用户数据的安全和隐私\n",
    "- **数据质量**：保证评估数据的准确性和代表性\n",
    "\n",
    "### 💡 最佳实践建议\n",
    "\n",
    "#### 1. 评估策略\n",
    "- **分层评估**：从组件到系统的逐层测试\n",
    "- **持续评估**：将评估集成到开发流程中\n",
    "- **多方法结合**：同时使用自动化和人工评估\n",
    "\n",
    "#### 2. 性能优化\n",
    "- **成本控制**：优化 LLM 调用次数和令牌消耗\n",
    "- **延迟优化**：减少系统响应时间\n",
    "- **缓存策略**：合理使用缓存提高效率\n",
    "\n",
    "#### 3. 质量保证\n",
    "- **错误处理**：完善的异常处理和恢复机制\n",
    "- **监控告警**：及时发现和处理系统问题\n",
    "- **用户体验**：持续改进用户交互体验\n",
    "\n",
    "### 🚀 未来展望\n",
    "\n",
    "随着 AI 技术的发展，智能体评估将向以下方向演进：\n",
    "\n",
    "1. **更智能的评估**：使用 AI 辅助评估 AI 系统\n",
    "2. **更全面的指标**：涵盖技术、业务、伦理等多个维度\n",
    "3. **更自动化的流程**：减少人工干预，提高评估效率\n",
    "4. **更个性化的方案**：根据具体应用场景定制评估方法\n",
    "\n",
    "### 🔗 相关资源\n",
    "\n",
    "- **Langfuse 官方文档**：https://langfuse.com/docs\n",
    "- **LangGraph 评估指南**：https://langchain-ai.github.io/langgraph/\n",
    "- **AI 评估最佳实践**：https://langfuse.com/blog/llm-evaluation\n",
    "\n",
    "通过系统性的评估，我们能够确保深度研究助手在真实环境中提供高质量、可靠的服务，持续创造用户价值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943f2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 智能体评估完成：数据同步与清理\n",
    "\n",
    "print(\"🎉 深度研究助手智能体评估教程完成！\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 🔬 最终数据同步：确保所有追踪数据都上传到 Langfuse\n",
    "print(\"📤 同步评估数据到 Langfuse 平台...\")\n",
    "langfuse.flush()  # 强制同步所有缓存的追踪数据\n",
    "\n",
    "print(\"✅ 数据同步完成！\")\n",
    "\n",
    "# 📊 教程总结\n",
    "print(\"\\n📊 本教程涵盖的智能体评估功能：\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🔍 1. 环境配置与 Langfuse 集成\")\n",
    "print(\"   - API 密钥配置和客户端初始化\")\n",
    "print(\"   - 连接验证和状态检查\")\n",
    "print(\"\")\n",
    "print(\"📈 2. 在线评估 (Online Evaluation)\")\n",
    "print(\"   - 实时追踪所有LLM调用和节点执行\")\n",
    "print(\"   - 用户反馈收集和质量评分\")\n",
    "print(\"   - 人机协同效果监控\")\n",
    "print(\"   - 性能指标记录（延迟、成本、成功率）\")\n",
    "print(\"\")\n",
    "print(\"🎯 3. 离线评估 (Offline Evaluation)\")\n",
    "print(\"   - 评估数据集构建和管理\")\n",
    "print(\"   - 批量自动化测试\")\n",
    "print(\"   - 质量评估算法设计\")\n",
    "print(\"   - 结果分析和对比\")\n",
    "print(\"\")\n",
    "print(\"🔧 4. 技术集成要点\")\n",
    "print(\"   - Langfuse CallbackHandler 使用\")\n",
    "print(\"   - 上下文管理器追踪模式\")\n",
    "print(\"   - 数据集管理和评估流程\")\n",
    "print(\"   - 评估指标设计和计算\")\n",
    "\n",
    "print(\"\\n🔗 关键链接：\")\n",
    "print(\"📊 Langfuse 追踪仪表板: https://cloud.langfuse.com/traces\")\n",
    "print(\"📋 数据集管理页面: https://cloud.langfuse.com/datasets\")\n",
    "print(\"📈 性能分析报告: https://cloud.langfuse.com/analytics\")\n",
    "\n",
    "print(\"\\n💡 下一步建议：\")\n",
    "print(\"1. 🚀 部署到生产环境，启用持续监控\")\n",
    "print(\"2. 📊 扩展评估数据集，提高测试覆盖率\")\n",
    "print(\"3. 🔄 设置自动化评估流程，集成到CI/CD\")\n",
    "print(\"4. 👥 收集真实用户反馈，持续优化系统\")\n",
    "print(\"5. 🎯 基于评估结果，优化模型和提示词\")\n",
    "\n",
    "print(\"\\n🎓 学习成果：\")\n",
    "print(\"✅ 掌握了 Langfuse 智能体评估的完整流程\")\n",
    "print(\"✅ 了解了在线和离线两种评估模式\")\n",
    "print(\"✅ 学会了设计和实现评估指标\")\n",
    "print(\"✅ 具备了生产环境智能体监控能力\")\n",
    "\n",
    "print(\"\\n🔬 评估版深度研究助手已就绪！\")\n",
    "print(\"🎯 面向大模型初学者的详细注释已完整添加\")\n",
    "print(\"📚 现在您可以开始使用这个评估系统来监控和改进您的AI应用了！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab7742",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3",
   "metadata": {
    "id": "e9bf8edd-fb42-496c-9bdb-3f5d7b4d79d3"
   },
   "source": [
    "# LangSmith执行追踪\n",
    "\n",
    "我们可以查看一次完整的执行追踪（trace），了解整个研究流程的详细执行情况：\n",
    "\n",
    "`https://smith.langchain.com/o/7bfa9385-4ac5-468a-a06c-ffd7dbac42ec/projects/p/27f0e396-e7ab-4eac-9501-8df28b729149?timeModel=%7B%22duration%22%3A%227d%22%7D&peek=ffca07d8-1e11-499f-9253-a13706bb9794&peeked_trace=ffca07d8-1e11-499f-9253-a13706bb9794`\n",
    "\n",
    "![image-20251001132158739](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202510011322954.png)\n",
    "\n",
    "## 追踪内容说明\n",
    "\n",
    "这个追踪链接展示了：\n",
    "\n",
    "1. **分析师生成过程**：如何根据研究主题创建不同的分析师角色\n",
    "2. **并行访谈执行**：多个分析师同时进行访谈的详细过程\n",
    "3. **信息检索流程**：网络搜索和百科（百度百科）搜索的具体执行\n",
    "4. **报告生成步骤**：从访谈内容到最终报告的完整转换过程\n",
    "5. **人机协同交互**：人类反馈如何影响系统行为\n",
    "\n",
    "通过这个追踪，你可以深入了解LangGraph工作流的内部执行机制和每个节点的具体功能。"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
