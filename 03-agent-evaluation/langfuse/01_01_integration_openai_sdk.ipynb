{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1869.89 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 787.00 GB, Free: 1125.37 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki7E44X5ViQB"
   },
   "source": [
    "# OpenAI SDK集成Langfuse获得完整的可观测性\n",
    "---\n",
    "\n",
    "## 📚 什么是大模型可观测性？\n",
    "\n",
    "**可观测性（Observability）** 是监控和调试大语言模型应用的关键技术。想象一下，当你的AI应用在生产环境中运行时，你需要知道：\n",
    "\n",
    "- 🤔 **模型回答了什么？** - 查看每次对话的完整内容\n",
    "- ⏱️ **响应速度如何？** - 监控延迟和性能指标  \n",
    "- 💰 **花费了多少？** - 跟踪Token使用和成本\n",
    "- 🐛 **哪里出错了？** - 快速定位和解决问题\n",
    "- 📊 **效果怎么样？** - 评估模型回答质量\n",
    "\n",
    "**Langfuse** 就是一个专门为大模型应用设计的可观测性平台，它能够：\n",
    "- 自动记录所有API调用\n",
    "- 提供直观的可视化界面\n",
    "- 支持评分和评估\n",
    "- 帮助优化模型性能\n",
    "\n",
    "## 🎯 本教程将教会你什么？\n",
    "\n",
    "1. **基础集成** - 如何用几行代码集成Langfuse\n",
    "2. **多种调用方式** - 文本、图像、流式、异步调用\n",
    "3. **高级功能** - 函数调用、评分系统、链路追踪\n",
    "4. **最佳实践** - 生产环境中的使用技巧\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfMAzJYcirtK"
   },
   "source": [
    "# 🚀 示例手册：OpenAI 集成（Python）\n",
    "\n",
    "## 为什么需要这个教程？\n",
    "\n",
    "作为大模型技术初学者，你可能遇到过这些问题：\n",
    "- 不知道模型到底输出了什么\n",
    "- 无法追踪API调用的成本\n",
    "- 调试问题时找不到历史记录\n",
    "- 不知道如何评估模型效果\n",
    "\n",
    "**这个教程将彻底解决这些问题！** 通过简单的代码修改，你就能获得企业级的可观测性能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0A389k2irtK"
   },
   "source": [
    "## 📖 教程概述\n",
    "\n",
    "这是一个**零基础友好**的示例手册，演示如何在 Python 项目中集成 Langfuse 与 OpenAI。\n",
    "\n",
    "### 🔍 Langfuse 能为你做什么？\n",
    "\n",
    "**Langfuse 会记录每次模型调用的输入输出**，就像给你的AI应用装上了\"黑匣子\"：\n",
    "\n",
    "1. **📝 完整记录** - 保存每次对话的完整上下文\n",
    "2. **🔍 问题排查** - 快速定位错误和异常\n",
    "3. **📊 质量评估** - 通过评分系统评估模型效果\n",
    "4. **💰 成本控制** - 监控Token使用和API费用\n",
    "5. **📈 性能优化** - 分析响应时间和吞吐量\n",
    "\n",
    "### 🎯 学习目标\n",
    "\n",
    "完成本教程后，你将掌握：\n",
    "- ✅ 如何安装和配置Langfuse\n",
    "- ✅ 如何替换OpenAI SDK实现自动追踪\n",
    "- ✅ 如何查看和分析追踪数据\n",
    "- ✅ 如何添加自定义元数据和评分\n",
    "- ✅ 如何在生产环境中使用\n",
    "\n",
    "按照 [集成指南](https://langfuse.com/integrations/model-providers/openai-py) 将本集成添加到你的 OpenAI 项目中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq04G_FSWjF-"
   },
   "source": [
    "## 🛠️ 环境准备\n",
    "\n",
    "### 前置知识要求\n",
    "\n",
    "在开始之前，你需要了解一些基础概念：\n",
    "\n",
    "- **Python基础** - 变量、函数、类的基本使用\n",
    "- **API调用** - 了解HTTP请求和响应\n",
    "- **环境变量** - 如何安全地存储敏感信息\n",
    "- **Jupyter Notebook** - 基本的notebook操作\n",
    "\n",
    "### 系统要求\n",
    "\n",
    "- Python 3.10+\n",
    "- 稳定的网络连接\n",
    "- OpenAI API密钥\n",
    "- Langfuse账户（免费注册）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYoil3FcOIQt"
   },
   "source": [
    "### 📦 依赖包版本说明\n",
    "\n",
    "**重要版本要求：**\n",
    "- **OpenAI SDK** `>=0.27.8` - 基础功能支持\n",
    "- **OpenAI SDK** `>=1.0.0` - 异步函数和流式输出支持（推荐）\n",
    "\n",
    "**为什么需要特定版本？**\n",
    "- 旧版本可能缺少某些功能\n",
    "- 新版本有更好的性能和稳定性\n",
    "- 流式输出需要较新的SDK支持\n",
    "\n",
    "**初学者提示：** 如果你不确定当前版本，直接安装最新版本即可！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVOOiBtUPtOO",
    "outputId": "2d9a4742-d70e-4cbf-c7fe-63a25dad2bcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langfuse==3.3.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (3.3.0)\n",
      "Requirement already satisfied: openai==1.107.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (1.107.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (2.32.5)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.17.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai==1.107.0) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai==1.107.0) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai==1.107.0) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai==1.107.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai==1.107.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai==1.107.0) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==1.107.0) (3.10)\n",
      "Requirement already satisfied: certifi in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.3.0) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (6.32.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.3.0) (0.58b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests<3,>=2->langfuse==3.3.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests<3,>=2->langfuse==3.3.0) (2.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 📥 安装必要的依赖包\n",
    "# 这个命令会安装两个核心包：\n",
    "# 1. langfuse - 可观测性平台的核心库\n",
    "# 2. openai - OpenAI官方SDK\n",
    "\n",
    "%pip install langfuse==3.3.0 openai==1.107.0\n",
    "\n",
    "# 💡 初学者提示：\n",
    "# - %pip 是Jupyter Notebook的魔法命令，用于安装Python包\n",
    "# - == 指定了确切的版本号，确保环境一致性\n",
    "# - 如果安装失败，检查网络连接或尝试使用国内镜像源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K72KpSE2OiJY",
    "outputId": "e137b755-2a4e-4db8-fc97-d8da6754b1aa"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n",
      "LANGFUSE_PUBLIC_KEY:  ········\n",
      "LANGFUSE_SECRET_KEY:  ········\n",
      "LANGFUSE_HOST:  ········\n"
     ]
    }
   ],
   "source": [
    "# 🔐 环境变量配置 - 安全存储敏感信息\n",
    "# 环境变量是存储API密钥等敏感信息的最佳实践\n",
    "# 避免在代码中硬编码密钥，防止泄露\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全地设置环境变量\n",
    "    如果环境变量不存在，会提示用户输入\n",
    "    使用getpass模块隐藏输入内容，防止密码泄露\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 🤖 OpenAI API 配置\n",
    "# OpenAI API密钥：从 https://platform.openai.com/api-keys 获取\n",
    "# 这是调用GPT模型必需的认证信息\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# API代理地址：如果你使用第三方代理服务（如国内代理）\n",
    "# 示例：https://api.apiyi.com/v1\n",
    "# 如果直接使用OpenAI官方API，可以留空\n",
    "_set_env(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 🌐 Langfuse 配置\n",
    "# Langfuse是一个可观测性平台，需要注册账户获取密钥\n",
    "# 注册地址：https://cloud.langfuse.com\n",
    "\n",
    "# 公开密钥：用于标识你的项目\n",
    "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
    "\n",
    "# 秘密密钥：用于认证，请妥善保管\n",
    "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "# 服务器地址：选择离你最近的区域\n",
    "# 🇪🇺 欧盟区域(推荐) https://cloud.langfuse.com\n",
    "# 🇺🇸 美国区域 https://us.cloud.langfuse.com\n",
    "_set_env(\"LANGFUSE_HOST\")\n",
    "\n",
    "# 💡 初学者提示：\n",
    "# 1. 环境变量存储在操作系统中，重启后需要重新设置\n",
    "# 2. 生产环境中建议使用.env文件或云服务配置\n",
    "# 3. 永远不要在代码中硬编码API密钥！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ldSEJ0bAP4sj"
   },
   "outputs": [],
   "source": [
    "# 🎯 核心集成：替换OpenAI SDK\n",
    "# 这是整个教程的核心！只需要一行代码的修改\n",
    "# 只需替换 import 语句，就能用 Langfuse 版本的 OpenAI SDK 获得完整的可观测性\n",
    "\n",
    "# 原来的导入方式：\n",
    "# from openai import OpenAI\n",
    "\n",
    "# 新的导入方式（自动集成Langfuse）：\n",
    "from langfuse.openai import openai\n",
    "\n",
    "# 🚀 魔法就在这里！\n",
    "# Langfuse提供了对原生OpenAI SDK的完全兼容封装\n",
    "# 这意味着：\n",
    "# 1. ✅ 你的现有代码无需修改\n",
    "# 2. ✅ 自动记录所有API调用\n",
    "# 3. ✅ 保持完全相同的接口\n",
    "# 4. ✅ 零学习成本\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# 想象一下，Langfuse就像一个\"透明的中间层\"\n",
    "# 它拦截你的OpenAI调用，记录数据，然后转发给真正的OpenAI API\n",
    "# 对你来说，使用方式完全一样，但获得了强大的可观测性能力\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ovnAAdbaLmD"
   },
   "source": [
    "## 🎯 实战示例\n",
    "\n",
    "现在让我们通过具体的代码示例来学习如何使用Langfuse！\n",
    "\n",
    "### 📝 示例1：文本聊天补全\n",
    "\n",
    "**什么是聊天补全？**\n",
    "聊天补全是GPT模型的核心功能，它根据你提供的对话历史，生成下一个回复。就像和真人聊天一样！\n",
    "\n",
    "**应用场景：**\n",
    "- 智能客服\n",
    "- 写作助手  \n",
    "- 代码生成\n",
    "- 问答系统\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c8RhokKUP9I0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "计算结果: 2\n"
     ]
    }
   ],
   "source": [
    "# 🧮 简单计算器示例\n",
    "# 这个例子展示如何让GPT扮演一个精确的计算器\n",
    "\n",
    "# 发起聊天补全请求\n",
    "completion = openai.chat.completions.create(\n",
    "  # 📝 追踪信息\n",
    "  name=\"calculator-demo\",  # 给这次调用起个名字，方便在Langfuse中查找\n",
    "\n",
    "  # 🤖 模型选择\n",
    "  model=\"gpt-4o\",  # 使用GPT-4o模型，你也可以选择gpt-3.5-turbo等\n",
    "\n",
    "  # 💬 对话内容\n",
    "  messages=[\n",
    "      # system消息：定义AI的角色和行为\n",
    "      {\"role\": \"system\", \"content\": \"您是一个非常精确的计算器。您只输出计算结果。\"},\n",
    "\n",
    "      # user消息：用户的实际问题\n",
    "      {\"role\": \"user\", \"content\": \"1 + 1 = \"}\n",
    "  ],\n",
    "\n",
    "  # 🌡️ 温度控制\n",
    "  temperature=0,  # 0表示最稳定，适合数学计算等需要准确答案的场景\n",
    "\n",
    "  # 🏷️ 自定义元数据\n",
    "  metadata={\n",
    "      \"task_type\": \"calculator\",  # 任务类型\n",
    "      \"difficulty\": \"easy\",       # 难度等级\n",
    "      \"user_id\": \"demo_user\"      # 用户ID\n",
    "  }\n",
    ")\n",
    "\n",
    "# 📊 获取模型响应\n",
    "# completion.choices[0] 获取第一个（通常也是唯一的）回复\n",
    "# .message.content 获取回复的文本内容\n",
    "result = completion.choices[0].message.content\n",
    "print(f\"计算结果: {result}\")\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# 1. messages是一个对话历史列表，按时间顺序排列\n",
    "# 2. system消息设置AI的角色，user消息是用户输入\n",
    "# 3. temperature控制输出的随机性（0-2，越高越随机）\n",
    "# 4. metadata可以存储任何自定义信息，用于分析和调试\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAqxBgOqKTzO"
   },
   "source": [
    "### 🖼️ 示例2：图像聊天补全\n",
    "\n",
    "**什么是图像聊天补全？**\n",
    "GPT-4o等模型不仅能理解文字，还能\"看懂\"图片！这让AI可以：\n",
    "- 描述图片内容\n",
    "- 回答关于图片的问题\n",
    "- 分析图片中的信息\n",
    "- 进行图像相关的创作\n",
    "\n",
    "**应用场景：**\n",
    "- 图像内容审核\n",
    "- 视觉问答系统\n",
    "- 图像标注和描述\n",
    "- 多模态AI助手\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_sM_Pe0YIfTT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像分析结果: 这幅画呈现出一个充满未来感和科技感的场景。画面的主色调是深绿色，给人一种神秘和超现实的感觉。画面中央有一个明亮的光束，直冲天际，形成了一个强烈的视觉焦点。这个光束的顶部有一个字母组合“LANGFUSE”，以发光的形式展示，似乎在传达某种重要的信息或品牌标识。\n",
      "\n",
      "在光束的周围，背景中可以看到垂直的线条和点状的元素，仿佛是数据流或数字雨，营造出一种网络或虚拟现实的氛围。画面的底部有一些模糊的轮廓，可能是城市的轮廓或科技结构，进一步增强了未来世界的感觉。\n",
      "\n",
      "整体来看，这幅画通过其色彩、光线和构图，传达了一种科技与虚拟现实交织的主题，给人一种探索未知领域的感觉。\n"
     ]
    }
   ],
   "source": [
    "# 🖼️ 图像分析示例\n",
    "# 这个例子展示如何让GPT分析一张图片\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  # 📝 追踪信息\n",
    "  name=\"image-analysis-demo\",  # 给这次图像分析起个名字\n",
    "\n",
    "  # 🤖 模型选择（支持视觉的模型）\n",
    "  model=\"gpt-4o-mini\",  # 支持视觉的模型：GPT-4o、GPT-4o mini、GPT-4 Turbo\n",
    "\n",
    "  # 💬 多模态对话内容\n",
    "  messages=[\n",
    "      # system消息：定义AI在图像分析中的角色\n",
    "      {\"role\": \"system\", \"content\": \"您是一个被训练来描述和解释图像的AI。描述图像中的主要物体和动作。\"},\n",
    "\n",
    "      # user消息：包含文字和图片的复合内容\n",
    "      {\"role\": \"user\", \"content\": [\n",
    "        # 文字部分：给AI的指令\n",
    "        {\"type\": \"text\", \"text\": \"这幅画描绘了什么？请详细描述。\"},\n",
    "\n",
    "        # 图片部分：要分析的图片\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://static.langfuse.com/langfuse-dev/langfuse-example-image.jpeg\"\n",
    "          },\n",
    "        },\n",
    "      ]}\n",
    "  ],\n",
    "\n",
    "  # 🌡️ 温度控制\n",
    "  temperature=0,  # 图像描述需要准确性，使用较低温度\n",
    "\n",
    "  # 🏷️ 自定义元数据\n",
    "  metadata={\n",
    "      \"task_type\": \"image_analysis\",\n",
    "      \"image_source\": \"langfuse_example\",\n",
    "      \"analysis_type\": \"description\"\n",
    "  }\n",
    ")\n",
    "\n",
    "# 📊 获取分析结果\n",
    "analysis_result = completion.choices[0].message.content\n",
    "print(f\"图像分析结果: {analysis_result}\")\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# 1. 多模态消息：user消息可以同时包含文字和图片\n",
    "# 2. 图片格式：支持URL链接或base64编码\n",
    "# 3. 视觉模型：只有特定模型支持图像理解\n",
    "# 4. Langfuse记录：会自动保存图片URL，方便后续查看\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4iJpqYQirtM"
   },
   "source": [
    "## 🔍 查看追踪结果\n",
    "\n",
    "现在你可以前往 [Langfuse控制台](https://cloud.langfuse.com) 查看刚才的API调用记录！\n",
    "\n",
    "**在Langfuse中你可以看到：**\n",
    "- 📊 **完整的对话历史** - 包括system和user消息\n",
    "- ⏱️ **响应时间** - 了解API性能\n",
    "- 💰 **Token使用量** - 监控成本\n",
    "- 🏷️ **自定义元数据** - 你添加的标签和分类\n",
    "- 🖼️ **图片链接** - 方便复现图像分析\n",
    "\n",
    "![Langfuse追踪界面示例](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509211139659.png)\n",
    "\n",
    "**💡 初学者提示：**\n",
    "- 每次API调用都会自动生成一个\"Trace\"（追踪记录）\n",
    "- 你可以通过name字段快速找到特定的调用\n",
    "- 元数据帮助你分类和筛选不同的调用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFYWEbD6IfTU"
   },
   "source": [
    "### 🌊 示例3：流式聊天补全\n",
    "\n",
    "**什么是流式输出？**\n",
    "流式输出让AI可以\"边想边说\"，就像真人对话一样逐字逐句地回复，而不是等待完整答案。\n",
    "\n",
    "**流式输出的优势：**\n",
    "- ⚡ **更快的响应感知** - 用户立即看到AI开始回复\n",
    "- 🎯 **更好的用户体验** - 避免长时间等待\n",
    "- 🔄 **实时交互** - 可以中途停止或修改\n",
    "- 📱 **适合移动端** - 减少网络超时风险\n",
    "\n",
    "**应用场景：**\n",
    "- 聊天机器人\n",
    "- 长文本生成\n",
    "- 实时翻译\n",
    "- 代码生成助手\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9gRlb2rKTaA",
    "outputId": "12be69ba-0f5b-46c7-afa0-b6390a9671c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AI开始讲笑话：\n",
      "----------------------------------------\n",
      "为什么程序员总是分不清圣诞节和万圣节？\n",
      "\n",
      "因为 Oct 31 = Dec 25！\n",
      "----------------------------------------\n",
      "✅ 笑话讲完了！\n"
     ]
    }
   ],
   "source": [
    "# 🎭 流式笑话生成示例\n",
    "# 这个例子展示如何实时获取AI的回复\n",
    "\n",
    "print(\"🤖 AI开始讲笑话：\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 发起流式聊天补全请求\n",
    "completion = openai.chat.completions.create(\n",
    "  # 📝 追踪信息\n",
    "  name=\"streaming-joke-demo\",  # 给这次流式调用起个名字\n",
    "\n",
    "  # 🤖 模型选择\n",
    "  model=\"gpt-4o\",\n",
    "\n",
    "  # 💬 对话内容\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"您是一位专业的喜剧演员，擅长讲有趣的笑话。\"},\n",
    "      {\"role\": \"user\", \"content\": \"讲一个关于编程的笑话给我听。\"}\n",
    "  ],\n",
    "\n",
    "  # 🌡️ 温度控制\n",
    "  temperature=0.7,  # 稍微提高温度，让笑话更有创意\n",
    "\n",
    "  # 🏷️ 自定义元数据\n",
    "  metadata={\n",
    "      \"task_type\": \"joke_generation\",\n",
    "      \"style\": \"programming_humor\",\n",
    "      \"streaming\": True\n",
    "  },\n",
    "\n",
    "  # 🌊 开启流式模式\n",
    "  stream=True  # 这是关键！开启后API会边生成边返回\n",
    ")\n",
    "\n",
    "# 📊 处理流式响应\n",
    "# completion现在是一个生成器，会逐步返回内容\n",
    "for chunk in completion:\n",
    "    # 检查chunk中是否有新内容\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        # 实时打印内容，end=\"\"表示不换行\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"✅ 笑话讲完了！\")\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# 1. stream=True 开启流式模式\n",
    "# 2. 返回的是生成器，需要用for循环处理\n",
    "# 3. delta.content 包含新增的内容片段\n",
    "# 4. flush=True 确保内容立即显示\n",
    "# 5. Langfuse会自动记录整个流式过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2pvm0qLKg7Q"
   },
   "source": [
    "### ⚡ 示例4：异步聊天补全\n",
    "\n",
    "**什么是异步调用？**\n",
    "异步调用允许程序在等待API响应时继续执行其他任务，提高程序的并发性能。\n",
    "\n",
    "**异步的优势：**\n",
    "- 🚀 **更高的并发性** - 同时处理多个请求\n",
    "- ⏱️ **更好的资源利用** - 避免阻塞等待\n",
    "- 📈 **更高的吞吐量** - 适合高并发场景\n",
    "- 🔄 **非阻塞操作** - 不会卡住整个程序\n",
    "\n",
    "**应用场景：**\n",
    "- 批量处理大量请求\n",
    "- 微服务架构\n",
    "- 高并发Web应用\n",
    "- 实时数据处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Hggwggv_MKpV"
   },
   "outputs": [],
   "source": [
    "# ⚡ 异步客户端初始化\n",
    "# 异步客户端允许非阻塞的API调用，提高程序性能\n",
    "\n",
    "from langfuse.openai import AsyncOpenAI\n",
    "\n",
    "# 创建异步客户端实例\n",
    "# 自动复用环境变量中的Langfuse配置\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# 1. AsyncOpenAI 是异步版本的OpenAI客户端\n",
    "# 2. 使用方式与同步客户端基本相同\n",
    "# 3. 主要区别是需要使用 await 关键字\n",
    "# 4. 适合需要并发处理多个请求的场景\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZIUKD8Z3KmvQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "异步计算结果: 101\n"
     ]
    }
   ],
   "source": [
    "# 🧮 异步计算器示例\n",
    "# 这个例子展示如何使用异步客户端进行API调用\n",
    "\n",
    "# 在异步函数内调用聊天补全接口\n",
    "completion = await async_client.chat.completions.create(\n",
    "  # 📝 追踪信息\n",
    "  name=\"async-calculator-demo\",  # 为本次异步调用命名\n",
    "\n",
    "  # 🤖 模型选择\n",
    "  model=\"gpt-4o\",\n",
    "\n",
    "  # 💬 对话内容\n",
    "  messages=[\n",
    "      {\"role\": \"system\", \"content\": \"你是一个非常精确的计算器。你只输出计算结果。\"},\n",
    "      {\"role\": \"user\", \"content\": \"1 + 100 = \"}\n",
    "  ],\n",
    "\n",
    "  # 🌡️ 温度控制\n",
    "  temperature=0,\n",
    "\n",
    "  # 🏷️ 自定义元数据\n",
    "  metadata={\n",
    "      \"task_type\": \"async_calculator\",\n",
    "      \"concurrency\": \"high\",\n",
    "      \"user_id\": \"async_demo_user\"\n",
    "  }\n",
    ")\n",
    "\n",
    "# 📊 获取异步响应结果\n",
    "result = completion.choices[0].message.content\n",
    "print(f\"异步计算结果: {result}\")\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# 1. await 关键字用于等待异步操作完成\n",
    "# 2. 异步调用不会阻塞程序执行\n",
    "# 3. Langfuse自动处理异步调用的追踪\n",
    "# 4. 适合需要同时处理多个请求的场景\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVbKbya4IfTX"
   },
   "source": [
    "前往 https://cloud.langfuse.com 或你自建的实例，可以在 Langfuse 中查看生成记录。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ky7CtCNzaSrn"
   },
   "source": [
    "### 🔧 示例5：函数调用（Function Calling）\n",
    "\n",
    "**什么是函数调用？**\n",
    "函数调用让AI可以调用你定义的函数，实现更复杂的交互和结构化输出。\n",
    "\n",
    "**函数调用的优势：**\n",
    "- 🎯 **结构化输出** - 获得格式化的JSON数据\n",
    "- 🔗 **外部集成** - 调用数据库、API等外部服务\n",
    "- 📊 **数据验证** - 使用Pydantic确保数据格式正确\n",
    "- 🤖 **智能决策** - AI根据情况选择调用哪个函数\n",
    "\n",
    "**应用场景：**\n",
    "- 智能助手（查询天气、发送邮件）\n",
    "- 数据提取和转换\n",
    "- 工作流自动化\n",
    "- API接口生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJfBdHowaRgs",
    "outputId": "cda9d41d-9a70-40a2-9625-93714c1f253c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pydantic==2.11.9 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.11.9)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic==2.11.9) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic==2.11.9) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic==2.11.9) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic==2.11.9) (0.4.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 📦 安装Pydantic依赖\n",
    "# Pydantic是一个强大的数据验证库，用于定义结构化数据模型\n",
    "\n",
    "%pip install pydantic==2.11.9\n",
    "\n",
    "# 💡 初学者理解：\n",
    "# Pydantic帮助我们：\n",
    "# 1. 定义数据结构（类似数据库表结构）\n",
    "# 2. 自动验证数据格式\n",
    "# 3. 生成JSON Schema供AI使用\n",
    "# 4. 确保AI返回的数据符合预期格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gA-zGk7VYYp",
    "outputId": "10430c87-cf69-4679-ea68-0aabb687de75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tmp/ipykernel_536/414262564.py:9: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  schema = StepByStepAIResponse.schema()  # 返回 JSON Schema，供 OpenAI 函数调用使用\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 定义函数调用返回值的数据结构，让模型生成结构化的 JSON\n",
    "class StepByStepAIResponse(BaseModel):\n",
    "    title: str  # 标题：例如“装机步骤”\n",
    "    steps: List[str]  # 步骤列表：每个元素是一句描述\n",
    "\n",
    "schema = StepByStepAIResponse.schema()  # 返回 JSON Schema，供 OpenAI 函数调用使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ORtNcN4-afDC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/tmp/ipykernel_536/539520121.py:14: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  \"parameters\": StepByStepAIResponse.schema()  # Pydantic 自动生成的参数定义\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 示例：引导模型调用我们定义的函数，并返回结构化结果\n",
    "response = openai.chat.completions.create(\n",
    "    name=\"test-function\",\n",
    "    model=\"gpt-4o\",  # 支持函数调用的模型版本(有些OpenAI代理不支持函数调用，这里可以替换为DeepSeek)\n",
    "    messages=[\n",
    "       {\"role\": \"user\", \"content\": \"如何组装一台电脑\"}\n",
    "    ],\n",
    "    functions=[\n",
    "        {\n",
    "          \"name\": \"get_answer_for_user_query\",  # 函数名称，需要与业务代码保持一致\n",
    "          \"description\": \"分步骤为用户提供答案\",  # 告诉模型函数的用途\n",
    "          \"parameters\": StepByStepAIResponse.schema()  # Pydantic 自动生成的参数定义\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_answer_for_user_query\"}  # 强制模型调用指定函数\n",
    ")\n",
    "\n",
    "# Langfuse 会记录函数调用的入参与出参，便于追踪\n",
    "output = json.loads(response.choices[0].message.function_call.arguments)  # 将字符串反序列化为 Python 字典\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qurrm-Ntp24O"
   },
   "source": [
    "前往 https://cloud.langfuse.com 或你自建的实例，可以在 Langfuse 中查看生成记录。\n",
    "\n",
    "示例：https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces/8942d39f62095985bf891156bbd563b9?timestamp=2025-09-21T03:42:03.721Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hMsPXFDIfTZ"
   },
   "source": [
    "## Langfuse 功能（用户、标签、元数据、会话）\n",
    "\n",
    "你可以在 OpenAI 请求中加入额外属性，以启用更多 Langfuse 功能。Langfuse 集成会自动解析这些字段。完整功能列表见 [文档](https://langfuse.com/integrations/model-providers/openai-py#custom-trace-properties)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7srTKGjaIfTZ"
   },
   "outputs": [],
   "source": [
    "result = openai.chat.completions.create(\n",
    "    name=\"test-chat-with-attributes\",  # trace 名称，对应 Langfuse 中的 Trace.name\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"您是一个非常精确的计算器。您只输出计算结果。\"},\n",
    "        {\"role\": \"user\", \"content\": \"1 + 1 = \"}],\n",
    "    temperature=0,\n",
    "    metadata={\n",
    "        \"langfuse_session_id\": \"session_123\", # 会话 ID，用于区分不同对话/请求\n",
    "        \"langfuse_user_id\": \"user_456\", # 业务用户 ID，让你在 Langfuse 中按用户聚合\n",
    "        \"langfuse_tags\": [\"calculator\"], # trace 标签，可用于 Langfuse 控制台筛选\n",
    "        \"someMetadataKey\": \"someValue\"  # trace 元数据，适合记录业务上下文\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPfrmgbEIfTZ"
   },
   "source": [
    "示例追踪：\n",
    "\n",
    "\n",
    "![image-20250921115506459](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509211155197.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su1OaQq3rPPh"
   },
   "source": [
    "## 将多次生成归并为单个 Trace\n",
    "\n",
    "在实际应用中，往往需要多次调用 OpenAI。借助 `@observe()` 装饰器，可以把一次 API 调用中的所有 LLM 请求归入 Langfuse 中同一个 `trace`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMDVxzS1ltWU",
    "outputId": "ce14c152-20f5-48f9-ffb4-60f934e8b85c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "城市如海，繁华成浪，  \n",
      "车流人潮，鸣笛作响。  \n",
      "古老街巷，历史如梦，  \n",
      "高楼林立，现实如钢。\n",
      "\n",
      "霓虹闪烁，夜色微凉，  \n",
      "灯火通明，织就辉煌。  \n",
      "人在匆匆，步履不停，  \n",
      "梦想追逐，心中点亮。\n",
      "\n",
      "四季流转，绿意常在，  \n",
      "公园小憩，共享一隅。  \n",
      "在这胸怀无垠的城里，  \n",
      "每个人都是故事的主。\n",
      "\n",
      "飞驰的地铁，跨越时空，  \n",
      "连接着南北，贯穿东西。  \n",
      "城市如诗，写尽繁华，  \n",
      "在这里生活，是一种荣华。\n",
      "\n",
      "给岁月以眼，历史以瞬，  \n",
      "在这座城市，梦也会生根。\n"
     ]
    }
   ],
   "source": [
    "from langfuse.openai import openai\n",
    "from langfuse import observe\n",
    "\n",
    "# 【@observe 装饰器】会自动：\n",
    "# 1. 为 main 函数创建一个顶层 trace\n",
    "# 2. 捕获函数内部的所有 Langfuse/OpenAI 调用，并将它们串联为一个完整链路\n",
    "@observe()  # 装饰器会自动创建 trace 并嵌套各次生成\n",
    "def main(country: str, user_id: str, **kwargs) -> str:\n",
    "    # 嵌套调用 1：询问国家首都\n",
    "    capital = openai.chat.completions.create(\n",
    "      name=\"geography-teacher\",\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"您是一位地理老师，帮助学生学习国家的首都。当被问及时，只输出首都。\"},\n",
    "          {\"role\": \"user\", \"content\": country}],\n",
    "      temperature=0,\n",
    "    ).choices[0].message.content  # 读取模型回复\n",
    "\n",
    "    # 嵌套调用 2：请模型写一首关于首都的诗\n",
    "    poem = openai.chat.completions.create(\n",
    "      name=\"poet\",\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"你是一位诗人。创作一首关于城市的诗。\"},\n",
    "          {\"role\": \"user\", \"content\": capital}],\n",
    "      temperature=1,  # 提高温度，让诗歌更有创意\n",
    "      max_tokens=200,  # 控制输出长度\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    return poem\n",
    "\n",
    "# 直接调用 main 函数，Langfuse 会自动生成 trace 并可在控制台查看链路\n",
    "print(main(\"北京\", \"FLY\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehx2NZuIrPPh"
   },
   "source": [
    "前往 https://cloud.langfuse.com 或你自建的实例，可以在 Langfuse 中查看完整链路。\n",
    "\n",
    "![多次 OpenAI 调用的追踪图](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509211159038.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HeMqTWgK4xL"
   },
   "source": [
    "## 完整方案：与 Langfuse SDK 协同\n",
    "\n",
    "`trace` 是 Langfuse 的核心对象，你可以为它附加丰富的元数据。详见 [Python SDK 文档](https://langfuse.com/docs/sdk/python#traces-1)。\n",
    "\n",
    "自定义 trace 后可以实现以下能力：\n",
    "- 自定义名称，用来区分不同类型的链路\n",
    "- 以用户为粒度的追踪\n",
    "- 通过版本与发布信息进行实验管理\n",
    "- 保存自定义元数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28to65wpK4xL",
    "outputId": "6eec4e7c-b210-4690-a50d-ed9909f46e3e"
   },
   "outputs": [],
   "source": [
    "from langfuse.openai import openai\n",
    "from langfuse import observe, get_client\n",
    "\n",
    "langfuse = get_client()  # 获取底层 Langfuse 客户端，可在装饰器之外手动操作 trace\n",
    "\n",
    "@observe()  # 装饰器会自动创建 trace 并嵌套各次生成\n",
    "def main(country: str, user_id: str, **kwargs) -> str:\n",
    "    # 嵌套调用 1：获取国家首都\n",
    "    capital = openai.chat.completions.create(\n",
    "      name=\"geography-teacher\",\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"您是一位地理老师，帮助学生学习国家的首都。当被问及时，只输出首都。\"},\n",
    "          {\"role\": \"user\", \"content\": country}],\n",
    "      temperature=0,\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    # 嵌套调用 2：根据首都生成诗歌\n",
    "    poem = openai.chat.completions.create(\n",
    "      name=\"poet\",\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"你是一位诗人。创作一首关于城市的诗。\"},\n",
    "          {\"role\": \"user\", \"content\": capital}],\n",
    "      temperature=1,\n",
    "      max_tokens=200,\n",
    "    ).choices[0].message.content\n",
    "\n",
    "    # 手动更新当前 trace 的属性，让仪表盘信息更完整\n",
    "    langfuse.update_current_trace(\n",
    "        name=\"City poem generator\",  # 自定义 trace 名称\n",
    "        session_id=\"1234\",  # 业务会话 ID\n",
    "        user_id=user_id,  # 业务用户 ID\n",
    "        tags=[\"tag1\", \"tag2\"],  # 标签，支持在 Langfuse 中搜索\n",
    "        public=True,  # 是否允许分享 Trace 链接\n",
    "        metadata = {\"env\": \"development\"}  # 自定义元数据，例如环境标记\n",
    "    )\n",
    "\n",
    "    return poem\n",
    "\n",
    "# create_trace_id() 会生成一个可复用的追踪 ID，你也可以改为自己的业务 ID\n",
    "trace_id = langfuse.create_trace_id()\n",
    "\n",
    "# 通过关键字参数 `langfuse_observation_id` 将 trace_id 传递给装饰器，方便串联上下游调用\n",
    "print(main(\"北京\", \"admin\", langfuse_observation_id=trace_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u5n_RUic4nb"
   },
   "source": [
    "示例：\n",
    "![image-20250921120555385](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509211205931.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3jxed-VrPPi"
   },
   "source": [
    "## 以编程方式添加评分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMO6tn53rPPi"
   },
   "source": [
    "你可以向 trace 添加 [评分](https://langfuse.com/docs/scores)，记录用户反馈或自动化评估结果。评分可用于在 Langfuse 中筛选追踪，并会显示在控制台中。详见评分文档。\n",
    "\n",
    "评分通过 `trace_id` 与对应的 trace 关联。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0argbJhrPPi"
   },
   "outputs": [],
   "source": [
    "from langfuse import observe, get_client\n",
    "\n",
    "langfuse = get_client()  # 获取底层客户端，用于主流程之外的操作\n",
    "\n",
    "@observe()  # 装饰器会自动创建 trace 并嵌套各次生成\n",
    "def main():\n",
    "    # 在装饰器内部，可随时获取当前 trace 的 ID\n",
    "    trace_id = langfuse.get_current_trace_id()\n",
    "\n",
    "    # TODO: 在此处编写你的业务逻辑，例如继续调用其他 API、处理用户输入等\n",
    "\n",
    "    return \"res\", trace_id\n",
    "\n",
    "# 执行被装饰的函数，Langfuse 会生成 trace\n",
    "_, trace_id = main()\n",
    "\n",
    "# 在 trace 上下文外部也可以继续操作，例如向这次 trace 添加评分\n",
    "langfuse.create_score(\n",
    "    trace_id=trace_id,  # 指定要打分的 trace\n",
    "    name=\"my-score-name\",  # 评分名称，用于区分不同指标\n",
    "    value=1  # 分值，可以是布尔、整数、浮点数，视业务场景而定\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp899q3edni6"
   },
   "source": [
    "示例：![image-20250921120901924](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509211209271.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz7LxE7-1XSK"
   },
   "source": [
    "## 🔧 常见问题和解决方案\n",
    "\n",
    "### 问题1：无法连接到OpenAI API\n",
    "\n",
    "**症状：**\n",
    "```\n",
    "openai.APIError: Connection error\n",
    "```\n",
    "\n",
    "**解决方案：**\n",
    "1. 检查网络连接\n",
    "2. 确认API密钥正确\n",
    "3. 检查OPENAI_BASE_URL设置（如果使用代理）\n",
    "4. 检查防火墙设置\n",
    "\n",
    "### 问题2：Langfuse连接失败\n",
    "\n",
    "**症状：**\n",
    "```\n",
    "langfuse.LangfuseConnectionError\n",
    "```\n",
    "\n",
    "**解决方案：**\n",
    "1. 确认LANGFUSE_PUBLIC_KEY和LANGFUSE_SECRET_KEY正确\n",
    "2. 检查LANGFUSE_HOST地址\n",
    "3. 确认网络可以访问Langfuse服务器\n",
    "4. 检查账户是否有效\n",
    "\n",
    "### 问题3：Token使用量过高\n",
    "\n",
    "**解决方案：**\n",
    "1. 使用更便宜的模型（如gpt-3.5-turbo）\n",
    "2. 减少prompt长度\n",
    "3. 设置max_tokens限制输出长度\n",
    "4. 在Langfuse中监控使用量\n",
    "\n",
    "### 问题4：响应速度慢\n",
    "\n",
    "**解决方案：**\n",
    "1. 使用异步调用处理大量请求\n",
    "2. 选择更快的模型\n",
    "3. 减少prompt复杂度\n",
    "4. 使用流式输出提升用户体验\n",
    "\n",
    "### 问题5：找不到追踪记录\n",
    "\n",
    "**解决方案：**\n",
    "1. 确认API调用成功执行\n",
    "2. 检查Langfuse配置是否正确\n",
    "3. 等待几分钟（数据同步可能有延迟）\n",
    "4. 在Langfuse控制台使用name字段搜索\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YckyL3hr1XSK"
   },
   "source": [
    "## 🌟 生产环境最佳实践\n",
    "\n",
    "### 1. 安全性\n",
    "\n",
    "**保护API密钥：**\n",
    "- ✅ 使用环境变量存储密钥\n",
    "- ✅ 使用.env文件（不要提交到Git）\n",
    "- ✅ 定期轮换API密钥\n",
    "- ❌ 永远不要在代码中硬编码密钥\n",
    "\n",
    "**示例.env文件：**\n",
    "```bash\n",
    "OPENAI_API_KEY=sk-your-openai-key\n",
    "LANGFUSE_PUBLIC_KEY=pk-your-langfuse-public-key\n",
    "LANGFUSE_SECRET_KEY=sk-your-langfuse-secret-key\n",
    "LANGFUSE_HOST=https://cloud.langfuse.com\n",
    "```\n",
    "\n",
    "### 2. 性能优化\n",
    "\n",
    "**减少成本：**\n",
    "- 选择合适的模型（不总是最大的）\n",
    "- 设置max_tokens限制\n",
    "- 缓存重复的请求结果\n",
    "- 使用批处理处理大量数据\n",
    "\n",
    "**提升速度：**\n",
    "- 使用异步调用\n",
    "- 启用流式输出\n",
    "- 合理设置timeout参数\n",
    "\n",
    "### 3. 错误处理\n",
    "\n",
    "```python\n",
    "import openai\n",
    "from langfuse.openai import openai as langfuse_openai\n",
    "import time\n",
    "\n",
    "def robust_api_call(messages, max_retries=3):\n",
    "    \"\"\"\n",
    "    带重试机制的API调用\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = langfuse_openai.chat.completions.create(\n",
    "                name=f\"robust-call-attempt-{attempt + 1}\",\n",
    "                model=\"gpt-4o\",\n",
    "                messages=messages,\n",
    "                timeout=30  # 30秒超时\n",
    "            )\n",
    "            return response\n",
    "        except openai.RateLimitError:\n",
    "            # 遇到速率限制，等待后重试\n",
    "            wait_time = 2 ** attempt  # 指数退避\n",
    "            print(f\"速率限制，等待 {wait_time} 秒...\")\n",
    "            time.sleep(wait_time)\n",
    "        except openai.APIError as e:\n",
    "            print(f\"API错误: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"重试次数用尽\")\n",
    "```\n",
    "\n",
    "### 4. 监控和追踪\n",
    "\n",
    "**在Langfuse中设置：**\n",
    "- 使用有意义的name字段\n",
    "- 添加详细的metadata\n",
    "- 设置用户ID和会话ID\n",
    "- 使用标签分类不同类型的请求\n",
    "\n",
    "**示例：**\n",
    "```python\n",
    "response = openai.chat.completions.create(\n",
    "    name=\"production-chat-v1.0\",\n",
    "    model=\"gpt-4o\",\n",
    "    messages=messages,\n",
    "    metadata={\n",
    "        \"environment\": \"production\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"feature\": \"customer_support\",\n",
    "        \"user_type\": \"premium\"\n",
    "    }\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### 5. 测试策略\n",
    "\n",
    "**单元测试：**\n",
    "- Mock OpenAI API调用\n",
    "- 测试错误处理逻辑\n",
    "- 验证输入输出格式\n",
    "\n",
    "**集成测试：**\n",
    "- 使用测试API密钥\n",
    "- 验证Langfuse追踪\n",
    "- 测试完整的工作流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHVuMzDl2RWG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
