{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "# 🔧 环境配置和检查\n",
    "\n",
    "## 概述\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "- 使用统一的conda环境\n",
    "- 通过国内镜像源快速安装依赖\n",
    "- 加速模型下载\n",
    "- 检查系统配置\n",
    "\n",
    "## 配置步骤\n",
    "1. **Conda环境管理** - 激活统一的学习环境\n",
    "2. **包管理器优化** - 配置pip使用清华镜像源\n",
    "3. **模型下载加速** - 设置HuggingFace镜像代理\n",
    "4. **系统环境诊断** - 检查硬件和软件配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 激活conda环境\n",
    "%%script bash\n",
    "# 初始化 conda\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/AIAgent101/blob/main/06-agent-evaluation/langfuse/01_02_integration_openai_structured_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIyq5cb2GsrX"
   },
   "source": [
    "# 教程：使用 Langfuse 追踪 OpenAI 结构化输出\n",
    "\n",
    "在本教程中，您将学习如何使用 Langfuse 来监控和追踪 OpenAI 的结构化输出功能。\n",
    "\n",
    "## 什么是结构化输出？\n",
    "从非结构化输入生成结构化数据是当今人工智能的核心应用场景。结构化输出使得链式大语言模型调用、UI组件生成和基于模型的评估更加可靠。[结构化输出](https://openai.com/index/introducing-structured-outputs-in-the-api/) 是 OpenAI API 的一项新功能，它基于 JSON 模式和函数调用构建，能够在模型输出中强制执行严格的数据结构模式。\n",
    "\n",
    "**核心优势：**\n",
    "- 🎯 **可靠性**：确保模型输出严格遵循预定义的数据结构\n",
    "- 🔗 **链式调用**：使多个模型调用之间的数据传递更加稳定\n",
    "- 🎨 **UI生成**：为前端组件提供标准化的数据格式\n",
    "- 📊 **模型评估**：便于对模型输出进行自动化评估和分析\n",
    "\n",
    "## 如何在 Langfuse 中追踪结构化输出？\n",
    "如果您使用 OpenAI Python SDK，可以使用 [Langfuse 的直接替换方案](https://langfuse.com/integrations/model-providers/openai-py) 来获得完整的日志记录，只需更改导入语句即可。通过这种方式，您可以在 Langfuse 中监控 OpenAI 生成的结构化输出。\n",
    "\n",
    "**导入方式对比：**\n",
    "```diff\n",
    "- import openai                    # 原始导入方式\n",
    "+ from langfuse.openai import openai  # Langfuse 集成导入\n",
    "\n",
    "其他可选导入方式：\n",
    "+ from langfuse.openai import OpenAI, AsyncOpenAI, AzureOpenAI, AsyncAzureOpenAI\n",
    "```\n",
    "\n",
    "**重要说明：** 仅需修改导入语句，其余代码保持不变，即可自动获得完整的调用追踪和监控功能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH5klZnTTzuq"
   },
   "source": [
    "## 步骤 1：初始化 Langfuse\n",
    "使用您从 Langfuse UI 项目设置中获取的 [API 密钥](https://langfuse.com/faq/all/where-are-langfuse-api-keys) 来初始化 Langfuse 客户端，并将它们添加到您的环境变量中。\n",
    "\n",
    "**配置说明：**\n",
    "- 🔑 **API 密钥**：从 Langfuse 控制台获取公钥和私钥\n",
    "- 🌍 **服务器地址**：选择合适的区域服务器（欧盟或美国）\n",
    "- 🤖 **OpenAI 密钥**：用于调用 OpenAI 的 API 服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "n11_vChnEdrg",
    "outputId": "62fe2938-612b-4fb5-84f6-b7dad7c36c49"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langfuse==3.3.0\n",
      "  Downloading langfuse-3.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting openai==1.107.0\n",
      "  Downloading openai-1.107.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting backoff>=1.10.0 (from langfuse==3.3.0)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse==3.3.0)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.32.4)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.17.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.107.0) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.3.0) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.3.0) (0.58b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (2.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (3.23.0)\n",
      "Downloading langfuse-3.3.0-py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.107.0-py3-none-any.whl (950 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.0/951.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opentelemetry-proto, backoff, opentelemetry-exporter-otlp-proto-common, openai, opentelemetry-exporter-otlp-proto-http, langfuse\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.108.0\n",
      "    Uninstalling openai-1.108.0:\n",
      "      Successfully uninstalled openai-1.108.0\n",
      "Successfully installed backoff-2.2.1 langfuse-3.3.0 openai-1.107.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-http-1.37.0 opentelemetry-proto-1.37.0\n"
     ]
    }
   ],
   "source": [
    "# 📥 安装必要的依赖包\n",
    "# 这个命令会安装两个核心包：\n",
    "# 1. langfuse - 可观测性平台的核心库\n",
    "# 2. openai - OpenAI官方SDK\n",
    "\n",
    "%pip install langfuse==3.3.0 openai==1.107.0"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 🔐 环境变量配置 - 安全存储敏感信息\n",
    "# 环境变量是存储API密钥等敏感信息的最佳实践\n",
    "# 避免在代码中硬编码密钥，防止泄露\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全地设置环境变量\n",
    "    如果环境变量不存在，会提示用户输入\n",
    "    使用getpass模块隐藏输入内容，防止密码泄露\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 🤖 OpenAI API 配置\n",
    "# OpenAI API密钥：从 https://platform.openai.com/api-keys 获取\n",
    "# 这是调用GPT模型必需的认证信息\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# API代理地址：如果你使用第三方代理服务（如国内代理）\n",
    "# 示例：https://api.apiyi.com/v1\n",
    "# 如果直接使用OpenAI官方API，可以留空\n",
    "_set_env(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 🌐 Langfuse 配置\n",
    "# Langfuse是一个可观测性平台，需要注册账户获取密钥\n",
    "# 注册地址：https://cloud.langfuse.com\n",
    "\n",
    "# 公开密钥：用于标识你的项目\n",
    "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
    "\n",
    "# 秘密密钥：用于认证，请妥善保管\n",
    "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "# 服务器地址：选择离你最近的区域\n",
    "# 🇪🇺 欧盟区域(推荐) https://cloud.langfuse.com\n",
    "# 🇺🇸 美国区域 https://us.cloud.langfuse.com\n",
    "_set_env(\"LANGFUSE_HOST\")\n",
    "\n",
    "# 💡 初学者提示：\n",
    "# 1. 环境变量存储在操作系统中，重启后需要重新设置\n",
    "# 2. 生产环境中建议使用.env文件或云服务配置\n",
    "# 3. 永远不要在代码中硬编码API密钥！"
   ],
   "metadata": {
    "id": "HGGR722gd2yr",
    "outputId": "2fae4a09-56e0-417c-ed7c-2adbcf826188",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n",
      "LANGFUSE_PUBLIC_KEY: ··········\n",
      "LANGFUSE_SECRET_KEY: ··········\n",
      "LANGFUSE_HOST: ··········\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK8oTvWHMaTx"
   },
   "source": [
    "## 步骤 2：数学辅导示例\n",
    "\n",
    "在这个示例中，我们将构建一个数学辅导工具，它将解决数学问题的步骤输出为结构化对象数组。\n",
    "\n",
    "**应用场景：**\n",
    "- 📚 **教育应用**：为学生提供逐步解题指导\n",
    "- 🎯 **个性化学习**：用户可以按自己的节奏逐步学习解题过程\n",
    "- 🔍 **步骤追踪**：每个解题步骤都可以单独显示和分析\n",
    "- 💡 **交互式学习**：支持用户在任意步骤暂停和思考\n",
    "\n",
    "这种设置对于需要将每个步骤单独显示的应用程序非常有用，允许用户按照自己的节奏逐步学习解题过程。\n",
    "\n",
    "（示例改编自 [OpenAI cookbook](https://cookbook.openai.com/examples/structured_outputs_intro)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7Fi0mk1c_c4"
   },
   "source": [
    "**重要提示：** 虽然 OpenAI 也通过其测试版 API (`client.beta.chat.completions.parse`) 提供结构化输出解析功能，但这种方法目前不允许设置 Langfuse 特定的属性，如 `name`、`metadata`、`userId` 等。请使用下面描述的标准 `client.chat.completions.create` 方法配合 `response_format` 参数的方式。\n",
    "\n",
    "**两种方法对比：**\n",
    "- ✅ **推荐方式**：`client.chat.completions.create` + `response_format` - 支持完整的 Langfuse 追踪功能\n",
    "- ⚠️ **限制方式**：`client.beta.chat.completions.parse` - 功能受限，无法设置追踪属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8IZjc893Ef1c"
   },
   "outputs": [],
   "source": [
    "# 使用 Langfuse 的直接替换方案，仅通过更改导入即可获得完整的日志记录功能\n",
    "# 这样，您就可以在 Langfuse 中监控 OpenAI 生成的结构化输出\n",
    "from langfuse.openai import OpenAI\n",
    "import json  # 用于处理 JSON 数据的标准库\n",
    "\n",
    "# 指定要使用的 OpenAI 模型\n",
    "# gpt-4o-2024-08-06 是支持结构化输出的最新模型版本\n",
    "openai_model = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "# 创建 OpenAI 客户端实例\n",
    "# 这个客户端会自动集成 Langfuse 的追踪功能\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YV7nfCKqV-FS"
   },
   "source": [
    "在 `response_format` 参数中，您现在可以通过 `json_schema` 提供 JSON Schema。当使用 `response_format` 并设置 `strict: true` 时，模型的输出将严格遵循提供的模式。\n",
    "\n",
    "**关键概念解释：**\n",
    "- 📋 **JSON Schema**：定义数据结构的标准格式，类似于数据的\"蓝图\"\n",
    "- 🔒 **strict: true**：启用严格模式，确保输出100%符合定义的结构\n",
    "- ✅ **结构保证**：模型输出将始终包含所需的字段和数据类型\n",
    "\n",
    "函数调用保持类似的方式，但通过新的参数 `strict: true`，您现在可以确保为函数提供的模式得到严格遵循。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iKWRHCjVF55-"
   },
   "outputs": [],
   "source": [
    "# 定义数学辅导的系统提示词\n",
    "# 这个提示词告诉模型如何扮演数学老师的角色\n",
    "math_tutor_prompt = '''\n",
    "    你是一个有用的数学辅导老师。你将收到一个数学问题，\n",
    "    你的目标是输出逐步解决方案以及最终答案。\n",
    "    对于每个步骤，只需提供输出作为方程式，使用解释字段详细说明推理过程。\n",
    "'''\n",
    "\n",
    "def get_math_solution(question):\n",
    "    \"\"\"\n",
    "    获取数学问题的结构化解决方案\n",
    "\n",
    "    参数:\n",
    "        question (str): 要解决的数学问题\n",
    "\n",
    "    返回:\n",
    "        包含逐步解决方案的结构化响应\n",
    "    \"\"\"\n",
    "    # 调用 OpenAI API 创建聊天完成请求\n",
    "    # 使用结构化输出确保返回格式符合预定义的 JSON Schema\n",
    "    response = client.chat.completions.create(\n",
    "    model = openai_model,  # 指定使用的模型版本\n",
    "    messages=[\n",
    "        {\n",
    "            # 系统消息：定义AI助手的角色和行为规范\n",
    "            \"role\": \"system\",\n",
    "            \"content\": math_tutor_prompt\n",
    "        },\n",
    "        {\n",
    "            # 用户消息：包含具体的数学问题\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ],\n",
    "    # 定义响应格式 - 这是结构化输出的核心配置\n",
    "    response_format={\n",
    "        # 指定使用 JSON Schema 来定义输出结构\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            # Schema 的名称，用于标识这个特定的数据结构\n",
    "            \"name\": \"math_reasoning\",\n",
    "            # 定义具体的数据结构模式\n",
    "            \"schema\": {\n",
    "                # 根对象类型\n",
    "                \"type\": \"object\",\n",
    "                # 定义对象的属性\n",
    "                \"properties\": {\n",
    "                    # steps: 解题步骤数组\n",
    "                    \"steps\": {\n",
    "                        \"type\": \"array\",  # 数组类型\n",
    "                        # 数组中每个元素的结构\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",  # 每个步骤是一个对象\n",
    "                            \"properties\": {\n",
    "                                # 解释字段：说明这一步的推理过程\n",
    "                                \"explanation\": {\"type\": \"string\"},\n",
    "                                # 输出字段：这一步的数学表达式或结果\n",
    "                                \"output\": {\"type\": \"string\"}\n",
    "                            },\n",
    "                            # 必需字段：每个步骤都必须包含解释和输出\n",
    "                            \"required\": [\"explanation\", \"output\"],\n",
    "                            # 不允许额外属性，确保结构严格\n",
    "                            \"additionalProperties\": False\n",
    "                        }\n",
    "                    },\n",
    "                    # final_answer: 最终答案\n",
    "                    \"final_answer\": {\"type\": \"string\"}\n",
    "                },\n",
    "                # 根对象的必需字段\n",
    "                \"required\": [\"steps\", \"final_answer\"],\n",
    "                # 不允许额外属性\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            # 启用严格模式：确保输出100%符合定义的结构\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    "    )\n",
    "\n",
    "    # 返回模型生成的消息内容\n",
    "    # choices[0] 表示选择第一个（通常也是唯一的）响应\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qn9N-utqINy_",
    "outputId": "c720d83e-164c-467c-8aad-cd5adbb987b5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "原始 JSON 响应:\n",
      "{\"final_answer\":\"x = -3.75\",\"steps\":[{\"explanation\":\"首先，我们从方程两边减去7，以便将8x独立出来。\",\"output\":\"8x + 7 - 7 = -23 - 7\"},{\"explanation\":\"简化方程，我们得到：8x = -30。\",\"output\":\"8x = -30\"},{\"explanation\":\"接下来，我们将方程的两边都除以8，以便求出x的值。\",\"output\":\"x = -30 / 8\"},{\"explanation\":\"通过计算-30 ÷ 8，我们得到x的值是-3.75。\",\"output\":\"x = -3.75\"}]}\n"
     ]
    }
   ],
   "source": [
    "# 使用示例问题进行测试\n",
    "# 这是一个一元一次方程，适合演示逐步解题过程\n",
    "question = \"如何解这个方程：8x + 7 = -23\"\n",
    "\n",
    "# 调用我们定义的函数获取结构化解决方案\n",
    "result = get_math_solution(question)\n",
    "\n",
    "# 打印原始的 JSON 格式响应内容\n",
    "# 这展示了模型返回的结构化数据\n",
    "print(\"原始 JSON 响应:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAwEts-hCA73",
    "outputId": "7cad4c92-f455-4912-b3ea-b02476fb5aa0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "步骤 1: 首先，我们从方程两边减去7，以便将8x独立出来。\n",
      "\n",
      "8x + 7 - 7 = -23 - 7\n",
      "\n",
      "\n",
      "步骤 2: 简化方程，我们得到：8x = -30。\n",
      "\n",
      "8x = -30\n",
      "\n",
      "\n",
      "步骤 3: 接下来，我们将方程的两边都除以8，以便求出x的值。\n",
      "\n",
      "x = -30 / 8\n",
      "\n",
      "\n",
      "步骤 4: 通过计算-30 ÷ 8，我们得到x的值是-3.75。\n",
      "\n",
      "x = -3.75\n",
      "\n",
      "\n",
      "最终答案:\n",
      "\n",
      "\n",
      "x = -3.75\n"
     ]
    }
   ],
   "source": [
    "# 逐步打印解题结果 - 展示如何处理结构化输出\n",
    "\n",
    "# 将 JSON 字符串解析为 Python 字典\n",
    "# 这展示了结构化输出的一个重要优势：数据格式可预测且易于处理\n",
    "result = json.loads(result.content)\n",
    "\n",
    "# 提取解题步骤数组和最终答案\n",
    "steps = result['steps']\n",
    "final_answer = result['final_answer']\n",
    "\n",
    "# 遍历每个解题步骤并格式化输出\n",
    "for i in range(len(steps)):\n",
    "    # 打印步骤编号和解释\n",
    "    print(f\"步骤 {i+1}: {steps[i]['explanation']}\\n\")\n",
    "    # 打印该步骤的数学表达式\n",
    "    print(steps[i]['output'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 打印最终答案\n",
    "print(\"最终答案:\\n\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBoP_eEfLGMb"
   },
   "source": [
    "## 步骤 3：在 Langfuse 中查看您的追踪记录\n",
    "\n",
    "现在您可以在 Langfuse 中查看追踪记录和 JSON Schema。\n",
    "\n",
    "**Langfuse 追踪功能的价值：**\n",
    "- 📊 **完整记录**：记录每次 API 调用的详细信息\n",
    "- 🔍 **结构分析**：可视化展示 JSON Schema 和实际输出\n",
    "- ⏱️ **性能监控**：追踪响应时间和 token 使用情况\n",
    "- 🐛 **调试支持**：帮助识别和解决问题\n",
    "- 📈 **使用统计**：分析模型使用模式和成本\n",
    "\n",
    "[Langfuse 中的示例追踪记录](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=058c85fac3a31c8fcad5291467b92633&timestamp=2025-09-23T02%3A49%3A38.363Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhY0etZVU33V"
   },
   "source": [
    "![在 Langfuse UI 中查看示例追踪记录](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509231059860.png)\n",
    "**图片说明：** 上图展示了 Langfuse 界面中的追踪记录，您可以看到：\n",
    "- 🔄 **调用流程**：完整的 API 调用链路\n",
    "- 📝 **输入输出**：系统提示、用户问题和模型响应\n",
    "- 📊 **结构化数据**：JSON Schema 和实际输出的对比\n",
    "- ⏱️ **性能指标**：响应时间、token 消耗等关键指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIJcuwxHc_c7"
   },
   "source": [
    "## 替代方案：使用 SDK 的 `parse` 辅助方法\n",
    "\n",
    "新版本的 SDK 添加了 `parse` 辅助方法，允许您使用自己的 Pydantic 模型而无需定义 JSON Schema。\n",
    "\n",
    "**Pydantic 方法的优势：**\n",
    "- 🐍 **Python 原生**：使用 Python 类定义数据结构，更符合 Python 开发习惯\n",
    "- 🔧 **类型检查**：提供更好的 IDE 支持和类型提示\n",
    "- 📝 **简洁语法**：相比 JSON Schema，代码更简洁易读\n",
    "- ✅ **自动验证**：Pydantic 提供强大的数据验证功能\n",
    "\n",
    "**注意：** 这种方法目前在 Langfuse 追踪中的功能可能有限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JMsxDJB8c_c7"
   },
   "outputs": [],
   "source": [
    "# 导入 Pydantic 用于定义数据模型\n",
    "# Pydantic 是 Python 中最流行的数据验证库\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 定义数学推理的数据模型\n",
    "class MathReasoning(BaseModel):\n",
    "    \"\"\"数学推理结果的数据模型\"\"\"\n",
    "\n",
    "    # 内嵌类定义单个解题步骤的结构\n",
    "    class Step(BaseModel):\n",
    "        \"\"\"单个解题步骤的数据模型\"\"\"\n",
    "        explanation: str  # 解释说明：描述这一步的推理过程\n",
    "        output: str       # 输出结果：这一步的数学表达式或计算结果\n",
    "\n",
    "    # 主模型的字段定义\n",
    "    steps: list[Step]    # 解题步骤列表：包含所有解题步骤\n",
    "    final_answer: str    # 最终答案：问题的最终解答\n",
    "\n",
    "def get_math_solution(question: str):\n",
    "    \"\"\"\n",
    "    使用 Pydantic 模型获取数学问题的结构化解决方案\n",
    "\n",
    "    参数:\n",
    "        question (str): 要解决的数学问题\n",
    "\n",
    "    返回:\n",
    "        包含解析后的结构化数据的响应\n",
    "    \"\"\"\n",
    "    # 使用 beta API 的 parse 方法\n",
    "    # 这个方法会自动将 Pydantic 模型转换为 JSON Schema\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=openai_model,  # 指定模型\n",
    "        messages=[\n",
    "            # 系统消息：定义AI的角色\n",
    "            {\"role\": \"system\", \"content\": math_tutor_prompt},\n",
    "            # 用户消息：具体的数学问题\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        # 直接传入 Pydantic 模型类，SDK 会自动处理转换\n",
    "        response_format=MathReasoning,\n",
    "    )\n",
    "\n",
    "    # 返回响应消息\n",
    "    # 注意：使用 .parsed 属性可以直接获取解析后的 Pydantic 对象\n",
    "    return response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dqsfV3APc_c7",
    "outputId": "e991f2cc-0515-478a-81a9-f42281fcca9e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "解题步骤:\n",
      "[Step(explanation='首先，我们要将等式中的常数项移到方程的另一边，以便能够隔离变量x。我们通过从等式两边减去7来做到这一点。', output='8x + 7 - 7 = -23 - 7'), Step(explanation='计算等式两边的简化结果。左边的7减去7得到0，而右边的-23减去7得到-30。', output='8x = -30'), Step(explanation='现在，我们要通过除以系数8来使x单独成为方程中的一项。我们通过在两边除以8来做到这一点。', output='8x/8 = -30/8'), Step(explanation='计算等式的结果，x等于-30除以8。', output='x = -3.75')]\n",
      "\n",
      "最终答案:\n",
      "x = -3.75\n"
     ]
    }
   ],
   "source": [
    "# 调用函数并获取解析后的 Pydantic 对象\n",
    "# .parsed 属性直接返回类型化的 Python 对象，而不是 JSON 字符串\n",
    "result = get_math_solution(question).parsed\n",
    "\n",
    "# 打印解题步骤 - 这里 result.steps 是 Step 对象的列表\n",
    "print(\"解题步骤:\")\n",
    "print(result.steps)\n",
    "\n",
    "# 打印最终答案 - 直接访问对象属性\n",
    "print(\"\\n最终答案:\")\n",
    "print(result.final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZwl6lpIc_c7"
   },
   "source": [
    "## 在 Langfuse 中查看您的追踪记录\n",
    "\n",
    "现在您可以在 Langfuse 中查看追踪记录和您提供的 Pydantic 模型。\n",
    "\n",
    "**Pydantic 方法的追踪特点：**\n",
    "- 🏗️ **模型结构**：显示 Pydantic 模型的类定义和字段类型\n",
    "- 🔄 **自动转换**：展示从 Pydantic 到 JSON Schema 的自动转换过程\n",
    "- 📊 **类型信息**：保留完整的 Python 类型注解信息\n",
    "\n",
    "[Langfuse 中的 Pydantic 示例追踪记录](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/59c4376a-c8eb-4ecb-8780-2f028b87e7eb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhOCprf1c_c7"
   },
   "source": [
    "![在 Langfuse UI 中查看 Pydantic 方法的追踪记录](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509231103540.png)\n",
    "\n",
    "**图片说明：** 上图展示了使用 Pydantic 方法时的 Langfuse 追踪界面，您可以看到：\n",
    "- 🐍 **Pydantic 模型**：完整的类定义和字段类型信息\n",
    "- 🔄 **Schema 转换**：自动生成的 JSON Schema\n",
    "- 📝 **类型安全**：强类型的数据结构和验证\n",
    "- ⚡ **开发效率**：更简洁的代码和更好的 IDE 支持"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}