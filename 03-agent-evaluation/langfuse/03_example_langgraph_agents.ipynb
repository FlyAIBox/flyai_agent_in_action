{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(flyai_agent_in_action)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\n",
      "✅ 正在使用的环境路径: /workspace/envs/flyai_agent_in_action\n",
      "\n",
      "💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"flyai_agent_in_action\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 flyai_agent_in_action 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的 Python 单元格将使用 Notebook 当前选择的 Jupyter 内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(flyai_agent_in_action)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(flyai_agent_in_action)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate flyai_agent_in_action\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /root/.config/pip/pip.conf\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "global.index-url='https://pypi.tuna.tsinghua.edu.cn/simple'\n",
      ":env:.target=''\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFaceAgent\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                  |\n",
      "|:-------------|:----------------------------------------------------------------------|\n",
      "| 操作系统     | Linux 5.15.0-126-generic                                              |\n",
      "| CPU 信息     | Intel(R) Xeon(R) Platinum 8468 (48 physical cores, 192 logical cores) |\n",
      "| 内存信息     | 2015.36 GB (Available: 1848.40 GB)                                    |\n",
      "| GPU 信息     | No GPU found (checked nvidia-smi, lshw not found)                     |\n",
      "| CUDA 信息    | 12.6                                                                  |\n",
      "| Python 版本  | 3.12.11                                                               |\n",
      "| Conda 版本   | conda 25.7.0                                                          |\n",
      "| 物理磁盘空间 | Total: 2014.78 GB, Used: 787.65 GB, Free: 1124.71 GB                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2_SkHQMwIsZ"
   },
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Example - Trace and Evaluate LangGraph Agents\" description: \"This guide shows how to evaluate LangGraph Agents with Langfuse using online and offline evaluation methods.\" category: \"Integrations\" -->\n",
    "\n",
    "# LangGraph Agent追踪与评估完整指南\n",
    "\n",
    "## 📖 教程概述\n",
    "\n",
    "在本教程中，我们将深入学习如何使用 [Langfuse](https://langfuse.com)（一个强大的大模型可观测性平台）与 [Hugging Face Datasets](https://huggingface.co/datasets)，来**全面监控 [LangGraph Agent](https://github.com/langchain-ai/langgraph) 的执行过程（traces）**并**科学评估其性能表现**。\n",
    "\n",
    "## 🎯 学习目标\n",
    "\n",
    "本指南将帮助您掌握将 AI Agent快速且可靠地部署到生产环境所需的核心技能：\n",
    "- **在线评估**：实时监控生产环境中的Agent表现\n",
    "- **离线评估**：使用基准数据集进行系统性测试\n",
    "\n",
    "\n",
    "## 🔍 为什么 AI Agent评估如此重要？\n",
    "\n",
    "在 AI Agent开发过程中，评估是确保系统质量的关键环节：\n",
    "\n",
    "- **🐛 问题诊断**：当Agent任务执行失败或结果不理想时，能够快速定位问题根源\n",
    "- **📊 性能监控**：实时追踪系统的成本消耗、响应延迟等关键指标\n",
    "- **🔄 持续改进**：通过用户反馈和评估数据，不断提升Agent的可靠性与安全性\n",
    "- **🚀 生产就绪**：确保Agent在真实环境中能够稳定运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzPbsmLrfoSN"
   },
   "source": [
    "## 🛠️ 步骤 0：环境准备与依赖安装\n",
    "\n",
    "### 📦 安装核心依赖库\n",
    "\n",
    "在开始本教程之前，我们需要安装以下核心库：\n",
    "\n",
    "- **`langgraph`**：用于构建多节点、状态驱动的 AI Agent工作流\n",
    "- **`langfuse`**：提供大模型应用的可观测性和评估功能  \n",
    "- **`langchain`** 系列：用于 LLM 应用开发的核心框架\n",
    "- **`datasets`**：Hugging Face 的数据集处理库\n",
    "\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"⚠️\" -->\n",
    "**📌 重要提示：**\n",
    "- 本教程使用 **Langfuse Python SDK v3**，它提供了更好的性能和新特性\n",
    "- 建议在虚拟环境中运行本教程以避免依赖冲突\n",
    "<!-- CALLOUT_END -->\n",
    "\n",
    "### 🔧 环境要求\n",
    "\n",
    "- **Python 版本**：3.8 或更高版本\n",
    "- **操作系统**：支持 Windows、macOS、Linux\n",
    "- **网络**：需要访问 OpenAI API 和 Langfuse 服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqXWStafwIsd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EI_0ZfzfoSO",
    "outputId": "d4418732-c65a-41cd-a7b3-931838ea85d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langfuse==3.3.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (3.3.0)\n",
      "Requirement already satisfied: langchain==0.3.27 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langgraph==0.6.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.6.7)\n",
      "Requirement already satisfied: langchain-openai==0.3.31 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain_community==0.3.27 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_huggingface==0.3.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.3.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (2.32.5)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langfuse==3.3.0) (1.17.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain==0.3.27) (0.3.75)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain==0.3.27) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain==0.3.27) (0.4.31)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain==0.3.27) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain==0.3.27) (6.0.3)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langgraph==0.6.7) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langgraph==0.6.7) (0.6.4)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langgraph==0.6.7) (0.2.6)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langgraph==0.6.7) (3.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain-openai==0.3.31) (1.107.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain-openai==0.3.31) (0.11.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_community==0.3.27) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_community==0.3.27) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_community==0.3.27) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_community==0.3.27) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_community==0.3.27) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_community==0.3.27) (2.3.3)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_huggingface==0.3.1) (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain_huggingface==0.3.1) (0.35.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27) (0.9.0)\n",
      "Requirement already satisfied: anyio in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.3.0) (0.16.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (4.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.6.7) (1.10.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph==0.6.7) (3.11.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (4.67.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (6.32.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.3.0) (0.58b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community==0.3.27) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests<3,>=2->langfuse==3.3.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests<3,>=2->langfuse==3.3.0) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.31) (2024.11.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27) (1.1.0)\n",
      "Requirement already satisfied: filelock in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface==0.3.1) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface==0.3.1) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface==0.3.1) (1.1.10)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.25.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 📦 安装所需的Python包\n",
    "# 使用魔法命令 %pip 在Jupyter环境中安装依赖库\n",
    "%pip install langfuse==3.3.0 langchain==0.3.27 langgraph==0.6.7 langchain-openai==0.3.31 langchain_community==0.3.27 langchain_huggingface==0.3.1\n",
    "\n",
    "# 各库功能说明：\n",
    "# - langfuse: LLM应用的可观测性和评估平台\n",
    "# - langchain: 大语言模型应用开发框架\n",
    "# - langgraph: 基于langchain的图形化工作流构建工具\n",
    "# - langchain_openai: OpenAI模型的langchain集成\n",
    "# - langchain_community: 社区贡献的langchain扩展\n",
    "# - langchain_huggingface: Hugging Face模型的langchain集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHRsxz1VfoSP"
   },
   "source": [
    "## 🔑 步骤 1：配置 API 密钥和环境变量\n",
    "\n",
    "### 获取 Langfuse API 密钥\n",
    "\n",
    "在开始使用 Langfuse 之前，您需要获取 API 访问凭证：\n",
    "\n",
    "#### 方案一：使用 Langfuse Cloud（推荐）\n",
    "1. 访问 [Langfuse Cloud](https://cloud.langfuse.com) 并注册账户\n",
    "2. 创建新项目或选择现有项目\n",
    "3. 在项目设置页面获取以下密钥：\n",
    "   - `LANGFUSE_PUBLIC_KEY`：以 `pk-lf-` 开头的公钥\n",
    "   - `LANGFUSE_SECRET_KEY`：以 `sk-lf-` 开头的私钥\n",
    "\n",
    "#### 方案二：自托管 Langfuse\n",
    "如果您选择自托管部署，请按照 [Langfuse 自托管文档](https://langfuse.com/docs/deployment/self-host) 进行配置。\n",
    "\n",
    "### 获取 OpenAI API 密钥\n",
    "\n",
    "1. 访问 [OpenAI 平台](https://platform.openai.com/)\n",
    "2. 注册账户并完成身份验证\n",
    "3. 在 API 密钥页面创建新的 API 密钥\n",
    "4. 确保账户有足够的余额用于 API 调用\n",
    "\n",
    "### 🔐 安全提醒\n",
    "\n",
    "- **请勿将 API 密钥硬编码在代码中**\n",
    "- **生产环境建议使用环境变量或密钥管理系统**\n",
    "- **定期轮换密钥以提高安全性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZnxtWx9foSP",
    "outputId": "cbabe0f0-49e2-4500-8b9a-b43baaa5cc17"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:  ········\n",
      "OPENAI_BASE_URL:  ········\n",
      "LANGFUSE_PUBLIC_KEY:  ········\n",
      "LANGFUSE_SECRET_KEY:  ········\n",
      "LANGFUSE_HOST:  ········\n"
     ]
    }
   ],
   "source": [
    "# 🔐 环境变量配置 - 安全存储敏感信息\n",
    "# 环境变量是存储API密钥等敏感信息的最佳实践\n",
    "# 避免在代码中硬编码密钥，防止泄露\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全地设置环境变量\n",
    "    如果环境变量不存在，会提示用户输入\n",
    "    使用getpass模块隐藏输入内容，防止密码泄露\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 🤖 OpenAI API 配置\n",
    "# OpenAI API密钥：从 https://platform.openai.com/api-keys 获取\n",
    "# 这是调用GPT模型必需的认证信息\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# APIAgent地址：如果你使用第三方Agent服务（如国内Agent）\n",
    "# 示例：https://api.apiyi.com/v1\n",
    "# 如果直接使用OpenAI官方API，可以留空\n",
    "_set_env(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 🌐 Langfuse 配置\n",
    "# Langfuse是一个可观测性平台，需要注册账户获取密钥\n",
    "# 注册地址：https://cloud.langfuse.com\n",
    "\n",
    "# 公开密钥：用于标识你的项目\n",
    "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
    "\n",
    "# 秘密密钥：用于认证，请妥善保管\n",
    "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "# 服务器地址：选择离你最近的区域\n",
    "# 🇪🇺 欧盟区域(推荐) https://cloud.langfuse.com\n",
    "# 🇺🇸 美国区域 https://us.cloud.langfuse.com\n",
    "_set_env(\"LANGFUSE_HOST\")\n",
    "\n",
    "# 💡 初学者提示：\n",
    "# 1. 环境变量存储在操作系统中，重启后需要重新设置\n",
    "# 2. 生产环境中建议使用.env文件或云服务配置\n",
    "# 3. 永远不要在代码中硬编码API密钥！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJjyA41_wIsi"
   },
   "source": [
    "### 🔗 连接验证与客户端初始化\n",
    "\n",
    "设置完环境变量后，我们需要初始化 Langfuse 客户端并验证连接。\n",
    "\n",
    "**核心概念解释：**\n",
    "- **`get_client()`**：Langfuse 提供的便捷函数，会自动读取环境变量中的凭证\n",
    "- **客户端实例**：用于与 Langfuse 服务器通信的对象\n",
    "- **连接验证**：确保 API 密钥正确且网络连接正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvQRomm-wIsi",
    "outputId": "0fb60071-bc52-4d90-a487-5cb0bf3ff3c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Langfuse 客户端连接成功！API 认证通过\n",
      "🎯 现在可以开始追踪和评估 LLM 应用了\n"
     ]
    }
   ],
   "source": [
    "# 📡 导入 Langfuse 客户端并建立连接\n",
    "from langfuse import get_client\n",
    "\n",
    "# 🔧 初始化 Langfuse 客户端\n",
    "# get_client() 会自动从环境变量中读取 API 凭证\n",
    "langfuse = get_client()\n",
    "\n",
    "# ✅ 验证 API 连接和身份认证\n",
    "# auth_check() 方法会测试与 Langfuse 服务器的连接\n",
    "if langfuse.auth_check():\n",
    "    print(\"✅ Langfuse 客户端连接成功！API 认证通过\")\n",
    "    print(\"🎯 现在可以开始追踪和评估 LLM 应用了\")\n",
    "else:\n",
    "    print(\"❌ 认证失败！请检查以下项目：\")\n",
    "    print(\"   1. API 密钥是否正确设置\")\n",
    "    print(\"   2. 服务器地址是否正确\")\n",
    "    print(\"   3. 网络连接是否正常\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uulS5iGHfoSP"
   },
   "source": [
    "## 🧪 步骤 2：构建第一个 LangGraph Agent并验证追踪功能\n",
    "\n",
    "### 💡 什么是追踪（Tracing）？\n",
    "\n",
    "在 LLM 应用开发中，**追踪（Tracing）** 是指记录应用程序执行过程中的详细信息：\n",
    "- **执行路径**：Agent执行了哪些步骤\n",
    "- **性能指标**：每个步骤的耗时、令牌消耗等\n",
    "- **输入输出**：每个环节的输入和输出内容\n",
    "- **错误信息**：出现问题时的详细错误日志\n",
    "\n",
    "### 🎯 本节目标\n",
    "\n",
    "我们将创建一个简单的问答Agent来验证 Langfuse 追踪功能是否正常工作。\n",
    "\n",
    "**技术要点：**\n",
    "- 使用 **LangGraph** 构建状态驱动的Agent工作流\n",
    "- 通过 **CallbackHandler** 实现自动追踪\n",
    "- 在 Langfuse 仪表板中查看执行记录\n",
    "\n",
    "🔍 **运行成功标志**：如果配置正确，您将在 [Langfuse 追踪仪表板](https://cloud.langfuse.com/traces) 中看到详细的执行日志和性能指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcyynS9CfoSP",
    "outputId": "9092ae17-ff79-442e-d274-2de3e921fd41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 简单问答代理构建完成！\n",
      "🔧 代理架构：输入 → ChatBot节点 → 输出\n",
      "📝 支持功能：基本问答、上下文理解\n"
     ]
    }
   ],
   "source": [
    "# 🚀 构建简单的 LangGraph 问答Agent\n",
    "\n",
    "# 📦 导入必要的类型和工具\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# 🔧 定义Agent的状态结构\n",
    "class State(TypedDict):\n",
    "    # messages 字段存储对话历史，类型为列表\n",
    "    # Annotated[list, add_messages] 定义了状态更新的方式：\n",
    "    # - list: 数据类型为列表\n",
    "    # - add_messages: 更新时追加消息而不是覆盖（保持对话历史）\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 创建状态图构建器\n",
    "# StateGraph 是 LangGraph 的核心类，用于构建状态驱动的工作流\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 初始化 OpenAI 语言模型\n",
    "# - model: 使用 GPT-4o 模型（性能强大且成本适中）\n",
    "# - temperature: 设置为 0.2，输出相对稳定但保持一定创造性\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 💬 定义聊天机器人节点函数\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    聊天机器人的核心逻辑\n",
    "\n",
    "    参数:\n",
    "        state: 当前的对话状态，包含消息历史\n",
    "\n",
    "    返回:\n",
    "        包含新消息的字典，会被自动合并到状态中\n",
    "    \"\"\"\n",
    "    # 调用 LLM 处理当前所有消息，并返回回复\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 🔗 构建工作流图结构\n",
    "# 1. 添加节点：每个节点代表一个工作单元（通常是 Python 函数）\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 2. 设置入口点：告诉图从哪个节点开始执行\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# 3. 设置结束点：定义工作流的终止条件\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ⚙️ 编译图以获得可执行的Agent\n",
    "# compile() 方法将图定义转换为可运行的 CompiledGraph 对象\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"✅ 简单问答Agent构建完成！\")\n",
    "print(\"🔧 Agent架构：输入 → ChatBot节点 → 输出\")\n",
    "print(\"📝 支持功能：基本问答、上下文理解\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLXIdYuwwIsi",
    "outputId": "38e19edf-0627-4974-88b2-5ec7fa350611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始运行代理并启用 Langfuse 追踪...\n",
      "❓ 用户问题：Langfuse是什么，应用场景是?\n",
      "📊 追踪信息将发送到 Langfuse 平台\n",
      "--------------------------------------------------\n",
      "📥 代理执行步骤: {'chatbot': {'messages': [AIMessage(content='Langfuse 是一个用于监控和调试生成式 AI 应用程序的工具。它可以帮助开发者跟踪和分析与生成式 AI 模型（如 OpenAI 的 GPT-3 或其他类似模型）的交互。Langfuse 提供了可视化界面，帮助用户理解模型的行为、性能以及用户与模型的交互情况。\\n\\n应用场景包括：\\n\\n1. **调试生成式 AI 应用**：开发者可以使用 Langfuse 来监控模型的输出，识别潜在的问题，并进行调试和优化。\\n\\n2. **性能分析**：通过分析模型的响应时间和准确性，开发者可以优化模型的性能，提升用户体验。\\n\\n3. **用户交互分析**：Langfuse 可以帮助开发者了解用户如何与 AI 应用交互，从而改进应用的设计和功能。\\n\\n4. **安全和合规性**：通过监控和记录模型的输出，开发者可以确保应用符合相关的安全和合规性要求。\\n\\n总之，Langfuse 是一个强大的工具，适用于任何需要监控和优化生成式 AI 应用的场景。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 251, 'prompt_tokens': 17, 'total_tokens': 268, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_8458c98457', 'id': 'chatcmpl-CLQquIiGcpreLADevFjhJ0sX4Ou5M', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--df3efe01-6bcf-4f03-9a73-8975d4ebf095-0', usage_metadata={'input_tokens': 17, 'output_tokens': 251, 'total_tokens': 268, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "--------------------------------------------------\n",
      "✅ 代理执行完成！\n",
      "🔗 请访问 Langfuse 仪表板查看详细追踪信息\n",
      "📍 链接: https://cloud.langfuse.com/traces\n"
     ]
    }
   ],
   "source": [
    "# 🔍 启用 Langfuse 追踪并运行Agent\n",
    "\n",
    "# 📡 导入 Langfuse 的 LangChain 回调处理器\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 🎯 初始化 Langfuse 追踪处理器\n",
    "# CallbackHandler 会自动捕获 LangChain/LangGraph 的执行信息\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "print(\"🚀 开始运行Agent并启用 Langfuse 追踪...\")\n",
    "print(\"❓ 用户问题：Langfuse是什么，应用场景是?\")\n",
    "print(\"📊 追踪信息将发送到 Langfuse 平台\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 🏃 运行Agent并启用追踪\n",
    "# stream() 方法允许实时接收Agent的执行结果\n",
    "for step_result in graph.stream(\n",
    "    # 输入：包含用户消息的状态\n",
    "    {\"messages\": [HumanMessage(content=\"Langfuse是什么，应用场景是?\")]},\n",
    "    # 配置：启用 Langfuse 回调处理器进行追踪\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    "):\n",
    "    print(f\"📥 Agent执行步骤: {step_result}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"✅ Agent执行完成！\")\n",
    "print(\"🔗 请访问 Langfuse 仪表板查看详细追踪信息\")\n",
    "print(\"📍 链接: https://cloud.langfuse.com/traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPLt1hRkfoSQ"
   },
   "source": [
    "### 🔍 验证追踪功能：查看 Langfuse 仪表板\n",
    "\n",
    "#### 📊 如何检查追踪记录\n",
    "\n",
    "运行上述代码后，请按以下步骤验证追踪功能：\n",
    "\n",
    "1. **访问仪表板**：打开 [Langfuse 追踪仪表板](https://cloud.langfuse.com/traces)\n",
    "2. **查找记录**：在追踪列表中找到刚才的执行记录\n",
    "3. **分析数据**：点击记录查看详细的执行信息\n",
    "\n",
    "#### 🔬 追踪记录包含什么信息？\n",
    "\n",
    "在 Langfuse 中，您将看到以下重要信息：\n",
    "\n",
    "- **📝 Spans（跨度）**：每个执行步骤的详细记录\n",
    "- **📋 Logs（日志）**：执行过程中的日志信息  \n",
    "- **⏱️ 时间戳**：每个步骤的精确执行时间\n",
    "- **💰 成本信息**：API 调用的令牌消耗和费用\n",
    "- **📊 性能指标**：延迟、吞吐量等关键指标\n",
    "\n",
    "#### 📸 Langfuse 中的示例追踪截图\n",
    "\n",
    "![Langfuse 中的示例追踪](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251551391.png)\n",
    "\n",
    "💡 **小提示**：追踪记录可能需要几秒钟才能在仪表板中显示，请稍作等待。\n",
    "\n",
    "🔗 _[查看示例追踪记录](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=6efb8472addcad81fa932915e6a5eff2&timestamp=2025-09-25T07%3A48%3A04.647Z)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onjMD-ZJfoSQ"
   },
   "source": [
    "## 🔬 步骤 3：构建并观测复杂的邮件处理Agent\n",
    "\n",
    "### 🎯 进阶实战：真实业务场景模拟\n",
    "\n",
    "既然已确认基础追踪功能有效，现在我们来构建一个更加复杂且贴近实际业务场景的Agent系统。\n",
    "\n",
    "### 📧 业务场景：智能邮件管理助手\n",
    "\n",
    "我们将创建一个**邮件处理Agent**，具备以下功能：\n",
    "\n",
    "#### 🔧 核心功能模块\n",
    "- **📬 邮件接收**：读取和解析邮件内容\n",
    "- **🔍 垃圾邮件识别**：智能判断邮件是否为垃圾邮件\n",
    "- **🗂️ 自动分类**：对合法邮件进行分类处理\n",
    "- **✍️ 回复起草**：为重要邮件生成回复草稿\n",
    "- **📢 通知主人**：向韦恩先生汇报重要邮件\n",
    "\n",
    "#### 📊 追踪的高级指标\n",
    "\n",
    "通过这个复杂Agent，我们将观察以下关键指标：\n",
    "- **💰 成本追踪**：详细的令牌消耗和 API 费用\n",
    "- **⏱️ 性能分析**：每个处理步骤的耗时分布\n",
    "- **🔄 工作流路径**：Agent的决策逻辑和执行路径\n",
    "- **❌ 错误监控**：异常情况的捕获和分析\n",
    "\n",
    "### 🏗️ 技术架构特点\n",
    "\n",
    "- **状态驱动**：使用 LangGraph 的状态管理机制\n",
    "- **条件分支**：根据邮件类型执行不同的处理逻辑\n",
    "- **多节点协作**：模拟真实的业务处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhZyuiZNwIsj",
    "outputId": "66f2befe-309e-4f09-d675-405a64dcebbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 库导入完成，准备构建邮件处理代理...\n",
      "🔧 即将使用的核心组件：\n",
      "   - StateGraph: 构建状态驱动的工作流\n",
      "   - ChatOpenAI: 调用 GPT 模型进行智能处理\n",
      "   - TypedDict: 定义严格的数据结构\n"
     ]
    }
   ],
   "source": [
    "# 📦 导入构建复杂Agent所需的库\n",
    "\n",
    "import os  # 操作系统接口，用于环境变量管理\n",
    "from typing import TypedDict, List, Dict, Any, Optional  # 类型注解，提高代码可读性和IDE支持\n",
    "from langgraph.graph import StateGraph, START, END  # LangGraph核心组件：状态图、开始节点、结束节点\n",
    "from langchain_openai import ChatOpenAI  # OpenAI模型的LangChain集成\n",
    "from langchain_core.messages import HumanMessage  # LangChain消息类型\n",
    "\n",
    "print(\"📚 库导入完成，准备构建邮件处理Agent...\")\n",
    "print(\"🔧 即将使用的核心组件：\")\n",
    "print(\"   - StateGraph: 构建状态驱动的工作流\")\n",
    "print(\"   - ChatOpenAI: 调用 GPT 模型进行智能处理\")\n",
    "print(\"   - TypedDict: 定义严格的数据结构\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbdEGxUQwIsj",
    "outputId": "f37e46ee-8c8e-467e-8fe9-7953eeb071b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 邮件状态结构定义完成\n",
      "📋 状态字段说明：\n",
      "   - email: 原始邮件数据\n",
      "   - is_spam: 垃圾邮件判定结果\n",
      "   - draft_response: 回复草稿\n",
      "   - messages: LLM对话历史\n"
     ]
    }
   ],
   "source": [
    "# 🏗️ 定义邮件处理Agent的状态结构\n",
    "\n",
    "class EmailState(TypedDict):\n",
    "    \"\"\"\n",
    "    邮件处理Agent的状态数据结构\n",
    "\n",
    "    这个类定义了Agent在处理邮件过程中需要维护的所有状态信息\n",
    "    \"\"\"\n",
    "    # 📧 原始邮件信息\n",
    "    email: Dict[str, Any]  # 包含发件人、主题、正文等邮件完整信息\n",
    "\n",
    "    # 🔍 垃圾邮件检测结果\n",
    "    is_spam: Optional[bool]  # 是否为垃圾邮件（True/False/None）\n",
    "    spam_reason: Optional[str]  # 判定为垃圾邮件的原因说明\n",
    "\n",
    "    # 🗂️ 邮件分类信息\n",
    "    email_category: Optional[str]  # 邮件类别（如：商务、个人、紧急等）\n",
    "\n",
    "    # ✍️ 回复草稿\n",
    "    draft_response: Optional[str]  # 为主人准备的回复草稿\n",
    "\n",
    "    # 💬 对话历史记录\n",
    "    messages: List[Dict[str, Any]]  # 存储处理过程中的LLM对话记录\n",
    "\n",
    "print(\"✅ 邮件状态结构定义完成\")\n",
    "print(\"📋 状态字段说明：\")\n",
    "print(\"   - email: 原始邮件数据\")\n",
    "print(\"   - is_spam: 垃圾邮件判定结果\")\n",
    "print(\"   - draft_response: 回复草稿\")\n",
    "print(\"   - messages: LLM对话历史\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdo7y_0mwIsj",
    "outputId": "08199035-52cb-4553-cbbb-738aa5b2e4c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f2fd464f860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ✅ 初始化大语言模型（LLM），后续所有节点都会复用它进行推理\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 🧱 在运行实际图之前再次定义状态结构，确保每个节点能拿到自己需要的数据\n",
    "class EmailState(TypedDict):\n",
    "    email: Dict[str, Any]            # 📬 当前待处理的原始邮件内容（发件人、主题、正文）\n",
    "    is_spam: Optional[bool]          # 🚨 垃圾邮件判定结果，None 表示尚未判定\n",
    "    draft_response: Optional[str]    # ✍️ Alfred 起草的回复内容\n",
    "    messages: List[Dict[str, Any]]   # 🗒️ LangChain 对话历史，用来记录模型调用\n",
    "\n",
    "# 🔁 定义工作流中的每个节点函数\n",
    "def read_email(state: EmailState):\n",
    "    \"\"\"\n",
    "    入口节点：展示邮件基础信息，帮助我们在命令行中观察流程。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]  # 从状态中取出当前邮件\n",
    "    print(f\"阿尔弗雷德正在处理来自 {email['sender']} 的邮件，主题为：{email['subject']}\")\n",
    "    return {}  # 节点只做展示，不修改状态\n",
    "\n",
    "def classify_email(state: EmailState):\n",
    "    \"\"\"\n",
    "    使用 LLM 判断当前邮件是否为垃圾邮件。\n",
    "    如果是垃圾邮件就不记录模型对话，避免污染历史。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]\n",
    "\n",
    "    # 构造提示词，向 LLM 传入邮件的所有关键信息（中文初学者友好版本）\n",
    "    prompt = f\"\"\"\n",
    "请以阿尔弗雷德（Alfred，韦恩先生的管家，同时知晓其“蝙蝠侠”身份）的视角，分析下面这封邮件，判断其是垃圾邮件（SPAM）还是正常邮件（HAM），并说明是否需要提醒韦恩先生注意。\n",
    "\n",
    "邮件内容：\n",
    "发件人（From）：{email['sender']}\n",
    "主题（Subject）：{email['subject']}\n",
    "正文（Body）：{email['body']}\n",
    "\n",
    "请先判断这封邮件是否为垃圾邮件。\n",
    "只返回一个英文单词作为最终答案：若是垃圾邮件，返回“SPAM”；若是正常邮件，返回“HAM”。不要输出多余文字。\n",
    "答案：\n",
    "    \"\"\"\n",
    "    messages = [HumanMessage(content=prompt)]  # LangChain 要求传入 HumanMessage 对象\n",
    "    response = model.invoke(messages)  # 调用 LLM 获得判定结果\n",
    "\n",
    "    response_text = response.content.lower()  # 统一转小写，便于关键词匹配\n",
    "    print(response_text)  # 在控制台输出，方便我们调试和观察\n",
    "    is_spam = \"spam\" in response_text and \"ham\" not in response_text  # 同时排除同时出现 spam/ham 的情况\n",
    "\n",
    "    if not is_spam:\n",
    "        # 如果不是垃圾邮件，就将本次问答追加到对话历史中，供后续节点使用\n",
    "        new_messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response.content}\n",
    "        ]\n",
    "    else:\n",
    "        # 垃圾邮件无需记录上下文，保持原有的消息记录\n",
    "        new_messages = state.get(\"messages\", [])\n",
    "\n",
    "    return {\n",
    "        \"is_spam\": is_spam,       # 把垃圾邮件判定结果写回状态\n",
    "        \"messages\": new_messages  # 同步对话历史\n",
    "    }\n",
    "\n",
    "def handle_spam(state: EmailState):\n",
    "    \"\"\"\n",
    "    垃圾邮件分支：这里只演示打印提示语，真实项目可以写入数据库或报警。\n",
    "    \"\"\"\n",
    "    print(\"阿尔弗雷德已经将邮件标记为垃圾邮件。\")\n",
    "    print(\"该电子邮件已被移至垃圾邮件文件夹。\")\n",
    "    return {}  # 返回空字典表示不修改状态字段\n",
    "\n",
    "def drafting_response(state: EmailState):\n",
    "    \"\"\"\n",
    "    合法邮件分支：让 LLM 帮忙撰写一份礼貌的回复草稿。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]\n",
    "\n",
    "    # 维持提示词，明确输出语气和需要覆盖的关键内容（中文初学者友好版本）\n",
    "    prompt = f\"\"\"\n",
    "请以阿尔弗雷德（Alfred，韦恩先生的管家）的口吻，为下面这封邮件起草一份礼貌、简洁且专业的初稿回复。\n",
    "\n",
    "邮件内容：\n",
    "发件人（From）：{email['sender']}\n",
    "主题（Subject）：{email['subject']}\n",
    "正文（Body）：{email['body']}\n",
    "\n",
    "请生成一段简短、专业、语气友善的中文回复草稿，供韦恩先生审阅并在发送前个性化润色。\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 将最新的问答追加到对话历史里，保持上下文完整\n",
    "    new_messages = state.get(\"messages\", []) + [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"draft_response\": response.content,  # 保存生成的邮件草稿\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "def notify_mr_wayne(state: EmailState):\n",
    "    \"\"\"\n",
    "    收尾节点：模拟向布鲁斯·韦恩汇报邮件处理结果。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]\n",
    "\n",
    "    print(\"\" + \"=\"*50)\n",
    "    print(f\"Sir, you've received an email from {email['sender']}.\")\n",
    "    print(f\"Subject: {email['subject']}\")\n",
    "    print(\"I've prepared a draft response for your review:\")\n",
    "    print(\"-\"*50)\n",
    "    print(state[\"draft_response\"])\n",
    "    print(\"=\"*50 + \"\")\n",
    "\n",
    "    return {}\n",
    "\n",
    "# 🧭 路由逻辑：根据垃圾邮件判定选择下一步的分支\n",
    "def route_email(state: EmailState) -> str:\n",
    "    if state[\"is_spam\"]:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"legitimate\"\n",
    "\n",
    "# 🛠️ 创建状态图，将上面定义的节点串联成一个 LangGraph 工作流\n",
    "email_graph = StateGraph(EmailState)\n",
    "\n",
    "# 📌 注册节点——每一行都会把函数变成图里的一个执行节点\n",
    "email_graph.add_node(\"read_email\", read_email)  # 首先读取并展示邮件信息\n",
    "email_graph.add_node(\"classify_email\", classify_email)  # 然后请 LLM 判定垃圾邮件\n",
    "email_graph.add_node(\"handle_spam\", handle_spam)  # 垃圾邮件走单独的处理分支\n",
    "email_graph.add_node(\"drafting_response\", drafting_response)  # 合法邮件生成回复草稿\n",
    "email_graph.add_node(\"notify_mr_wayne\", notify_mr_wayne)  # 最后向主人汇报结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1I6C2-0jwIsj",
    "outputId": "abd06b5b-3165-4cf7-996b-6fc9864059ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7f2fd464f860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ➕ 配置节点之间的流转顺序\n",
    "email_graph.add_edge(START, \"read_email\")  # 图的起点先进入 read_email 节点\n",
    "\n",
    "# 🧠 判定之后根据结果流向不同分支\n",
    "email_graph.add_edge(\"read_email\", \"classify_email\")  # 展示完邮件后调用分类逻辑\n",
    "\n",
    "# 🔀 添加条件分支：route_email 返回字符串决定下一条边\n",
    "email_graph.add_conditional_edges(\n",
    "    \"classify_email\",  # 根据垃圾邮件判定结果来决定去向\n",
    "    route_email,\n",
    "    {\n",
    "        \"spam\": \"handle_spam\",          # 判定为垃圾邮件则直接走 handle_spam 节点\n",
    "        \"legitimate\": \"drafting_response\"  # 合法邮件则继续撰写回复\n",
    "    }\n",
    ")\n",
    "\n",
    "# ✅ 收尾：无论哪个分支走完都回到 END 节点\n",
    "email_graph.add_edge(\"handle_spam\", END)  # 垃圾邮件处理完毕即结束\n",
    "email_graph.add_edge(\"drafting_response\", \"notify_mr_wayne\")  # 回复草稿后通知主人\n",
    "email_graph.add_edge(\"notify_mr_wayne\", END)  # 汇报结束后整个流程收尾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "haTVi7UmwIsj"
   },
   "outputs": [],
   "source": [
    "# 🧮 将图结构编译成可执行的 LangGraph Agent对象\n",
    "compiled_graph = email_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GlJYoVUbwIsj"
   },
   "outputs": [],
   "source": [
    "# 📨 准备两封示例邮件，帮助我们观察不同分支的执行效果\n",
    "legitimate_email = {\n",
    "    \"sender\": \"京东客服\",  # 发件人\n",
    "    \"subject\": \"关于您近期订单的发票开具说明\",  # 邮件主题\n",
    "    \"body\": \"尊敬的韦恩先生，您好！关于您在京东的近期订单，增值税电子普通发票已开具并推送至您的邮箱。如需纸质发票或抬头变更，请在7日内通过“我的订单-申请开票”发起，我们将尽快处理。给您带来不便，敬请谅解。\"  # 邮件正文\n",
    "}\n",
    "\n",
    "spam_email = {\n",
    "    \"sender\": \"某数字货币项目方\",  # 垃圾邮件常见的推销者\n",
    "    \"subject\": \"限时暴涨100倍，立即上车！\",  # 诱导性标题\n",
    "    \"body\": \"韦恩先生，我们新上线了一款数字货币，承诺稳稳赚、稳赚不赔！扫码加群，前100名赠送空投名额，错过今天再等一年！\"  # 明显的垃圾推广/诈骗文案\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwnC2JXXwIsj",
    "outputId": "9aa7dc2d-908f-447c-a7ce-b47f36dae81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing legitimate email...\n",
      "阿尔弗雷德正在处理来自 京东客服 的邮件，主题为：关于您近期订单的发票开具说明\n",
      "ham\n",
      "==================================================\n",
      "Sir, you've received an email from 京东客服.\n",
      "Subject: 关于您近期订单的发票开具说明\n",
      "I've prepared a draft response for your review:\n",
      "--------------------------------------------------\n",
      "尊敬的京东客服团队，\n",
      "\n",
      "感谢您及时发送关于我近期订单的发票信息。我已收到电子发票，并确认其内容无误。目前无需纸质发票或抬头变更。如有进一步需求，我会在规定时间内通过相关渠道联系您。\n",
      "\n",
      "感谢您的协助与支持。\n",
      "\n",
      "祝好，\n",
      "\n",
      "阿尔弗雷德  \n",
      "韦恩先生的管家\n",
      "==================================================\n",
      "Processing spam email...\n",
      "阿尔弗雷德正在处理来自 某数字货币项目方 的邮件，主题为：限时暴涨100倍，立即上车！\n",
      "spam\n",
      "阿尔弗雷德已经将邮件标记为垃圾邮件。\n",
      "该电子邮件已被移至垃圾邮件文件夹。\n"
     ]
    }
   ],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 🧩 初始化 Langfuse 的回调处理器，用于自动记录执行轨迹\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# ✅ 运行合法邮件示例，演示完整工作流\n",
    "print(\"Processing legitimate email...\")\n",
    "legitimate_result = compiled_graph.invoke(\n",
    "    input={\n",
    "        \"email\": legitimate_email,\n",
    "        \"is_spam\": None,\n",
    "        \"draft_response\": None,\n",
    "        \"messages\": []\n",
    "        },\n",
    "    config={\"callbacks\": [langfuse_handler]}  # 将回调挂到图的执行配置上\n",
    ")\n",
    "\n",
    "# 🚨 再运行垃圾邮件示例，观察分支差异\n",
    "print(\"Processing spam email...\")\n",
    "spam_result = compiled_graph.invoke(\n",
    "    input={\n",
    "        \"email\": spam_email,\n",
    "        \"is_spam\": None,\n",
    "        \"draft_response\": None,\n",
    "        \"messages\": []\n",
    "        },\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjkhTgLWfoSQ"
   },
   "source": [
    "### 追踪结构\n",
    "\n",
    "Langfuse 会记录包含若干 **span（跨度）** 的**trace（追踪）**，每个 span 代表Agent逻辑中的一个步骤。本例中的追踪包含整体运行以及如下子跨度：\n",
    "- 工具调用（\n",
    "- LLM 调用（使用 'gpt-4o' 的 Responses API）\n",
    "\n",
    "你可以检查这些记录以精确了解时间消耗、令牌使用量等：\n",
    "\n",
    "![Langfuse 中的追踪树](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251730026.png)\n",
    "\n",
    "_[前往该追踪](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=2d1f23b960fb1ff0bdaf7623fda4936c&timestamp=2025-09-25T09%3A06%3A06.476Z)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHZAkQuefoSQ"
   },
   "source": [
    "## 在线评估\n",
    "\n",
    "在线评估指在真实线上环境（生产环境的实际使用中）对Agent进行评估。这需要对真实用户交互进行持续监控与结果分析。\n",
    "\n",
    "我们在此总结了多种评估技术的指南：[链接](https://langfuse.com/blog/2025-03-04-llm-evaluation-101-best-practices-and-challenges)。\n",
    "\n",
    "### 生产环境常见监控指标\n",
    "\n",
    "1. **成本（Costs）**：埋点会记录令牌用量，你可按每个令牌的价格估算成本。\n",
    "2. **延迟（Latency）**：观察完成每个步骤或整次运行所需的时间。\n",
    "3. **用户反馈（User Feedback）**：用户可直接提供反馈（如点赞/点踩）以帮助迭代与修正Agent。\n",
    "4. **LLM 评审（LLM-as-a-Judge）**：使用额外的 LLM 近实时评估Agent输出（如检测毒性或正确性）。\n",
    "\n",
    "下面展示这些指标的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHMvJ1QlfoSQ"
   },
   "source": [
    "#### 1. 成本（Costs）\n",
    "\n",
    "下图展示了 `gpt-4o` 调用的用量，可据此识别高成本步骤并优化Agent。\n",
    "\n",
    "![成本](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251732570.png)\n",
    "\n",
    "_[前往该追踪](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=2d1f23b960fb1ff0bdaf7623fda4936c&timestamp=2025-09-25T09%3A06%3A06.476Z)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz0y9mn7foSQ"
   },
   "source": [
    "#### 2. 延迟（Latency）\n",
    "\n",
    "还可以查看完成每个步骤所需的时间。如下例所示，整个运行约 3 秒，你可以细分到各步骤。此举有助于识别瓶颈并优化Agent。\n",
    "\n",
    "![延迟](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251735069.png)\n",
    "\n",
    "_[前往该追踪](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=2d1f23b960fb1ff0bdaf7623fda4936c&timestamp=2025-09-25T09%3A06%3A06.476Z&display=timeline)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtKiK62HfoSR"
   },
   "source": [
    "#### 3. 用户反馈（User Feedback）\n",
    "\n",
    "如果你的Agent嵌入在用户界面中，可以采集用户的直接反馈（例如在聊天界面中的点赞/点踩）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "YI9siKKKfoSR"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# ✅ 方式一：使用上下文管理器返回的 span 对象给当前追踪打分\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"LangGraph\") as span:\n",
    "    # ... 在这里执行具体的 LangGraph 逻辑 ...\n",
    "\n",
    "    # 直接对 span 调用 score_trace 并附加补充信息\n",
    "    span.score_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\",\n",
    "        comment=\"This was correct, thank you\"\n",
    "    )\n",
    "\n",
    "# ✅ 方式二：仍在上下文中时，可使用 score_current_trace 简化调用\n",
    "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    "\n",
    "    # 使用当前上下文的 trace，而无需持有 span 对象\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\"\n",
    "    )\n",
    "\n",
    "# ✅ 方式三：如果已经离开上下文，也可以通过 trace_id 进行补录\n",
    "langfuse.create_score(\n",
    "    trace_id=\"predefined-trace-id\",  # ⚠️ 这里需要替换成真实的 trace_id\n",
    "    name=\"user-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"This was correct, thank you\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiemuS7YfoSR"
   },
   "source": [
    "用户反馈随后会被 Langfuse 捕获：\n",
    "\n",
    "![Langfuse 中捕获的用户反馈](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251746651.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29KsI9xcfoSR"
   },
   "source": [
    "#### 4. 自动化的 LLM 评审打分（LLM-as-a-Judge）\n",
    "\n",
    "LLM-as-a-Judge 提供了一种自动评估Agent输出的方法。你可以**配置一个独立的 LLM 调用**，用于评估输出的正确性、毒性、风格或其他你关心的指标。\n",
    "\n",
    "**工作流程：**\n",
    "1. 定义一个**评估模板**，例如“检查文本是否含有毒性”。\n",
    "2. 指定用于评审的模型（judge-model），例如 `gpt-4o-mini`。\n",
    "2. 每当Agent生成输出时，将其与模板一起传给“评审”LLM。\n",
    "3. 评审 LLM 给出评分或标签，并将结果记录到可观测性平台。\n",
    "\n",
    "Langfuse 示例：\n",
    "\n",
    "![LLM 评审模板](https://langfuse.com/images/cookbook/integration_openai-agents/evaluator-template.png)\n",
    "![LLM 评审器](https://langfuse.com/images/cookbook/integration_openai-agents/evaluator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "UGGlYrB7foSR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spam email...\n",
      "阿尔弗雷德正在处理来自 某数字货币项目方 的邮件，主题为：限时暴涨100倍，立即上车！\n",
      "spam\n",
      "阿尔弗雷德已经将邮件标记为垃圾邮件。\n",
      "该电子邮件已被移至垃圾邮件文件夹。\n"
     ]
    }
   ],
   "source": [
    "# 🔁 如果需要单独再次验证垃圾邮件路径，可以复用下面的调用代码\n",
    "print(\"Processing spam email...\")\n",
    "spam_result = compiled_graph.invoke(\n",
    "    input={\n",
    "        \"email\": spam_email,\n",
    "        \"is_spam\": None,\n",
    "        \"draft_response\": None,\n",
    "        \"messages\": []\n",
    "        },\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izr-3LiQfoSR"
   },
   "source": [
    "可以看到，该示例的答案被评审为“无毒性（not toxic）”。\n",
    "\n",
    "![LLM 评审得分示例](https://langfuse.com/images/cookbook/example-langgraph-evaluation/llm-as-a-judge-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7fN0UTkfoSR"
   },
   "source": [
    "#### 5. 可观测性指标总览\n",
    "\n",
    "所有上述指标都可以在统一的仪表盘中可视化。这样你可以快速查看Agent在多次会话中的表现，并随时间跟踪质量指标。\n",
    "\n",
    "![可观测性指标总览](https://langfuse.com/images/cookbook/integration_openai-agents/dashboard-dark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlwltgEkfoSR"
   },
   "source": [
    "## 离线评估（Offline Evaluation）\n",
    "\n",
    "在线评估可用于获取实时反馈，但同样需要进行**离线评估（offline evaluation）**——即在开发前或开发过程中进行系统性的检查。这样可以在发布变更到生产环境之前，保障质量与可靠性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5R8eNQxfoSR"
   },
   "source": [
    "### 数据集评估（Dataset Evaluation）\n",
    "\n",
    "在离线评估中，通常会：\n",
    "1. 准备一个基准数据集（包含提示词与期望输出的成对样本）\n",
    "2. 使用该数据集批量运行你的 Agent\n",
    "3. 将模型输出与期望结果进行比较，或采用额外的自动打分机制\n",
    "\n",
    "下面我们使用一个问答数据集示例：[q&a-dataset](https://huggingface.co/datasets/junzhang1207/search-dataset)，其中包含问题与期望答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOAP8e45wIsp",
    "outputId": "b2807d06-e184-43b7-cb94-e2e2ef8e40af"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 设置HuggingFace代理\u001b[39;00m\n\u001b[32m      5\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33menv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHF_ENDPOINT=https://hf-mirror.com\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 设置HuggingFaceAgent\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "# 📥 从 Hugging Face 下载示例数据集，这里包含问答形式的条目\n",
    "dataset = load_dataset(\"junzhang1207/search-dataset\", split=\"train\")\n",
    "df = pd.DataFrame(dataset)  # 转成 DataFrame 方便筛选与遍历\n",
    "print(\"First few rows of search-dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功读取本地 JSONL 文件: dataset/data.jsonl\n",
      "\n",
      "数据集前5行:\n",
      "                                     id  \\\n",
      "0  20caf138-0c81-4ef9-be60-fe919e0d68d4   \n",
      "1  1f37d9fd-1bcc-4f79-b004-bc0e1e944033   \n",
      "2  76173a7f-d645-4e3e-8e0d-cca139e00ebe   \n",
      "3  5f5ef4ca-91fe-4610-a8a9-e15b12e3c803   \n",
      "4  64dbed0d-d91b-4acd-9a9c-0a7aa83115ec   \n",
      "\n",
      "                                            question  \\\n",
      "0                 steve jobs statue location budapst   \n",
      "1  Why is the Battle of Stalingrad considered a t...   \n",
      "2  In what year did 'The Birth of a Nation' surpa...   \n",
      "3  How many Russian soldiers surrendered to AFU i...   \n",
      "4   What event led to the creation of Google Images?   \n",
      "\n",
      "                                     expected_answer       category       area  \n",
      "0  The Steve Jobs statue is located in Budapest, ...           Arts  Knowledge  \n",
      "1  The Battle of Stalingrad is considered a turni...   General News       News  \n",
      "2  This question is based on a false premise. 'Th...  Entertainment       News  \n",
      "3  About 300 Russian soldiers surrendered to the ...   General News       News  \n",
      "4  Jennifer Lopez's appearance in a green Versace...     Technology       News  \n",
      "\n",
      "数据集形状: (929, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 📌 替换为你本地文件的实际路径，例如：'dataset/data.jsonl '\n",
    "local_jsonl_path = 'dataset/data.jsonl'\n",
    "\n",
    "if os.path.exists(local_jsonl_path):\n",
    "    try:\n",
    "        # 使用 pd.read_json 并指定 lines=True 来读取 JSON Lines 格式\n",
    "        local_df = pd.read_json(local_jsonl_path, lines=True)\n",
    "        \n",
    "        print(f\"✅ 成功读取本地 JSONL 文件: {local_jsonl_path}\")\n",
    "        print(\"\\n数据集前5行:\")\n",
    "        print(local_df.head())\n",
    "        print(f\"\\n数据集形状: {local_df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 读取文件时发生错误: {e}\")\n",
    "else:\n",
    "    print(f\"文件不存在，请检查路径是否正确: {local_jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlgYY3VmwIsp"
   },
   "source": [
    "接下来，我们在 Langfuse 中创建一个数据集实体以追踪运行；随后将数据集中的每条记录添加到系统中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdn2qSCwwIsp",
    "outputId": "ef3bb5d3-21bc-416a-aef3-5dccfedd2b05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(id='cmg0ap0r900z9ad07gwjr3y2r', name='qa-dataset_langgraph-agent', description='从Hugging Face上传的问答数据集', metadata={'date': '2025-09-21', 'type': 'benchmark'}, project_id='cmequpe0j00euad07w6wrvkzg', created_at=datetime.datetime(2025, 9, 26, 3, 41, 31, 29000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 30, 9, 31, 49, 357000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()\n",
    "\n",
    "langfuse_dataset_name = \"qa-dataset_langgraph-agent\"\n",
    "\n",
    "# 🗂️ 在 Langfuse 中创建一个新的数据集，用于存储评测样本\n",
    "langfuse.create_dataset(\n",
    "    name=langfuse_dataset_name,\n",
    "    description=\"从Hugging Face上传的问答数据集\",\n",
    "    metadata={\n",
    "        \"date\": \"2025-09-21\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_BkN1d1QwIsq"
   },
   "outputs": [],
   "source": [
    "# 🎯 仅选取 30 条示例数据上传，实际项目可根据需求调整\n",
    "df_30 = local_df.sample(30)\n",
    "\n",
    "for idx, row in df_30.iterrows():\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=langfuse_dataset_name,\n",
    "        input={\"text\": row[\"question\"]},            # Langfuse 需要明确的输入字段\n",
    "        expected_output={\"text\": row[\"expected_answer\"]}  # 提供标准答案便于后续评估\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3yJVaBnwIsq"
   },
   "source": [
    "![Langfuse 中的数据集条目](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509261143566.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7NHQ4XowIsq"
   },
   "source": [
    "#### 在数据集上运行Agent\n",
    "\n",
    "首先，构建一个使用 OpenAI 模型回答问题的简易 LangGraph Agent。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ACoBDVzbwIsq"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage  # 如需自定义输入消息可以使用该类型\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# 🧱 定义状态结构：messages 字段会自动累积对话历史\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 初始化一个新的状态图构建器\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 准备要调用的 OpenAI 聊天模型\n",
    "llm = ChatOpenAI(model=\"gpt-4.5-preview\")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    单节点聊天机器人：\n",
    "    - 将当前所有消息传给 LLM\n",
    "    - 返回模型的回复，LangGraph 会自动把它追加到状态里\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 🔗 注册节点与入口、出口\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ⚙️ compile() 会返回可直接调用的图实例\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXSe2ScIwIsq"
   },
   "source": [
    "接着，我们定义一个辅助函数 `my_agent()`，其职责是：\n",
    "1. 创建一个 Langfuse 追踪（trace）\n",
    "2. 获取 `langfuse_handler_trace`，用于为 LangGraph 的执行过程打点\n",
    "3. 运行我们的 Agent，并在调用时传入 `langfuse_handler_trace` 以记录执行细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZQTQKusMwIsq"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langfuse import get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate  # 可用于自定义提示模板（本示例暂未使用）\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 构建一个带 Langfuse 追踪能力的 LangGraph Agent\n",
    "graph_builder = StateGraph(State)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")  # 选择对话模型\n",
    "langfuse = get_client()  # 复用前面配置好的 Langfuse 客户端\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    核心节点：将对话历史交给 LLM，并把生成结果包装成 LangGraph 需要的格式。\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "def my_agent(question, langfuse_handler):\n",
    "    \"\"\"\n",
    "    对外暴露的便捷函数：\n",
    "    1. 打开一个 Langfuse span 以便观测这次请求；\n",
    "    2. 调用 LangGraph Agent获取回答；\n",
    "    3. 将输入输出写回 Langfuse，方便后续评估。\n",
    "    \"\"\"\n",
    "\n",
    "    # 创建一个顶层追踪 span，所有上下文都会记录在这里\n",
    "    with langfuse.start_as_current_span(name=\"my-langgraph-agent\") as root_span:\n",
    "\n",
    "        # Step 2: LangChain processing\n",
    "        response = graph.invoke(\n",
    "            input={\"messages\": [HumanMessage(content=question)]},\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "\n",
    "        # 将原始问题和模型回答同步到 Langfuse 仪表盘\n",
    "        root_span.update_trace(\n",
    "            input=question,\n",
    "            output=response[\"messages\"][1].content)\n",
    "\n",
    "        print(question)\n",
    "        print(response[\"messages\"][1].content)\n",
    "\n",
    "    return response[\"messages\"][1].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iri3PYQwIsq"
   },
   "source": [
    "最后，我们遍历数据集中的每一条样本，运行Agent，并将生成的追踪与该数据集条目进行关联。如有需要，还可以附加一个快速的评估分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJyw6m61wIsq",
    "outputId": "7115f780-5ce4-49c2-c281-40cd7bcd76bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'first edition of how to win friends and influence people published in 1932'}\n",
      "The first edition of \"How to Win Friends and Influence People,\" written by Dale Carnegie, was actually published in 1936, not 1932. This seminal self-help book provides practical advice on interpersonal skills and effective communication to improve one's ability to connect with others and enhance personal and professional relationships.\n",
      "{'text': 'kamala harris 2024 vp pick'}\n",
      "As of now, Vice President Kamala Harris is the incumbent Vice President serving under President Joe Biden. For the 2024 presidential election, it is expected that she would continue to be the Vice Presidential candidate if President Biden runs for re-election. However, it's important to note that political landscapes can change, and official announcements or decisions regarding the 2024 election have not been made publicly. For the most up-to-date information, you should follow news sources or official announcements from the political parties involved.\n",
      "{'text': 'what is the karman line definition?'}\n",
      "The Kármán line is traditionally defined as the boundary between Earth's atmosphere and outer space. It is situated at an altitude of 100 kilometers (about 62 miles) above sea level. The line is named after Theodore von Kármán, a Hungarian-American engineer and physicist, who calculated that above this altitude, the atmosphere becomes too thin to support conventional aircraft flight because the necessary orbital speed would exceed the capabilities of these aircraft. As such, the Kármán line is often used to distinguish between aeronautics and astronautics and is recognized by various organizations, including the Fédération Aéronautique Internationale (FAI), which sets standards for air sports and aeronautical records.\n",
      "{'text': 'What is the goal of the Quagga Project?'}\n",
      "The goal of the Quagga Project is to bring back animals that closely resemble the extinct quagga, a subspecies of the plains zebra. Quaggas once lived in South Africa but were driven to extinction in the late 19th century due to excessive hunting. The project involves selective breeding of plains zebras that have similar striping patterns to the historical quagga, aiming to recreate the appearance and certain behaviors of the original subspecies. While the animals produced by the project are not genetically identical to the extinct quagga, they represent an effort to restore a similar ecological niche in the region.\n",
      "{'text': \"In Amy Millan's third solo album 'Whiskey Lullabies', released between 'Honey from the Tombs' and 'Masters of the Burial', how did her experiences with Stars and Broken Social Scene influence the album's sound compared to her previous solo work?\"}\n",
      "Amy Millan's third solo album, \"Whiskey Lullabies,\" if it were to exist, would likely draw on her extensive experiences with Stars and Broken Social Scene. In her previous solo albums, \"Honey from the Tombs\" and \"Masters of the Burial,\" Millan showcased her ability to blend folk, country, and indie rock elements, crafting a distinct sound that differs from her work with those bands.\n",
      "\n",
      "Stars are known for their lush, orchestral pop sound, while Broken Social Scene is celebrated for its experimental and collaborative approach. These influences could have led \"Whiskey Lullabies\" to feature richer instrumentation and more complex arrangements compared to her earlier solo work. The album might exhibit a balance of introspective lyrics and expansive soundscapes, incorporating elements such as layered vocals, diverse instrumentation, and a blend of electronic and acoustic sounds, reflecting the collaborative spirit of Broken Social Scene.\n",
      "\n",
      "Compared to her previous solo efforts, \"Whiskey Lullabies\" would likely reflect a matured sound, integrating Millan's growth as an artist through her experiences with these bands. The album may display a nuanced emotional depth, drawing from her journey with Stars' romanticism and Broken Social Scene's raw energy and eclecticism, resulting in a compelling fusion of introspective songwriting and expansive sonic exploration.\n",
      "{'text': 'zuffa antitrust lawsuit ninth circut court decision'}\n",
      "As of my last update in October 2023, there hasn't been a decision from the Ninth Circuit Court regarding the Zuffa antitrust lawsuit. Zuffa, LLC, the parent company of the UFC, had been involved in an antitrust lawsuit where plaintiffs accused the company of engaging in anti-competitive practices to dominate the MMA market. The case was a major point of interest in the sports and legal communities, but for the most current information, you may want to look into the latest court documents or news updates.\n",
      "{'text': \"Given that Earth's rotation period relative to the fixed stars is approximately 86,164.0989 seconds, and the main apparent motion of celestial bodies in Earth's sky is to the west at a rate of 15Â°/h, how many apparent diameters of the Sun would an observer on Earth's equator see pass overhead in exactly 24 hours?\"}\n",
      "To solve this problem, we need to determine how many apparent diameters of the Sun would pass overhead as observed from Earth's equator in a 24-hour period, given the Sun's apparent motion across the sky.\n",
      "\n",
      "First, let's start with the details we have:\n",
      "\n",
      "1. The Earth's rotation period relative to the fixed stars, known as the sidereal day, is approximately 86,164.0989 seconds, or about 23 hours, 56 minutes, and 4.0989 seconds. This period represents the time it takes for the Earth to rotate 360° relative to the fixed stars.\n",
      "\n",
      "2. The main apparent motion of celestial bodies (including the Sun) in Earth's sky is to the west at a rate of 15° per hour. This is due to the Earth's rotation.\n",
      "\n",
      "3. We are considering a period of exactly 24 hours.\n",
      "\n",
      "To find how many apparent diameters of the Sun pass overhead, we also need to know the apparent angular diameter of the Sun. The average apparent angular diameter of the Sun is about 0.53 degrees.\n",
      "\n",
      "Now, we calculate the total apparent westward motion of the Sun in 24 hours:\n",
      "\n",
      "- In 24 hours, the Earth rotates 360° relative to the Sun.\n",
      "- Therefore, the apparent motion of the Sun relative to an observer on Earth’s surface is also 360° in 24 hours.\n",
      "\n",
      "Next, we calculate the number of apparent Sun diameters that fit into this 360° rotation:\n",
      "\n",
      "- We divide the total apparent motion (360°) by the angular diameter of the Sun (0.53°):\n",
      "\n",
      "  \\[\n",
      "  \\text{Number of Sun diameters} = \\frac{360°}{0.53°} \\approx 679.25\n",
      "  \\]\n",
      "\n",
      "Thus, an observer on Earth's equator would see approximately 679 apparent diameters of the Sun pass overhead in exactly 24 hours.\n",
      "{'text': 'Who awarded Steve Jobs a posthumous Medal of Freedom?'}\n",
      "Steve Jobs was posthumously awarded the Presidential Medal of Freedom by President Joe Biden.\n",
      "{'text': 'When did Abraham Lincoln arrive in Washington, D.C. as President-elect?'}\n",
      "Abraham Lincoln arrived in Washington, D.C. as President-elect on February 23, 1861.\n",
      "{'text': 'which year did simone biles win olympic gold on uneven bars'}\n",
      "Simone Biles has never won an Olympic gold medal on the uneven bars. Her strengths are typically in other events, such as floor exercise, vault, and all-around. At the 2016 Rio Olympics, she won gold in the all-around, vault, and floor exercise, but not on the uneven bars.\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 📡 初始化追踪组件：CallbackHandler 会把 LangChain 的每一步同步到 Langfuse\n",
    "langfuse_handler = CallbackHandler()\n",
    "langfuse = get_client()\n",
    "\n",
    "dataset = langfuse.get_dataset('qa-dataset_langgraph-agent')  # 获取上一步创建的数据集\n",
    "\n",
    "for item in dataset.items:\n",
    "    # ✅ item.run() 会为每个样本开启一个子追踪，方便查看单条样本的执行情况\n",
    "    with item.run(\n",
    "        run_name=\"run_gpt-4o\",\n",
    "        run_description=\"My first run\",\n",
    "        run_metadata={\"model\": \"gpt-4o\"},\n",
    "    ) as root_span:\n",
    "        # 进入此上下文的所有调用都会自动关联到当前 dataset item\n",
    "\n",
    "        # 🎯 运行核心业务逻辑时，再开一个 generation 上下文记录单次模型调用\n",
    "        with langfuse.start_as_current_generation(\n",
    "            name=\"llm-call\",\n",
    "            model=\"gpt-4o\",\n",
    "            input=item.input\n",
    "        ) as generation:\n",
    "            # 用我们刚才封装的 my_agent 完成实际问答\n",
    "            output = my_agent(str(item.input), langfuse_handler)\n",
    "            generation.update(output=output)\n",
    "\n",
    "        # 📝 可选择对结果打分（例如人工点评或自动指标）\n",
    "        root_span.score_trace(\n",
    "            name=\"user-feedback\",\n",
    "            value=1,\n",
    "            comment=\"This is a comment\",  # 可记录评分原因，便于回溯\n",
    "        )\n",
    "\n",
    "# 🔚 所有调用结束后刷新客户端，确保缓冲区里的数据都被发送\n",
    "langfuse.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rofi0MnywIsq"
   },
   "source": [
    "你可以在不同的 Agent 配置之间重复这一流程，例如：\n",
    "- 模型（如 gpt-4o-mini、o1 等）\n",
    "- 提示词（Prompts）\n",
    "- 工具（如是否启用搜索能力）\n",
    "- Agent 复杂度（多Agent vs 单Agent）\n",
    "\n",
    "随后可在 Langfuse 中进行并排对比。在此示例中，我们在 30 条数据集问题上分别运行了 3 次Agent，每次使用不同的 OpenAI 模型。可以看到，随着模型能力增大，正确回答的数量按预期提升。`correct_answer` 分数由一个[“模型充当评审”（LLM-as-a-Judge）评估器](https://langfuse.com/docs/scores/model-based-evals)生成，它会基于数据集中给出的参考答案来评估输出是否正确。\n",
    "\n",
    "![数据集运行概览](https://langfuse.com/images/cookbook/example-langgraph-evaluation/dataset_runs.png)\n",
    "![数据集运行对比](https://langfuse.com/images/cookbook/example-langgraph-evaluation/dataset-run-comparison.png)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
