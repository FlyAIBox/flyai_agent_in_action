{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {},
   "source": [
    "# 🔧 环境配置和检查\n",
    "\n",
    "## 概述\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助您：\n",
    "- 使用统一的conda环境\n",
    "- 通过国内镜像源快速安装依赖\n",
    "- 加速模型下载\n",
    "- 检查系统配置\n",
    "\n",
    "## 配置步骤\n",
    "1. **Conda环境管理** - 激活统一的学习环境\n",
    "2. **包管理器优化** - 配置pip使用清华镜像源\n",
    "3. **模型下载加速** - 设置HuggingFace镜像代理\n",
    "4. **系统环境诊断** - 检查硬件和软件配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 激活conda环境\n",
    "%%script bash\n",
    "# 初始化 conda\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate flyai_agent_in_action\n",
    "conda env list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助您：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FlyAIBox/AIAgent101/blob/main/06-agent-evaluation/langfuse/04_example_langgraph_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2_SkHQMwIsZ"
   },
   "source": [
    "<!-- NOTEBOOK_METADATA source: \"Jupyter Notebook\" title: \"Example - Trace and Evaluate LangGraph Agents\" description: \"This guide shows how to evaluate LangGraph Agents with Langfuse using online and offline evaluation methods.\" category: \"Integrations\" -->\n",
    "\n",
    "# LangGraph 代理追踪与评估完整指南\n",
    "\n",
    "## 📖 教程概述\n",
    "\n",
    "在本教程中，我们将深入学习如何使用 [Langfuse](https://langfuse.com)（一个强大的大模型可观测性平台）与 [Hugging Face Datasets](https://huggingface.co/datasets)，来**全面监控 [LangGraph 代理](https://github.com/langchain-ai/langgraph) 的执行过程（traces）**并**科学评估其性能表现**。\n",
    "\n",
    "## 🎯 学习目标\n",
    "\n",
    "本指南将帮助您掌握将 AI Agent快速且可靠地部署到生产环境所需的核心技能：\n",
    "- **在线评估**：实时监控生产环境中的代理表现\n",
    "- **离线评估**：使用基准数据集进行系统性测试\n",
    "\n",
    "\n",
    "## 🔍 为什么 AI Agent评估如此重要？\n",
    "\n",
    "在 AI Agent开发过程中，评估是确保系统质量的关键环节：\n",
    "\n",
    "- **🐛 问题诊断**：当代理任务执行失败或结果不理想时，能够快速定位问题根源\n",
    "- **📊 性能监控**：实时追踪系统的成本消耗、响应延迟等关键指标\n",
    "- **🔄 持续改进**：通过用户反馈和评估数据，不断提升代理的可靠性与安全性\n",
    "- **🚀 生产就绪**：确保代理在真实环境中能够稳定运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzPbsmLrfoSN"
   },
   "source": [
    "## 🛠️ 步骤 0：环境准备与依赖安装\n",
    "\n",
    "### 📦 安装核心依赖库\n",
    "\n",
    "在开始本教程之前，我们需要安装以下核心库：\n",
    "\n",
    "- **`langgraph`**：用于构建多节点、状态驱动的 AI Agent工作流\n",
    "- **`langfuse`**：提供大模型应用的可观测性和评估功能  \n",
    "- **`langchain`** 系列：用于 LLM 应用开发的核心框架\n",
    "- **`datasets`**：Hugging Face 的数据集处理库\n",
    "\n",
    "\n",
    "<!-- CALLOUT_START type: \"info\" emoji: \"⚠️\" -->\n",
    "**📌 重要提示：**\n",
    "- 本教程使用 **Langfuse Python SDK v3**，它提供了更好的性能和新特性\n",
    "- 建议在虚拟环境中运行本教程以避免依赖冲突\n",
    "<!-- CALLOUT_END -->\n",
    "\n",
    "### 🔧 环境要求\n",
    "\n",
    "- **Python 版本**：3.8 或更高版本\n",
    "- **操作系统**：支持 Windows、macOS、Linux\n",
    "- **网络**：需要访问 OpenAI API 和 Langfuse 服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqXWStafwIsd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "_EI_0ZfzfoSO",
    "outputId": "d4418732-c65a-41cd-a7b3-931838ea85d7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langfuse==3.3.0\n",
      "  Downloading langfuse-3.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langchain==0.3.27 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
      "Collecting langgraph==0.6.7\n",
      "  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain-openai==0.3.31\n",
      "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain_community==0.3.27\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Collecting backoff>=1.10.0 (from langfuse==3.3.0)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.15.4 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 (from langfuse==3.3.0)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.37.0)\n",
      "Requirement already satisfied: packaging<26.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0,>=1.10.7 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.11.9)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (2.32.4)\n",
      "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.12/dist-packages (from langfuse==3.3.0) (1.17.3)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (0.4.28)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (2.0.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.3.27) (6.0.2)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph==0.6.7)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph==0.6.7)\n",
      "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph==0.6.7)\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph==0.6.7) (3.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.31) (1.108.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai==0.3.31) (0.11.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (8.5.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community==0.3.27)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community==0.3.27) (2.0.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.35.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community==0.3.27) (1.20.1)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (4.10.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.15.4->langfuse==3.3.0) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.15.4->langfuse==3.3.0) (0.16.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (2025.3.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (1.1.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (1.33)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph==0.6.7)\n",
      "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph==0.6.7) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain==0.3.27) (0.25.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (0.11.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai==0.3.31) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (8.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (1.70.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0)\n",
      "  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse==3.3.0) (5.29.5)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse==3.3.0) (0.58b0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.10.7->langfuse==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community==0.3.27) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langfuse==3.3.0) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.27) (3.2.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.3.31) (2024.11.6)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse==3.3.0) (3.23.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain==0.3.27) (3.0.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community==0.3.27)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langfuse-3.3.0-py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.37.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: ormsgpack, opentelemetry-proto, mypy-extensions, marshmallow, backoff, typing-inspect, opentelemetry-exporter-otlp-proto-common, langgraph-sdk, dataclasses-json, opentelemetry-exporter-otlp-proto-http, langgraph-checkpoint, langchain-openai, langchain_huggingface, langgraph-prebuilt, langfuse, langgraph, langchain_community\n",
      "Successfully installed backoff-2.2.1 dataclasses-json-0.6.7 langchain-openai-0.3.31 langchain_community-0.3.27 langchain_huggingface-0.3.1 langfuse-3.3.0 langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 marshmallow-3.26.1 mypy-extensions-1.1.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-exporter-otlp-proto-http-1.37.0 opentelemetry-proto-1.37.0 ormsgpack-1.10.0 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "# 📦 安装所需的Python包\n",
    "# 使用魔法命令 %pip 在Jupyter环境中安装依赖库\n",
    "%pip install langfuse==3.3.0 langchain==0.3.27 langgraph==0.6.7 langchain-openai==0.3.31 langchain_community==0.3.27 langchain_huggingface\n",
    "\n",
    "# 各库功能说明：\n",
    "# - langfuse: LLM应用的可观测性和评估平台\n",
    "# - langchain: 大语言模型应用开发框架\n",
    "# - langgraph: 基于langchain的图形化工作流构建工具\n",
    "# - langchain_openai: OpenAI模型的langchain集成\n",
    "# - langchain_community: 社区贡献的langchain扩展\n",
    "# - langchain_huggingface: Hugging Face模型的langchain集成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHRsxz1VfoSP"
   },
   "source": [
    "## 🔑 步骤 1：配置 API 密钥和环境变量\n",
    "\n",
    "### 获取 Langfuse API 密钥\n",
    "\n",
    "在开始使用 Langfuse 之前，您需要获取 API 访问凭证：\n",
    "\n",
    "#### 方案一：使用 Langfuse Cloud（推荐）\n",
    "1. 访问 [Langfuse Cloud](https://cloud.langfuse.com) 并注册账户\n",
    "2. 创建新项目或选择现有项目\n",
    "3. 在项目设置页面获取以下密钥：\n",
    "   - `LANGFUSE_PUBLIC_KEY`：以 `pk-lf-` 开头的公钥\n",
    "   - `LANGFUSE_SECRET_KEY`：以 `sk-lf-` 开头的私钥\n",
    "\n",
    "#### 方案二：自托管 Langfuse\n",
    "如果您选择自托管部署，请按照 [Langfuse 自托管文档](https://langfuse.com/docs/deployment/self-host) 进行配置。\n",
    "\n",
    "### 获取 OpenAI API 密钥\n",
    "\n",
    "1. 访问 [OpenAI 平台](https://platform.openai.com/)\n",
    "2. 注册账户并完成身份验证\n",
    "3. 在 API 密钥页面创建新的 API 密钥\n",
    "4. 确保账户有足够的余额用于 API 调用\n",
    "\n",
    "### 🔐 安全提醒\n",
    "\n",
    "- **请勿将 API 密钥硬编码在代码中**\n",
    "- **生产环境建议使用环境变量或密钥管理系统**\n",
    "- **定期轮换密钥以提高安全性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mZnxtWx9foSP",
    "outputId": "cbabe0f0-49e2-4500-8b9a-b43baaa5cc17",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ··········\n",
      "OPENAI_BASE_URL: ··········\n",
      "LANGFUSE_PUBLIC_KEY: ··········\n",
      "LANGFUSE_SECRET_KEY: ··········\n",
      "LANGFUSE_HOST: ··········\n"
     ]
    }
   ],
   "source": [
    "# 🔐 环境变量配置 - 安全存储敏感信息\n",
    "# 环境变量是存储API密钥等敏感信息的最佳实践\n",
    "# 避免在代码中硬编码密钥，防止泄露\n",
    "\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    安全地设置环境变量\n",
    "    如果环境变量不存在，会提示用户输入\n",
    "    使用getpass模块隐藏输入内容，防止密码泄露\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# 🤖 OpenAI API 配置\n",
    "# OpenAI API密钥：从 https://platform.openai.com/api-keys 获取\n",
    "# 这是调用GPT模型必需的认证信息\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "\n",
    "# API代理地址：如果你使用第三方代理服务（如国内代理）\n",
    "# 示例：https://api.apiyi.com/v1\n",
    "# 如果直接使用OpenAI官方API，可以留空\n",
    "_set_env(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 🌐 Langfuse 配置\n",
    "# Langfuse是一个可观测性平台，需要注册账户获取密钥\n",
    "# 注册地址：https://cloud.langfuse.com\n",
    "\n",
    "# 公开密钥：用于标识你的项目\n",
    "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
    "\n",
    "# 秘密密钥：用于认证，请妥善保管\n",
    "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
    "\n",
    "# 服务器地址：选择离你最近的区域\n",
    "# 🇪🇺 欧盟区域(推荐) https://cloud.langfuse.com\n",
    "# 🇺🇸 美国区域 https://us.cloud.langfuse.com\n",
    "_set_env(\"LANGFUSE_HOST\")\n",
    "\n",
    "# 💡 初学者提示：\n",
    "# 1. 环境变量存储在操作系统中，重启后需要重新设置\n",
    "# 2. 生产环境中建议使用.env文件或云服务配置\n",
    "# 3. 永远不要在代码中硬编码API密钥！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJjyA41_wIsi"
   },
   "source": [
    "### 🔗 连接验证与客户端初始化\n",
    "\n",
    "设置完环境变量后，我们需要初始化 Langfuse 客户端并验证连接。\n",
    "\n",
    "**核心概念解释：**\n",
    "- **`get_client()`**：Langfuse 提供的便捷函数，会自动读取环境变量中的凭证\n",
    "- **客户端实例**：用于与 Langfuse 服务器通信的对象\n",
    "- **连接验证**：确保 API 密钥正确且网络连接正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvQRomm-wIsi",
    "outputId": "0fb60071-bc52-4d90-a487-5cb0bf3ff3c3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Langfuse 客户端连接成功！API 认证通过\n",
      "🎯 现在可以开始追踪和评估 LLM 应用了\n"
     ]
    }
   ],
   "source": [
    "# 📡 导入 Langfuse 客户端并建立连接\n",
    "from langfuse import get_client\n",
    "\n",
    "# 🔧 初始化 Langfuse 客户端\n",
    "# get_client() 会自动从环境变量中读取 API 凭证\n",
    "langfuse = get_client()\n",
    "\n",
    "# ✅ 验证 API 连接和身份认证\n",
    "# auth_check() 方法会测试与 Langfuse 服务器的连接\n",
    "if langfuse.auth_check():\n",
    "    print(\"✅ Langfuse 客户端连接成功！API 认证通过\")\n",
    "    print(\"🎯 现在可以开始追踪和评估 LLM 应用了\")\n",
    "else:\n",
    "    print(\"❌ 认证失败！请检查以下项目：\")\n",
    "    print(\"   1. API 密钥是否正确设置\")\n",
    "    print(\"   2. 服务器地址是否正确\")\n",
    "    print(\"   3. 网络连接是否正常\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uulS5iGHfoSP"
   },
   "source": [
    "## 🧪 步骤 2：构建第一个 LangGraph 代理并验证追踪功能\n",
    "\n",
    "### 💡 什么是追踪（Tracing）？\n",
    "\n",
    "在 LLM 应用开发中，**追踪（Tracing）** 是指记录应用程序执行过程中的详细信息：\n",
    "- **执行路径**：代理执行了哪些步骤\n",
    "- **性能指标**：每个步骤的耗时、令牌消耗等\n",
    "- **输入输出**：每个环节的输入和输出内容\n",
    "- **错误信息**：出现问题时的详细错误日志\n",
    "\n",
    "### 🎯 本节目标\n",
    "\n",
    "我们将创建一个简单的问答代理来验证 Langfuse 追踪功能是否正常工作。\n",
    "\n",
    "**技术要点：**\n",
    "- 使用 **LangGraph** 构建状态驱动的代理工作流\n",
    "- 通过 **CallbackHandler** 实现自动追踪\n",
    "- 在 Langfuse 仪表板中查看执行记录\n",
    "\n",
    "🔍 **运行成功标志**：如果配置正确，您将在 [Langfuse 追踪仪表板](https://cloud.langfuse.com/traces) 中看到详细的执行日志和性能指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcyynS9CfoSP",
    "outputId": "9092ae17-ff79-442e-d274-2de3e921fd41"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 简单问答代理构建完成！\n",
      "🔧 代理架构：输入 → ChatBot节点 → 输出\n",
      "📝 支持功能：基本问答、上下文理解\n"
     ]
    }
   ],
   "source": [
    "# 🚀 构建简单的 LangGraph 问答代理\n",
    "\n",
    "# 📦 导入必要的类型和工具\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# 🔧 定义代理的状态结构\n",
    "class State(TypedDict):\n",
    "    # messages 字段存储对话历史，类型为列表\n",
    "    # Annotated[list, add_messages] 定义了状态更新的方式：\n",
    "    # - list: 数据类型为列表\n",
    "    # - add_messages: 更新时追加消息而不是覆盖（保持对话历史）\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 创建状态图构建器\n",
    "# StateGraph 是 LangGraph 的核心类，用于构建状态驱动的工作流\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 初始化 OpenAI 语言模型\n",
    "# - model: 使用 GPT-4o 模型（性能强大且成本适中）\n",
    "# - temperature: 设置为 0.2，输出相对稳定但保持一定创造性\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\n",
    "\n",
    "# 💬 定义聊天机器人节点函数\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    聊天机器人的核心逻辑\n",
    "\n",
    "    参数:\n",
    "        state: 当前的对话状态，包含消息历史\n",
    "\n",
    "    返回:\n",
    "        包含新消息的字典，会被自动合并到状态中\n",
    "    \"\"\"\n",
    "    # 调用 LLM 处理当前所有消息，并返回回复\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 🔗 构建工作流图结构\n",
    "# 1. 添加节点：每个节点代表一个工作单元（通常是 Python 函数）\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# 2. 设置入口点：告诉图从哪个节点开始执行\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "\n",
    "# 3. 设置结束点：定义工作流的终止条件\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ⚙️ 编译图以获得可执行的代理\n",
    "# compile() 方法将图定义转换为可运行的 CompiledGraph 对象\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"✅ 简单问答代理构建完成！\")\n",
    "print(\"🔧 代理架构：输入 → ChatBot节点 → 输出\")\n",
    "print(\"📝 支持功能：基本问答、上下文理解\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLXIdYuwwIsi",
    "outputId": "38e19edf-0627-4974-88b2-5ec7fa350611"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "🚀 开始运行代理并启用 Langfuse 追踪...\n",
      "❓ 用户问题：Langfuse是什么，应用场景是?\n",
      "📊 追踪信息将发送到 Langfuse 平台\n",
      "--------------------------------------------------\n",
      "📥 代理执行步骤: {'chatbot': {'messages': [AIMessage(content='Langfuse是一种用于监控和调试生成式AI应用程序的工具。它旨在帮助开发者更好地理解和优化他们的AI模型和应用程序的性能。Langfuse可以提供关于AI模型的详细分析，包括其生成的内容、响应时间、错误率等关键指标。这些信息对于开发者来说非常重要，因为它们可以帮助识别潜在的问题和优化模型的表现。\\n\\n应用场景包括：\\n\\n1. **性能监控**：实时监控AI模型的性能，识别瓶颈和异常情况，以便及时进行调整和优化。\\n\\n2. **错误调试**：提供详细的错误报告和日志，帮助开发者快速定位问题的根源并进行修复。\\n\\n3. **用户体验优化**：通过分析用户交互数据，帮助开发者优化AI应用的用户体验，提高用户满意度。\\n\\n4. **模型评估**：比较不同模型的性能，选择最佳的模型进行部署。\\n\\n5. **数据分析**：收集和分析生成式AI的输出数据，以便进行进一步的研究和开发。\\n\\n总之，Langfuse是一个强大的工具，可以帮助开发者提高生成式AI应用的质量和效率。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 260, 'prompt_tokens': 17, 'total_tokens': 277, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_8458c98457', 'id': 'chatcmpl-CJtFSzEbJXMpYq2N5EPggyEGY3iet', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--bd9bdcb3-d422-41c4-85c5-0fcb983590cf-0', usage_metadata={'input_tokens': 17, 'output_tokens': 260, 'total_tokens': 277, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "--------------------------------------------------\n",
      "✅ 代理执行完成！\n",
      "🔗 请访问 Langfuse 仪表板查看详细追踪信息\n",
      "📍 链接: https://cloud.langfuse.com/traces\n"
     ]
    }
   ],
   "source": [
    "# 🔍 启用 Langfuse 追踪并运行代理\n",
    "\n",
    "# 📡 导入 Langfuse 的 LangChain 回调处理器\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 🎯 初始化 Langfuse 追踪处理器\n",
    "# CallbackHandler 会自动捕获 LangChain/LangGraph 的执行信息\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "print(\"🚀 开始运行代理并启用 Langfuse 追踪...\")\n",
    "print(\"❓ 用户问题：Langfuse是什么，应用场景是?\")\n",
    "print(\"📊 追踪信息将发送到 Langfuse 平台\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 🏃 运行代理并启用追踪\n",
    "# stream() 方法允许实时接收代理的执行结果\n",
    "for step_result in graph.stream(\n",
    "    # 输入：包含用户消息的状态\n",
    "    {\"messages\": [HumanMessage(content=\"Langfuse是什么，应用场景是?\")]},\n",
    "    # 配置：启用 Langfuse 回调处理器进行追踪\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    "):\n",
    "    print(f\"📥 代理执行步骤: {step_result}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"✅ 代理执行完成！\")\n",
    "print(\"🔗 请访问 Langfuse 仪表板查看详细追踪信息\")\n",
    "print(\"📍 链接: https://cloud.langfuse.com/traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPLt1hRkfoSQ"
   },
   "source": [
    "### 🔍 验证追踪功能：查看 Langfuse 仪表板\n",
    "\n",
    "#### 📊 如何检查追踪记录\n",
    "\n",
    "运行上述代码后，请按以下步骤验证追踪功能：\n",
    "\n",
    "1. **访问仪表板**：打开 [Langfuse 追踪仪表板](https://cloud.langfuse.com/traces)\n",
    "2. **查找记录**：在追踪列表中找到刚才的执行记录\n",
    "3. **分析数据**：点击记录查看详细的执行信息\n",
    "\n",
    "#### 🔬 追踪记录包含什么信息？\n",
    "\n",
    "在 Langfuse 中，您将看到以下重要信息：\n",
    "\n",
    "- **📝 Spans（跨度）**：每个执行步骤的详细记录\n",
    "- **📋 Logs（日志）**：执行过程中的日志信息  \n",
    "- **⏱️ 时间戳**：每个步骤的精确执行时间\n",
    "- **💰 成本信息**：API 调用的令牌消耗和费用\n",
    "- **📊 性能指标**：延迟、吞吐量等关键指标\n",
    "\n",
    "#### 📸 Langfuse 中的示例追踪截图\n",
    "\n",
    "![Langfuse 中的示例追踪](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251551391.png)\n",
    "\n",
    "💡 **小提示**：追踪记录可能需要几秒钟才能在仪表板中显示，请稍作等待。\n",
    "\n",
    "🔗 _[查看示例追踪记录](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=6efb8472addcad81fa932915e6a5eff2&timestamp=2025-09-25T07%3A48%3A04.647Z)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onjMD-ZJfoSQ"
   },
   "source": [
    "## 🔬 步骤 3：构建并观测复杂的邮件处理代理\n",
    "\n",
    "### 🎯 进阶实战：真实业务场景模拟\n",
    "\n",
    "既然已确认基础追踪功能有效，现在我们来构建一个更加复杂且贴近实际业务场景的代理系统。\n",
    "\n",
    "### 📧 业务场景：智能邮件管理助手\n",
    "\n",
    "我们将创建一个**邮件处理代理**，具备以下功能：\n",
    "\n",
    "#### 🔧 核心功能模块\n",
    "- **📬 邮件接收**：读取和解析邮件内容\n",
    "- **🔍 垃圾邮件识别**：智能判断邮件是否为垃圾邮件\n",
    "- **🗂️ 自动分类**：对合法邮件进行分类处理\n",
    "- **✍️ 回复起草**：为重要邮件生成回复草稿\n",
    "- **📢 通知主人**：向韦恩先生汇报重要邮件\n",
    "\n",
    "#### 📊 追踪的高级指标\n",
    "\n",
    "通过这个复杂代理，我们将观察以下关键指标：\n",
    "- **💰 成本追踪**：详细的令牌消耗和 API 费用\n",
    "- **⏱️ 性能分析**：每个处理步骤的耗时分布\n",
    "- **🔄 工作流路径**：代理的决策逻辑和执行路径\n",
    "- **❌ 错误监控**：异常情况的捕获和分析\n",
    "\n",
    "### 🏗️ 技术架构特点\n",
    "\n",
    "- **状态驱动**：使用 LangGraph 的状态管理机制\n",
    "- **条件分支**：根据邮件类型执行不同的处理逻辑\n",
    "- **多节点协作**：模拟真实的业务处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhZyuiZNwIsj",
    "outputId": "66f2befe-309e-4f09-d675-405a64dcebbd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "📚 库导入完成，准备构建邮件处理代理...\n",
      "🔧 即将使用的核心组件：\n",
      "   - StateGraph: 构建状态驱动的工作流\n",
      "   - ChatOpenAI: 调用 GPT 模型进行智能处理\n",
      "   - TypedDict: 定义严格的数据结构\n"
     ]
    }
   ],
   "source": [
    "# 📦 导入构建复杂代理所需的库\n",
    "\n",
    "import os  # 操作系统接口，用于环境变量管理\n",
    "from typing import TypedDict, List, Dict, Any, Optional  # 类型注解，提高代码可读性和IDE支持\n",
    "from langgraph.graph import StateGraph, START, END  # LangGraph核心组件：状态图、开始节点、结束节点\n",
    "from langchain_openai import ChatOpenAI  # OpenAI模型的LangChain集成\n",
    "from langchain_core.messages import HumanMessage  # LangChain消息类型\n",
    "\n",
    "print(\"📚 库导入完成，准备构建邮件处理代理...\")\n",
    "print(\"🔧 即将使用的核心组件：\")\n",
    "print(\"   - StateGraph: 构建状态驱动的工作流\")\n",
    "print(\"   - ChatOpenAI: 调用 GPT 模型进行智能处理\")\n",
    "print(\"   - TypedDict: 定义严格的数据结构\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KbdEGxUQwIsj",
    "outputId": "f37e46ee-8c8e-467e-8fe9-7953eeb071b6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ 邮件状态结构定义完成\n",
      "📋 状态字段说明：\n",
      "   - email: 原始邮件数据\n",
      "   - is_spam: 垃圾邮件判定结果\n",
      "   - draft_response: 回复草稿\n",
      "   - messages: LLM对话历史\n"
     ]
    }
   ],
   "source": [
    "# 🏗️ 定义邮件处理代理的状态结构\n",
    "\n",
    "class EmailState(TypedDict):\n",
    "    \"\"\"\n",
    "    邮件处理代理的状态数据结构\n",
    "\n",
    "    这个类定义了代理在处理邮件过程中需要维护的所有状态信息\n",
    "    \"\"\"\n",
    "    # 📧 原始邮件信息\n",
    "    email: Dict[str, Any]  # 包含发件人、主题、正文等邮件完整信息\n",
    "\n",
    "    # 🔍 垃圾邮件检测结果\n",
    "    is_spam: Optional[bool]  # 是否为垃圾邮件（True/False/None）\n",
    "    spam_reason: Optional[str]  # 判定为垃圾邮件的原因说明\n",
    "\n",
    "    # 🗂️ 邮件分类信息\n",
    "    email_category: Optional[str]  # 邮件类别（如：商务、个人、紧急等）\n",
    "\n",
    "    # ✍️ 回复草稿\n",
    "    draft_response: Optional[str]  # 为主人准备的回复草稿\n",
    "\n",
    "    # 💬 对话历史记录\n",
    "    messages: List[Dict[str, Any]]  # 存储处理过程中的LLM对话记录\n",
    "\n",
    "print(\"✅ 邮件状态结构定义完成\")\n",
    "print(\"📋 状态字段说明：\")\n",
    "print(\"   - email: 原始邮件数据\")\n",
    "print(\"   - is_spam: 垃圾邮件判定结果\")\n",
    "print(\"   - draft_response: 回复草稿\")\n",
    "print(\"   - messages: LLM对话历史\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sdo7y_0mwIsj",
    "outputId": "08199035-52cb-4553-cbbb-738aa5b2e4c5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7909a0f02ba0>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# ✅ 初始化大语言模型（LLM），后续所有节点都会复用它进行推理\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 🧱 在运行实际图之前再次定义状态结构，确保每个节点能拿到自己需要的数据\n",
    "class EmailState(TypedDict):\n",
    "    email: Dict[str, Any]            # 📬 当前待处理的原始邮件内容（发件人、主题、正文）\n",
    "    is_spam: Optional[bool]          # 🚨 垃圾邮件判定结果，None 表示尚未判定\n",
    "    draft_response: Optional[str]    # ✍️ Alfred 起草的回复内容\n",
    "    messages: List[Dict[str, Any]]   # 🗒️ LangChain 对话历史，用来记录模型调用\n",
    "\n",
    "# 🔁 定义工作流中的每个节点函数\n",
    "def read_email(state: EmailState):\n",
    "    \"\"\"\n",
    "    入口节点：展示邮件基础信息，帮助我们在命令行中观察流程。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]  # 从状态中取出当前邮件\n",
    "    print(f\"阿尔弗雷德正在处理来自 {email['sender']} 的邮件，主题为：{email['subject']}\")\n",
    "    return {}  # 节点只做展示，不修改状态\n",
    "\n",
    "def classify_email(state: EmailState):\n",
    "    \"\"\"\n",
    "    使用 LLM 判断当前邮件是否为垃圾邮件。\n",
    "    如果是垃圾邮件就不记录模型对话，避免污染历史。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]\n",
    "\n",
    "    # 构造提示词，向 LLM 传入邮件的所有关键信息（中文初学者友好版本）\n",
    "    prompt = f\"\"\"\n",
    "请以阿尔弗雷德（Alfred，韦恩先生的管家，同时知晓其“蝙蝠侠”身份）的视角，分析下面这封邮件，判断其是垃圾邮件（SPAM）还是正常邮件（HAM），并说明是否需要提醒韦恩先生注意。\n",
    "\n",
    "邮件内容：\n",
    "发件人（From）：{email['sender']}\n",
    "主题（Subject）：{email['subject']}\n",
    "正文（Body）：{email['body']}\n",
    "\n",
    "请先判断这封邮件是否为垃圾邮件。\n",
    "只返回一个英文单词作为最终答案：若是垃圾邮件，返回“SPAM”；若是正常邮件，返回“HAM”。不要输出多余文字。\n",
    "答案：\n",
    "    \"\"\"\n",
    "    messages = [HumanMessage(content=prompt)]  # LangChain 要求传入 HumanMessage 对象\n",
    "    response = model.invoke(messages)  # 调用 LLM 获得判定结果\n",
    "\n",
    "    response_text = response.content.lower()  # 统一转小写，便于关键词匹配\n",
    "    print(response_text)  # 在控制台输出，方便我们调试和观察\n",
    "    is_spam = \"spam\" in response_text and \"ham\" not in response_text  # 同时排除同时出现 spam/ham 的情况\n",
    "\n",
    "    if not is_spam:\n",
    "        # 如果不是垃圾邮件，就将本次问答追加到对话历史中，供后续节点使用\n",
    "        new_messages = state.get(\"messages\", []) + [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": response.content}\n",
    "        ]\n",
    "    else:\n",
    "        # 垃圾邮件无需记录上下文，保持原有的消息记录\n",
    "        new_messages = state.get(\"messages\", [])\n",
    "\n",
    "    return {\n",
    "        \"is_spam\": is_spam,       # 把垃圾邮件判定结果写回状态\n",
    "        \"messages\": new_messages  # 同步对话历史\n",
    "    }\n",
    "\n",
    "def handle_spam(state: EmailState):\n",
    "    \"\"\"\n",
    "    垃圾邮件分支：这里只演示打印提示语，真实项目可以写入数据库或报警。\n",
    "    \"\"\"\n",
    "    print(\"阿尔弗雷德已经将邮件标记为垃圾邮件。\")\n",
    "    print(\"该电子邮件已被移至垃圾邮件文件夹。\")\n",
    "    return {}  # 返回空字典表示不修改状态字段\n",
    "\n",
    "def drafting_response(state: EmailState):\n",
    "    \"\"\"\n",
    "    合法邮件分支：让 LLM 帮忙撰写一份礼貌的回复草稿。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]\n",
    "\n",
    "    # 维持提示词，明确输出语气和需要覆盖的关键内容（中文初学者友好版本）\n",
    "    prompt = f\"\"\"\n",
    "请以阿尔弗雷德（Alfred，韦恩先生的管家）的口吻，为下面这封邮件起草一份礼貌、简洁且专业的初稿回复。\n",
    "\n",
    "邮件内容：\n",
    "发件人（From）：{email['sender']}\n",
    "主题（Subject）：{email['subject']}\n",
    "正文（Body）：{email['body']}\n",
    "\n",
    "请生成一段简短、专业、语气友善的中文回复草稿，供韦恩先生审阅并在发送前个性化润色。\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 将最新的问答追加到对话历史里，保持上下文完整\n",
    "    new_messages = state.get(\"messages\", []) + [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response.content}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"draft_response\": response.content,  # 保存生成的邮件草稿\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "def notify_mr_wayne(state: EmailState):\n",
    "    \"\"\"\n",
    "    收尾节点：模拟向布鲁斯·韦恩汇报邮件处理结果。\n",
    "    \"\"\"\n",
    "    email = state[\"email\"]\n",
    "\n",
    "    print(\"\" + \"=\"*50)\n",
    "    print(f\"Sir, you've received an email from {email['sender']}.\")\n",
    "    print(f\"Subject: {email['subject']}\")\n",
    "    print(\"I've prepared a draft response for your review:\")\n",
    "    print(\"-\"*50)\n",
    "    print(state[\"draft_response\"])\n",
    "    print(\"=\"*50 + \"\")\n",
    "\n",
    "    return {}\n",
    "\n",
    "# 🧭 路由逻辑：根据垃圾邮件判定选择下一步的分支\n",
    "def route_email(state: EmailState) -> str:\n",
    "    if state[\"is_spam\"]:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"legitimate\"\n",
    "\n",
    "# 🛠️ 创建状态图，将上面定义的节点串联成一个 LangGraph 工作流\n",
    "email_graph = StateGraph(EmailState)\n",
    "\n",
    "# 📌 注册节点——每一行都会把函数变成图里的一个执行节点\n",
    "email_graph.add_node(\"read_email\", read_email)  # 首先读取并展示邮件信息\n",
    "email_graph.add_node(\"classify_email\", classify_email)  # 然后请 LLM 判定垃圾邮件\n",
    "email_graph.add_node(\"handle_spam\", handle_spam)  # 垃圾邮件走单独的处理分支\n",
    "email_graph.add_node(\"drafting_response\", drafting_response)  # 合法邮件生成回复草稿\n",
    "email_graph.add_node(\"notify_mr_wayne\", notify_mr_wayne)  # 最后向主人汇报结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1I6C2-0jwIsj",
    "outputId": "abd06b5b-3165-4cf7-996b-6fc9864059ce",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7909a0f02ba0>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# ➕ 配置节点之间的流转顺序\n",
    "email_graph.add_edge(START, \"read_email\")  # 图的起点先进入 read_email 节点\n",
    "\n",
    "# 🧠 判定之后根据结果流向不同分支\n",
    "email_graph.add_edge(\"read_email\", \"classify_email\")  # 展示完邮件后调用分类逻辑\n",
    "\n",
    "# 🔀 添加条件分支：route_email 返回字符串决定下一条边\n",
    "email_graph.add_conditional_edges(\n",
    "    \"classify_email\",  # 根据垃圾邮件判定结果来决定去向\n",
    "    route_email,\n",
    "    {\n",
    "        \"spam\": \"handle_spam\",          # 判定为垃圾邮件则直接走 handle_spam 节点\n",
    "        \"legitimate\": \"drafting_response\"  # 合法邮件则继续撰写回复\n",
    "    }\n",
    ")\n",
    "\n",
    "# ✅ 收尾：无论哪个分支走完都回到 END 节点\n",
    "email_graph.add_edge(\"handle_spam\", END)  # 垃圾邮件处理完毕即结束\n",
    "email_graph.add_edge(\"drafting_response\", \"notify_mr_wayne\")  # 回复草稿后通知主人\n",
    "email_graph.add_edge(\"notify_mr_wayne\", END)  # 汇报结束后整个流程收尾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "haTVi7UmwIsj"
   },
   "outputs": [],
   "source": [
    "# 🧮 将图结构编译成可执行的 LangGraph 代理对象\n",
    "compiled_graph = email_graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GlJYoVUbwIsj"
   },
   "outputs": [],
   "source": [
    "# 📨 准备两封示例邮件，帮助我们观察不同分支的执行效果\n",
    "legitimate_email = {\n",
    "    \"sender\": \"京东客服\",  # 发件人\n",
    "    \"subject\": \"关于您近期订单的发票开具说明\",  # 邮件主题\n",
    "    \"body\": \"尊敬的韦恩先生，您好！关于您在京东的近期订单，增值税电子普通发票已开具并推送至您的邮箱。如需纸质发票或抬头变更，请在7日内通过“我的订单-申请开票”发起，我们将尽快处理。给您带来不便，敬请谅解。\"  # 邮件正文\n",
    "}\n",
    "\n",
    "spam_email = {\n",
    "    \"sender\": \"某数字货币项目方\",  # 垃圾邮件常见的推销者\n",
    "    \"subject\": \"限时暴涨100倍，立即上车！\",  # 诱导性标题\n",
    "    \"body\": \"韦恩先生，我们新上线了一款数字货币，承诺稳稳赚、稳赚不赔！扫码加群，前100名赠送空投名额，错过今天再等一年！\"  # 明显的垃圾推广/诈骗文案\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pwnC2JXXwIsj",
    "outputId": "9aa7dc2d-908f-447c-a7ce-b47f36dae81b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing legitimate email...\n",
      "阿尔弗雷德正在处理来自 京东客服 的邮件，主题为：关于您近期订单的发票开具说明\n",
      "ham\n",
      "==================================================\n",
      "Sir, you've received an email from 京东客服.\n",
      "Subject: 关于您近期订单的发票开具说明\n",
      "I've prepared a draft response for your review:\n",
      "--------------------------------------------------\n",
      "尊敬的京东客服团队，\n",
      "\n",
      "感谢您及时发送关于我近期订单的发票信息。我已收到电子发票，并确认其内容无误。目前无需纸质发票或抬头变更。如有进一步需求，我会在规定时间内通过相关渠道申请。\n",
      "\n",
      "感谢您提供的优质服务。\n",
      "\n",
      "祝好，\n",
      "\n",
      "阿尔弗雷德  \n",
      "韦恩先生的管家\n",
      "==================================================\n",
      "Processing spam email...\n",
      "阿尔弗雷德正在处理来自 某数字货币项目方 的邮件，主题为：限时暴涨100倍，立即上车！\n",
      "spam\n",
      "阿尔弗雷德已经将邮件标记为垃圾邮件。\n",
      "该电子邮件已被移至垃圾邮件文件夹。\n"
     ]
    }
   ],
   "source": [
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 🧩 初始化 Langfuse 的回调处理器，用于自动记录执行轨迹\n",
    "langfuse_handler = CallbackHandler()\n",
    "\n",
    "# ✅ 运行合法邮件示例，演示完整工作流\n",
    "print(\"Processing legitimate email...\")\n",
    "legitimate_result = compiled_graph.invoke(\n",
    "    input={\n",
    "        \"email\": legitimate_email,\n",
    "        \"is_spam\": None,\n",
    "        \"draft_response\": None,\n",
    "        \"messages\": []\n",
    "        },\n",
    "    config={\"callbacks\": [langfuse_handler]}  # 将回调挂到图的执行配置上\n",
    ")\n",
    "\n",
    "# 🚨 再运行垃圾邮件示例，观察分支差异\n",
    "print(\"Processing spam email...\")\n",
    "spam_result = compiled_graph.invoke(\n",
    "    input={\n",
    "        \"email\": spam_email,\n",
    "        \"is_spam\": None,\n",
    "        \"draft_response\": None,\n",
    "        \"messages\": []\n",
    "        },\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjkhTgLWfoSQ"
   },
   "source": [
    "### 追踪结构\n",
    "\n",
    "Langfuse 会记录包含若干 **span（跨度）** 的**trace（追踪）**，每个 span 代表代理逻辑中的一个步骤。本例中的追踪包含整体运行以及如下子跨度：\n",
    "- 工具调用（\n",
    "- LLM 调用（使用 'gpt-4o' 的 Responses API）\n",
    "\n",
    "你可以检查这些记录以精确了解时间消耗、令牌使用量等：\n",
    "\n",
    "![Langfuse 中的追踪树](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251730026.png)\n",
    "\n",
    "_[前往该追踪](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=2d1f23b960fb1ff0bdaf7623fda4936c&timestamp=2025-09-25T09%3A06%3A06.476Z)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHZAkQuefoSQ"
   },
   "source": [
    "## 在线评估\n",
    "\n",
    "在线评估指在真实线上环境（生产环境的实际使用中）对代理进行评估。这需要对真实用户交互进行持续监控与结果分析。\n",
    "\n",
    "我们在此总结了多种评估技术的指南：[链接](https://langfuse.com/blog/2025-03-04-llm-evaluation-101-best-practices-and-challenges)。\n",
    "\n",
    "### 生产环境常见监控指标\n",
    "\n",
    "1. **成本（Costs）**：埋点会记录令牌用量，你可按每个令牌的价格估算成本。\n",
    "2. **延迟（Latency）**：观察完成每个步骤或整次运行所需的时间。\n",
    "3. **用户反馈（User Feedback）**：用户可直接提供反馈（如点赞/点踩）以帮助迭代与修正代理。\n",
    "4. **LLM 评审（LLM-as-a-Judge）**：使用额外的 LLM 近实时评估代理输出（如检测毒性或正确性）。\n",
    "\n",
    "下面展示这些指标的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHMvJ1QlfoSQ"
   },
   "source": [
    "#### 1. 成本（Costs）\n",
    "\n",
    "下图展示了 `gpt-4o` 调用的用量，可据此识别高成本步骤并优化代理。\n",
    "\n",
    "![成本](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251732570.png)\n",
    "\n",
    "_[前往该追踪](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=2d1f23b960fb1ff0bdaf7623fda4936c&timestamp=2025-09-25T09%3A06%3A06.476Z)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz0y9mn7foSQ"
   },
   "source": [
    "#### 2. 延迟（Latency）\n",
    "\n",
    "还可以查看完成每个步骤所需的时间。如下例所示，整个运行约 3 秒，你可以细分到各步骤。此举有助于识别瓶颈并优化代理。\n",
    "\n",
    "![延迟](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251735069.png)\n",
    "\n",
    "_[前往该追踪](https://cloud.langfuse.com/project/cmequpe0j00euad07w6wrvkzg/traces?peek=2d1f23b960fb1ff0bdaf7623fda4936c&timestamp=2025-09-25T09%3A06%3A06.476Z&display=timeline)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtKiK62HfoSR"
   },
   "source": [
    "#### 3. 用户反馈（User Feedback）\n",
    "\n",
    "如果你的代理嵌入在用户界面中，可以采集用户的直接反馈（例如在聊天界面中的点赞/点踩）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI9siKKKfoSR"
   },
   "outputs": [],
   "source": [
    "from langfuse import get_client\n",
    "\n",
    "langfuse = get_client()\n",
    "\n",
    "# ✅ 方式一：使用上下文管理器返回的 span 对象给当前追踪打分\n",
    "with langfuse.start_as_current_span(\n",
    "    name=\"LangGraph\") as span:\n",
    "    # ... 在这里执行具体的 LangGraph 逻辑 ...\n",
    "\n",
    "    # 直接对 span 调用 score_trace 并附加补充信息\n",
    "    span.score_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\",\n",
    "        comment=\"This was correct, thank you\"\n",
    "    )\n",
    "\n",
    "# ✅ 方式二：仍在上下文中时，可使用 score_current_trace 简化调用\n",
    "with langfuse.start_as_current_span(name=\"langgraph-request\") as span:\n",
    "    # ... LangGraph execution ...\n",
    "\n",
    "    # 使用当前上下文的 trace，而无需持有 span 对象\n",
    "    langfuse.score_current_trace(\n",
    "        name=\"user-feedback\",\n",
    "        value=1,\n",
    "        data_type=\"NUMERIC\"\n",
    "    )\n",
    "\n",
    "# ✅ 方式三：如果已经离开上下文，也可以通过 trace_id 进行补录\n",
    "langfuse.create_score(\n",
    "    trace_id=\"predefined-trace-id\",  # ⚠️ 这里需要替换成真实的 trace_id\n",
    "    name=\"user-feedback\",\n",
    "    value=1,\n",
    "    data_type=\"NUMERIC\",\n",
    "    comment=\"This was correct, thank you\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiemuS7YfoSR"
   },
   "source": [
    "用户反馈随后会被 Langfuse 捕获：\n",
    "\n",
    "![Langfuse 中捕获的用户反馈](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509251746651.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29KsI9xcfoSR"
   },
   "source": [
    "#### 4. 自动化的 LLM 评审打分（LLM-as-a-Judge）\n",
    "\n",
    "LLM-as-a-Judge 提供了一种自动评估代理输出的方法。你可以**配置一个独立的 LLM 调用**，用于评估输出的正确性、毒性、风格或其他你关心的指标。\n",
    "\n",
    "**工作流程：**\n",
    "1. 定义一个**评估模板**，例如“检查文本是否含有毒性”。\n",
    "2. 指定用于评审的模型（judge-model），例如 `gpt-4o-mini`。\n",
    "2. 每当代理生成输出时，将其与模板一起传给“评审”LLM。\n",
    "3. 评审 LLM 给出评分或标签，并将结果记录到可观测性平台。\n",
    "\n",
    "Langfuse 示例：\n",
    "\n",
    "![LLM 评审模板](https://langfuse.com/images/cookbook/integration_openai-agents/evaluator-template.png)\n",
    "![LLM 评审器](https://langfuse.com/images/cookbook/integration_openai-agents/evaluator.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGGlYrB7foSR"
   },
   "outputs": [],
   "source": [
    "# 🔁 如果需要单独再次验证垃圾邮件路径，可以复用下面的调用代码\n",
    "print(\"Processing spam email...\")\n",
    "spam_result = compiled_graph.invoke(\n",
    "    input={\n",
    "        \"email\": spam_email,\n",
    "        \"is_spam\": None,\n",
    "        \"draft_response\": None,\n",
    "        \"messages\": []\n",
    "        },\n",
    "    config={\"callbacks\": [langfuse_handler]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Izr-3LiQfoSR"
   },
   "source": [
    "可以看到，该示例的答案被评审为“无毒性（not toxic）”。\n",
    "\n",
    "![LLM 评审得分示例](https://langfuse.com/images/cookbook/example-langgraph-evaluation/llm-as-a-judge-score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7fN0UTkfoSR"
   },
   "source": [
    "#### 5. 可观测性指标总览\n",
    "\n",
    "所有上述指标都可以在统一的仪表盘中可视化。这样你可以快速查看代理在多次会话中的表现，并随时间跟踪质量指标。\n",
    "\n",
    "![可观测性指标总览](https://langfuse.com/images/cookbook/integration_openai-agents/dashboard-dark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlwltgEkfoSR"
   },
   "source": [
    "## 离线评估（Offline Evaluation）\n",
    "\n",
    "在线评估可用于获取实时反馈，但同样需要进行**离线评估（offline evaluation）**——即在开发前或开发过程中进行系统性的检查。这样可以在发布变更到生产环境之前，保障质量与可靠性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5R8eNQxfoSR"
   },
   "source": [
    "### 数据集评估（Dataset Evaluation）\n",
    "\n",
    "在离线评估中，通常会：\n",
    "1. 准备一个基准数据集（包含提示词与期望输出的成对样本）\n",
    "2. 使用该数据集批量运行你的 Agent\n",
    "3. 将模型输出与期望结果进行比较，或采用额外的自动打分机制\n",
    "\n",
    "下面我们使用一个问答数据集示例：[q&a-dataset](https://huggingface.co/datasets/junzhang1207/search-dataset)，其中包含问题与期望答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zOAP8e45wIsp",
    "outputId": "b2807d06-e184-43b7-cb94-e2e2ef8e40af",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First few rows of search-dataset:\n",
      "                                     id  \\\n",
      "0  20caf138-0c81-4ef9-be60-fe919e0d68d4   \n",
      "1  1f37d9fd-1bcc-4f79-b004-bc0e1e944033   \n",
      "2  76173a7f-d645-4e3e-8e0d-cca139e00ebe   \n",
      "3  5f5ef4ca-91fe-4610-a8a9-e15b12e3c803   \n",
      "4  64dbed0d-d91b-4acd-9a9c-0a7aa83115ec   \n",
      "\n",
      "                                            question  \\\n",
      "0                 steve jobs statue location budapst   \n",
      "1  Why is the Battle of Stalingrad considered a t...   \n",
      "2  In what year did 'The Birth of a Nation' surpa...   \n",
      "3  How many Russian soldiers surrendered to AFU i...   \n",
      "4   What event led to the creation of Google Images?   \n",
      "\n",
      "                                     expected_answer       category       area  \n",
      "0  The Steve Jobs statue is located in Budapest, ...           Arts  Knowledge  \n",
      "1  The Battle of Stalingrad is considered a turni...   General News       News  \n",
      "2  This question is based on a false premise. 'Th...  Entertainment       News  \n",
      "3  About 300 Russian soldiers surrendered to the ...   General News       News  \n",
      "4  Jennifer Lopez's appearance in a green Versace...     Technology       News  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 📥 从 Hugging Face 下载示例数据集，这里包含问答形式的条目\n",
    "dataset = load_dataset(\"junzhang1207/search-dataset\", split=\"train\")\n",
    "df = pd.DataFrame(dataset)  # 转成 DataFrame 方便筛选与遍历\n",
    "print(\"First few rows of search-dataset:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlgYY3VmwIsp"
   },
   "source": [
    "接下来，我们在 Langfuse 中创建一个数据集实体以追踪运行；随后将数据集中的每条记录添加到系统中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cdn2qSCwwIsp",
    "outputId": "ef3bb5d3-21bc-416a-aef3-5dccfedd2b05",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset(id='cmg0ap0r900z9ad07gwjr3y2r', name='qa-dataset_langgraph-agent', description='从Hugging Face上传的问答数据集', metadata={'date': '2025-09-21', 'type': 'benchmark'}, project_id='cmequpe0j00euad07w6wrvkzg', created_at=datetime.datetime(2025, 9, 26, 3, 41, 31, 29000, tzinfo=datetime.timezone.utc), updated_at=datetime.datetime(2025, 9, 26, 3, 41, 31, 29000, tzinfo=datetime.timezone.utc))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from langfuse import Langfuse\n",
    "langfuse = Langfuse()\n",
    "\n",
    "langfuse_dataset_name = \"qa-dataset_langgraph-agent\"\n",
    "\n",
    "# 🗂️ 在 Langfuse 中创建一个新的数据集，用于存储评测样本\n",
    "langfuse.create_dataset(\n",
    "    name=langfuse_dataset_name,\n",
    "    description=\"从Hugging Face上传的问答数据集\",\n",
    "    metadata={\n",
    "        \"date\": \"2025-09-21\",\n",
    "        \"type\": \"benchmark\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_BkN1d1QwIsq"
   },
   "outputs": [],
   "source": [
    "# 🎯 仅选取 30 条示例数据上传，实际项目可根据需求调整\n",
    "df_30 = df.sample(30)\n",
    "\n",
    "for idx, row in df_30.iterrows():\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=langfuse_dataset_name,\n",
    "        input={\"text\": row[\"question\"]},            # Langfuse 需要明确的输入字段\n",
    "        expected_output={\"text\": row[\"expected_answer\"]}  # 提供标准答案便于后续评估\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3yJVaBnwIsq"
   },
   "source": [
    "![Langfuse 中的数据集条目](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202509261143566.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7NHQ4XowIsq"
   },
   "source": [
    "#### 在数据集上运行代理\n",
    "\n",
    "首先，构建一个使用 OpenAI 模型回答问题的简易 LangGraph 代理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ACoBDVzbwIsq"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage  # 如需自定义输入消息可以使用该类型\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# 🧱 定义状态结构：messages 字段会自动累积对话历史\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 初始化一个新的状态图构建器\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# 🤖 准备要调用的 OpenAI 聊天模型\n",
    "llm = ChatOpenAI(model=\"gpt-4.5-preview\")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    单节点聊天机器人：\n",
    "    - 将当前所有消息传给 LLM\n",
    "    - 返回模型的回复，LangGraph 会自动把它追加到状态里\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 🔗 注册节点与入口、出口\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "\n",
    "# ⚙️ compile() 会返回可直接调用的图实例\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXSe2ScIwIsq"
   },
   "source": [
    "接着，我们定义一个辅助函数 `my_agent()`，其职责是：\n",
    "1. 创建一个 Langfuse 追踪（trace）\n",
    "2. 获取 `langfuse_handler_trace`，用于为 LangGraph 的执行过程打点\n",
    "3. 运行我们的 Agent，并在调用时传入 `langfuse_handler_trace` 以记录执行细节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZQTQKusMwIsq"
   },
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langfuse import get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate  # 可用于自定义提示模板（本示例暂未使用）\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 🏗️ 构建一个带 Langfuse 追踪能力的 LangGraph 代理\n",
    "graph_builder = StateGraph(State)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")  # 选择对话模型\n",
    "langfuse = get_client()  # 复用前面配置好的 Langfuse 客户端\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"\n",
    "    核心节点：将对话历史交给 LLM，并把生成结果包装成 LangGraph 需要的格式。\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph_builder.set_finish_point(\"chatbot\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "def my_agent(question, langfuse_handler):\n",
    "    \"\"\"\n",
    "    对外暴露的便捷函数：\n",
    "    1. 打开一个 Langfuse span 以便观测这次请求；\n",
    "    2. 调用 LangGraph 代理获取回答；\n",
    "    3. 将输入输出写回 Langfuse，方便后续评估。\n",
    "    \"\"\"\n",
    "\n",
    "    # 创建一个顶层追踪 span，所有上下文都会记录在这里\n",
    "    with langfuse.start_as_current_span(name=\"my-langgraph-agent\") as root_span:\n",
    "\n",
    "        # Step 2: LangChain processing\n",
    "        response = graph.invoke(\n",
    "            input={\"messages\": [HumanMessage(content=question)]},\n",
    "            config={\"callbacks\": [langfuse_handler]}\n",
    "        )\n",
    "\n",
    "        # 将原始问题和模型回答同步到 Langfuse 仪表盘\n",
    "        root_span.update_trace(\n",
    "            input=question,\n",
    "            output=response[\"messages\"][1].content)\n",
    "\n",
    "        print(question)\n",
    "        print(response[\"messages\"][1].content)\n",
    "\n",
    "    return response[\"messages\"][1].content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iri3PYQwIsq"
   },
   "source": [
    "最后，我们遍历数据集中的每一条样本，运行代理，并将生成的追踪与该数据集条目进行关联。如有需要，还可以附加一个快速的评估分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MJyw6m61wIsq",
    "outputId": "7115f780-5ce4-49c2-c281-40cd7bcd76bb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'text': 'Can the Great Wall of China be seen from space with the naked eye?'}\n",
      "No, the Great Wall of China cannot be seen from space with the naked eye. This is a common myth. While the wall is quite long, it is not wide enough to be distinguishable from the vast expanses of natural terrain surrounding it. Astronauts have confirmed that it is difficult to see the Great Wall from low Earth orbit without the aid of telescopic lenses.\n",
      "{'text': 'Who discovered that sunlight is necessary for plants to restore air that has been \"injured\" by a candle or a mouse?'}\n",
      "The discovery that sunlight is necessary for plants to restore air that has been \"injured\" by a candle or a mouse is attributed to Jan Ingenhousz. In the late 18th century, Ingenhousz conducted experiments that demonstrated how plants use sunlight to produce oxygen, which replenishes the air. This work built upon the earlier findings of Joseph Priestley, who had identified the concept of \"injured\" air but did not fully understand the role of sunlight in the process of photosynthesis.\n",
      "{'text': 'What is the official height of Mount Everest as of 2020?'}\n",
      "As of 2020, the official height of Mount Everest is 8,848.86 meters (29,031.7 feet). This measurement was announced in December 2020 following a joint survey by China and Nepal.\n",
      "{'text': 'What is the core object in NumPy that supports multi-dimensional arrays?'}\n",
      "The core object in NumPy that supports multi-dimensional arrays is the `ndarray`. This stands for \"N-dimensional array\" and is the fundamental data structure in NumPy for handling arrays of any dimension, allowing for efficient storage and manipulation of numerical data.\n",
      "{'text': 'kamala harris executive order banning fossil fuels'}\n",
      "As of my last update, there is no known executive order issued by Vice President Kamala Harris banning fossil fuels. Executive orders are typically issued by the sitting President rather than the Vice President, as the ability to issue such orders is a presidential power. If there have been recent developments on this topic, it would be best to consult the latest news sources or governmental announcements for accurate information.\n",
      "{'text': 'In which year did LeBron James win the NBA Regular Season and Playoffs Combined Scoring Title, cementing his status as the most prolific scorer in both the regular season and postseason combined?'}\n",
      "LeBron James won the NBA Regular Season and Playoffs Combined Scoring Title in 2018. This achievement highlighted his exceptional scoring ability throughout both the regular season and the postseason during that year.\n",
      "{'text': 'emma watson vanity fair controvercy feminism'}\n",
      "In 2017, Emma Watson became the subject of a controversy following her photo shoot for Vanity Fair magazine. The debate centered around a particular image from the shoot where Watson was photographed wearing a revealing top that some critics claimed was at odds with her advocacy for feminism.\n",
      "\n",
      "Critics argued that the photo was inconsistent with her feminist principles, suggesting that posing in such attire was contradictory to her stance on empowering women. In response, Watson defended herself by stating that feminism is about giving women the freedom to make their own choices and not being judged for their decisions. She emphasized that feminism is not a \"stick with which to beat other women with,\" highlighting that it is about equality and empowering women to express themselves in various ways.\n",
      "\n",
      "This incident sparked a broader conversation about feminism and how society often misconstrues or narrowly defines it, debating over whether one's clothing or appearance can undermine their feminist beliefs. Watson's stance was supported by many who agreed that feminism should encompass the right to decide how to present oneself without judgment or shame.\n",
      "{'text': \"What is the title of Amy Millan's debut solo album?\"}\n",
      "Amy Millan's debut solo album is titled \"Honey from the Tombs,\" released in 2006.\n",
      "{'text': 'Who starred in the 1978 television film adaptation of Les MisÃ©rables?'}\n",
      "The 1978 television film adaptation of \"Les Misérables\" starred Richard Jordan as Jean Valjean and Anthony Perkins as Javert. This adaptation was directed by Glenn Jordan and aired as a television film.\n",
      "{'text': 'When was Robert F. Kennedy assassinated?'}\n",
      "Robert F. Kennedy was assassinated on June 5, 1968. He was shot at the Ambassador Hotel in Los Angeles, California, and died from his injuries in the early hours of June 6, 1968.\n",
      "{'text': 'When was the Taj Mahal completed?'}\n",
      "The Taj Mahal was completed in 1648. Construction began in 1632 under the order of Mughal Emperor Shah Jahan in memory of his wife Mumtaz Mahal.\n",
      "{'text': 'How is Tetris used as a relaxation tool?'}\n",
      "Tetris is often used as a relaxation tool due to its engaging yet simple gameplay, which can help reduce stress and improve mental clarity. Here are several ways in which Tetris is utilized for relaxation:\n",
      "\n",
      "1. **Distraction from Stressors**: Tetris can serve as a distraction from stressful thoughts or environments. The focus required to play the game can help shift attention away from anxiety-inducing stimuli.\n",
      "\n",
      "2. **Flow State Induction**: The game is known for inducing a state of \"flow,\" where players become fully immersed in the task at hand. This flow state is associated with heightened concentration and a sense of time distortion, leading to a relaxing and rewarding experience.\n",
      "\n",
      "3. **Cognitive Load Management**: The repetitive nature of Tetris and the moderate cognitive load required to play can help occupy the mind, which might prevent overthinking or rumination.\n",
      "\n",
      "4. **Visual and Spatial Engagement**: Engaging with the spatial and visual elements of Tetris can provide a mental break that helps refresh and relax the mind.\n",
      "\n",
      "5. **Mood Enhancement**: Playing Tetris can lead to the release of dopamine, a neurotransmitter associated with feelings of pleasure and satisfaction, which can improve mood and reduce negative emotions.\n",
      "\n",
      "6. **Structured Challenge**: The predictable challenges within the game provide a sense of control and achievement, reducing feelings of helplessness and contributing to a calming experience.\n",
      "\n",
      "Overall, Tetris offers a combination of benefits that can make it an effective tool for relaxation and stress relief.\n",
      "{'text': 'What is the only 7X7 model name not yet assigned to a Boeing product?'}\n",
      "As of the latest available information, the only 7X7 model name not yet assigned to a Boeing product is the Boeing 797. Boeing designates its commercial aircraft with a three-digit code starting with \"7\" and a model number in the format \"7X7\" for its jetliners. The 797 has been speculated for future development but has not been officially assigned to any current Boeing aircraft.\n",
      "{'text': 'most valued card in spads'}\n",
      "It seems like you're referring to the game of Spades. In Spades, the most valued card is the Ace of Spades. The Ace of Spades is the highest-ranking card in the spade suit and typically plays a crucial role in determining trick outcomes. If you have any specific questions about the game or strategies, feel free to ask!\n",
      "{'text': \"What was the purpose of Hadrian's Wall?\"}\n",
      "Hadrian's Wall was a defensive fortification built by the Romans in the early 2nd century AD, specifically around 122 AD during the reign of Emperor Hadrian. The primary purpose of Hadrian's Wall was to mark the northern boundary of the Roman Empire in Britannia and to help control the movement of people and goods across the border. It served as a physical barrier to deter invasions and raids from the tribes in what is now Scotland, particularly the Picts.\n",
      "\n",
      "Additionally, the wall acted as a means to exert control over the surrounding area, marking the limits of Roman territory and influence. It also facilitated the collection of taxes and levies on goods moving through its gates and served as a demonstration of the might of the Roman Empire and its ability to impose order and security. Hadrian's Wall stretched approximately 73 miles (117 kilometers) across the north of England, from the River Tyne in the east to the Solway Firth in the west, and was supplemented by forts, turrets, and smaller structures along its length.\n",
      "{'text': 'how many successful mars missions has india completed'}\n",
      "As of now, India has completed one successful mission to Mars. The Indian Space Research Organisation (ISRO) launched the Mars Orbiter Mission (Mangalyaan) on November 5, 2013. It successfully entered Mars' orbit on September 24, 2014, making India the first Asian nation to reach the Martian orbit and the first country in the world to do so in its first attempt. The mission was primarily designed for technological demonstration, but it also carried scientific instruments to study the Martian surface and atmosphere.\n",
      "{'text': 'Why is the Battle of Stalingrad considered a turning point in World War II?'}\n",
      "The Battle of Stalingrad is considered a turning point in World War II for several reasons:\n",
      "\n",
      "1. **Strategic Importance**: Stalingrad was a key industrial city on the Volga River. Its capture would have allowed Germany to secure its southern flank, bolster its resource supply line, and maintain momentum towards the oil fields of the Caucasus. Its loss marked a halt in the German advance in the Soviet Union.\n",
      "\n",
      "2. **Psychological Impact**: The defeat at Stalingrad shattered the myth of Nazi invincibility. It was the first major defeat of the German Army in the war and significantly demoralized German forces and leadership. Conversely, it boosted Soviet morale and the resolve of Allied nations.\n",
      "\n",
      "3. **Resource Depletion**: The battle was one of the bloodiest in history, with severe casualties on both sides. The prolonged conflict drained German resources and manpower substantially, leading to a weakened ability to carry out further offensives.\n",
      "\n",
      "4. **Shift in Momentum**: After Stalingrad, the Soviets gained the initiative, launching a series of successful counter-offensives that would eventually lead to the recapture of territories lost to the Germans.\n",
      "\n",
      "5. **Political and Military Considerations**: The victory at Stalingrad showcased the capability and resilience of the Soviet Union, strengthening its bargaining position in diplomatic and military dealings with its Allies. \n",
      "\n",
      "The defeat signified not just the loss of a pivotal battle but marked the beginning of a series of retreats by the Germans on the Eastern Front, eventually contributing to the overall Allied victory in Europe.\n",
      "{'text': 'Katy Perry net worth 2023'}\n",
      "As of 2023, Katy Perry's net worth is estimated to be around $330 million. She has accumulated her wealth through her successful music career, including hit albums and singles, as well as through various business ventures, endorsements, and her role as a judge on \"American Idol.\" Keep in mind that net worth estimates can vary and are subject to change based on new financial information and reports.\n",
      "{'text': 'What did Putin announce about Donbas in December 2021?'}\n",
      "In December 2021, Russian President Vladimir Putin made statements regarding the Donbas region, which is situated in eastern Ukraine and has been a focal point of conflict between Ukrainian forces and Russian-backed separatists since 2014. During this period, Putin emphasized his support for the separatists in the region, asserting that Russia should continue to provide economic and humanitarian assistance to Donbas. He also reiterated his stance that the conflict was a result of historical ties and grievances, and defended Russia's involvement by criticizing the actions of the Ukrainian government and Western nations.\n",
      "\n",
      "Moreover, Putin warned of potential aggressive actions by Ukraine and NATO, and claimed that these actions could threaten Russia's national security. He emphasized the need for security guarantees from the West to prevent NATO's further expansion eastward. This rhetoric was part of broader tensions, as Russia was amassing troops near the Ukrainian border, raising international concerns about the possibility of a full-scale invasion, which eventually occurred in February 2022.\n",
      "{'text': 'Who is Novak Djokovic married to?'}\n",
      "Novak Djokovic is married to Jelena Djokovic. They got married in July 2014 and have two children together.\n",
      "{'text': 'When was the Palace of Versailles completed?'}\n",
      "The Palace of Versailles was officially completed in 1682, although construction and renovation continued for several decades. It initially started as a hunting lodge built by Louis XIII in 1623, and was transformed and expanded by his son, Louis XIV, who moved the royal court there, making it the seat of political power in France.\n",
      "{'text': 'What were some of the economic and social impacts observed in former Soviet republics following the dissolution of the Soviet Union?'}\n",
      "Following the dissolution of the Soviet Union in 1991, the former Soviet republics experienced significant economic and social impacts:\n",
      "\n",
      "### Economic Impacts:\n",
      "\n",
      "1. **Transition to Market Economies**: Most republics transitioned from centrally planned economies to market-based systems. This transition was challenging, with many countries experiencing severe economic contractions, hyperinflation, and high unemployment.\n",
      "\n",
      "2. **Privatization**: The privatization of state-owned enterprises was a common economic reform, often resulting in the rapid emergence of oligarchs who acquired significant assets at low prices.\n",
      "\n",
      "3. **Trade Disruptions**: The dismantling of the integrated Soviet economic system caused disruptions in trade. Former republics had to establish new trade relations, both among themselves and internationally.\n",
      "\n",
      "4. **Currency Instability**: Many countries initially faced currency instability and high inflation as they moved from the Soviet ruble to their national currencies.\n",
      "\n",
      "5. **Foreign Investment**: Some countries, particularly those rich in natural resources like oil and gas, attracted significant foreign investment, aiding economic recovery and growth.\n",
      "\n",
      "### Social Impacts:\n",
      "\n",
      "1. **Unemployment and Poverty**: The economic turmoil led to increased unemployment and poverty rates, severely affecting living standards.\n",
      "\n",
      "2. **Migration**: Economic difficulty and regional conflicts prompted significant migration, both within and outside the region, as people sought better opportunities.\n",
      "\n",
      "3. **Social Inequality**: The transitions often resulted in increased social inequality, with wealth becoming concentrated in the hands of a few.\n",
      "\n",
      "4. **Education and Health**: Public services, including education and healthcare, suffered due to reduced funding and social spending, impacting overall quality and access.\n",
      "\n",
      "5. **Identity and Ethnic Tensions**: The collapse of Soviet authority sometimes exacerbated ethnic tensions and conflicts, as people navigated new national identities and governance structures.\n",
      "\n",
      "6. **Demographic Changes**: With declining birth rates and rising emigration, many countries experienced significant demographic shifts, affecting long-term economic prospects.\n",
      "\n",
      "These impacts varied across the region, significantly influenced by each country's unique historical, cultural, and economic context. Over time, some countries adapted successfully to these challenges, achieving greater economic stability and integration into the global economy, while others continued to face prolonged struggles.\n",
      "{'text': \"When does 'The Boys' TV series end?\"}\n",
      "As of now, there is no official announcement regarding the end of \"The Boys\" TV series. The show has been renewed for a fourth season, which is currently in development. Keep an eye on updates from the show's producers or streaming platform, Prime Video, for any future announcements regarding its conclusion.\n",
      "{'text': 'How many people were injured in the Dollywood park flooding?'}\n",
      "I'm sorry, I don't have real-time data. As of my last update, I don't have information about recent incidents such as flooding at Dollywood. To get the most current and accurate information, I recommend checking local news sources or the official Dollywood website or social media channels.\n",
      "{'text': 'When did Virgin Atlantic drop its Hong Kong route after nearly 30 years?'}\n",
      "Virgin Atlantic announced in October 2022 that it was permanently dropping its Hong Kong route after nearly 30 years of service.\n",
      "{'text': \"What unique alloy of gold and silver was used to create the Copa AmÃ©rica trophy purchased from 'Casa Escasany' in Buenos Aires in 1916?\"}\n",
      "The Copa América trophy purchased from \"Casa Escasany\" in Buenos Aires in 1916 is made from a unique alloy of gold and silver known as \"alpaca\" or \"nickel silver.\" This alloy contains copper, nickel, and zinc, giving it a silver-like appearance without containing any actual silver.\n",
      "{'text': 'Current composition of the United States Senate by party?'}\n",
      "As of the latest information available up to October 2023, the composition of the United States Senate consists of:\n",
      "\n",
      "- **Democratic Party:** 48 Senators\n",
      "- **Republican Party:** 49 Senators\n",
      "- **Independents:** 3 Senators\n",
      "\n",
      "The Independents in the Senate typically caucus with the Democrats, giving the Democratic Party a functional majority in terms of legislative control, especially when factoring in the tie-breaking vote of the Vice President, who is a Democrat. However, it’s always good to confirm this with up-to-date sources, as these numbers can change following elections, appointments, or changes in party affiliation.\n",
      "{'text': 'How many Russian soldiers surrendered to AFU in Kursk region?'}\n",
      "As of my last update, there have been no confirmed reports of Russian soldiers surrendering to the Armed Forces of Ukraine (AFU) specifically in the Kursk region. The Kursk region is part of Russia and borders Ukraine, so direct military engagements between Russian and Ukrainian forces in the region would be unusual under current geopolitical circumstances. Please check the latest news sources for any updates on this situation, as the facts may have changed since my last update.\n",
      "{'text': 'Who was the first player ejected from Survivor?'}\n",
      "The first player to be ejected from the TV show \"Survivor\" was Osten Taylor. He was ejected during the seventh season, \"Survivor: Pearl Islands.\" Notably, he was the first contestant in the history of the show to quit the game voluntarily.\n",
      "{'text': 'List the highest four cards in Spades in order.'}\n",
      "In the suit of Spades, the highest four cards in order are:\n",
      "\n",
      "1. Ace of Spades\n",
      "2. King of Spades\n",
      "3. Queen of Spades\n",
      "4. Jack of Spades\n"
     ]
    }
   ],
   "source": [
    "from langfuse import get_client\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "# 📡 初始化追踪组件：CallbackHandler 会把 LangChain 的每一步同步到 Langfuse\n",
    "langfuse_handler = CallbackHandler()\n",
    "langfuse = get_client()\n",
    "\n",
    "dataset = langfuse.get_dataset('qa-dataset_langgraph-agent')  # 获取上一步创建的数据集\n",
    "\n",
    "for item in dataset.items:\n",
    "    # ✅ item.run() 会为每个样本开启一个子追踪，方便查看单条样本的执行情况\n",
    "    with item.run(\n",
    "        run_name=\"run_gpt-4o\",\n",
    "        run_description=\"My first run\",\n",
    "        run_metadata={\"model\": \"gpt-4o\"},\n",
    "    ) as root_span:\n",
    "        # 进入此上下文的所有调用都会自动关联到当前 dataset item\n",
    "\n",
    "        # 🎯 运行核心业务逻辑时，再开一个 generation 上下文记录单次模型调用\n",
    "        with langfuse.start_as_current_generation(\n",
    "            name=\"llm-call\",\n",
    "            model=\"gpt-4o\",\n",
    "            input=item.input\n",
    "        ) as generation:\n",
    "            # 用我们刚才封装的 my_agent 完成实际问答\n",
    "            output = my_agent(str(item.input), langfuse_handler)\n",
    "            generation.update(output=output)\n",
    "\n",
    "        # 📝 可选择对结果打分（例如人工点评或自动指标）\n",
    "        root_span.score_trace(\n",
    "            name=\"user-feedback\",\n",
    "            value=1,\n",
    "            comment=\"This is a comment\",  # 可记录评分原因，便于回溯\n",
    "        )\n",
    "\n",
    "# 🔚 所有调用结束后刷新客户端，确保缓冲区里的数据都被发送\n",
    "langfuse.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rofi0MnywIsq"
   },
   "source": [
    "你可以在不同的 Agent 配置之间重复这一流程，例如：\n",
    "- 模型（如 gpt-4o-mini、o1 等）\n",
    "- 提示词（Prompts）\n",
    "- 工具（如是否启用搜索能力）\n",
    "- Agent 复杂度（多代理 vs 单代理）\n",
    "\n",
    "随后可在 Langfuse 中进行并排对比。在此示例中，我们在 30 条数据集问题上分别运行了 3 次代理，每次使用不同的 OpenAI 模型。可以看到，随着模型能力增大，正确回答的数量按预期提升。`correct_answer` 分数由一个[“模型充当评审”（LLM-as-a-Judge）评估器](https://langfuse.com/docs/scores/model-based-evals)生成，它会基于数据集中给出的参考答案来评估输出是否正确。\n",
    "\n",
    "![数据集运行概览](https://langfuse.com/images/cookbook/example-langgraph-evaluation/dataset_runs.png)\n",
    "![数据集运行对比](https://langfuse.com/images/cookbook/example-langgraph-evaluation/dataset-run-comparison.png)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}