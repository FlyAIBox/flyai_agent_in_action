# å¤§æ¨¡å‹è¯„ä¼°ä½“ç³»ä¸Langfuseå®æˆ˜æŒ‡å—

> é¢å‘å¤§æ¨¡å‹æŠ€æœ¯åˆå­¦è€…çš„å…¨é¢è¯„ä¼°å®æˆ˜æ•™ç¨‹

## ç›®å½•

1. [ç¬¬ä¸€ç« ï¼šä¸ºä»€ä¹ˆè¯´è¯„ä¼°æ˜¯AIäº§å“è½åœ°çš„"æœ€åä¸€å…¬é‡Œ"ï¼Ÿ](#ç¬¬ä¸€ç« ä¸ºä»€ä¹ˆè¯´è¯„ä¼°æ˜¯aiäº§å“è½åœ°çš„æœ€åä¸€å…¬é‡Œ)
2. [ç¬¬äºŒç« ï¼šå¤§æ¨¡å‹è¯„ä¼°æ–¹æ³•è®ºï¼šé€‰æ‹©æœ€é€‚åˆä½ çš„è¯„ä¼°ç­–ç•¥](#ç¬¬äºŒç« å¤§æ¨¡å‹è¯„ä¼°æ–¹æ³•è®ºé€‰æ‹©æœ€é€‚åˆä½ çš„è¯„ä¼°ç­–ç•¥)
3. [ç¬¬ä¸‰ç« ï¼šä¸»æµè¯„ä¼°å·¥å…·æ·±åº¦è§£æ](#ç¬¬ä¸‰ç« ä¸»æµè¯„ä¼°å·¥å…·æ·±åº¦è§£æ)
4. [ç¬¬å››ç« ï¼šé¡¹ç›®å®æˆ˜ï¼ˆä¸Šï¼‰- æ­å»ºå¯è§‚æµ‹çš„è¯„ä¼°åŸºç¡€è®¾æ–½](#ç¬¬å››ç« é¡¹ç›®å®æˆ˜ä¸Š-æ­å»ºå¯è§‚æµ‹çš„è¯„ä¼°åŸºç¡€è®¾æ–½)
5. [ç¬¬äº”ç« ï¼šé¡¹ç›®å®æˆ˜ï¼ˆä¸­ï¼‰- è‡ªåŠ¨åŒ–è¯„ä¼°ä¸æ•°æ®é©±åŠ¨åˆ†æ](#ç¬¬äº”ç« é¡¹ç›®å®æˆ˜ä¸­-è‡ªåŠ¨åŒ–è¯„ä¼°ä¸æ•°æ®é©±åŠ¨åˆ†æ)
6. [ç¬¬å…­ç« ï¼šé¡¹ç›®å®æˆ˜ï¼ˆä¸‹ï¼‰- ç¼–å†™è¯„ä¼°æŠ¥å‘Šä¸ä¼˜åŒ–å»ºè®®](#ç¬¬å…­ç« é¡¹ç›®å®æˆ˜ä¸‹-ç¼–å†™è¯„ä¼°æŠ¥å‘Šä¸ä¼˜åŒ–å»ºè®®)
7. [ç¬¬ä¸ƒç« ï¼šçŸ¥è¯†ç‚¹å›é¡¾ä¸å®æˆ˜ä½œä¸š](#ç¬¬ä¸ƒç« çŸ¥è¯†ç‚¹å›é¡¾ä¸å®æˆ˜ä½œä¸š)

---

## ç¬¬ä¸€ç« ï¼šä¸ºä»€ä¹ˆè¯´è¯„ä¼°æ˜¯AIäº§å“è½åœ°çš„"æœ€åä¸€å…¬é‡Œ"ï¼Ÿ

### 1.1 AIäº§å“å¼€å‘çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸ

åœ¨AIäº§å“å¼€å‘ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ç»å†ä»¥ä¸‹é˜¶æ®µï¼š

```mermaid
graph LR
    A[éœ€æ±‚åˆ†æ] --> B[æ•°æ®å‡†å¤‡]
    B --> C[æ¨¡å‹è®­ç»ƒ]
    C --> D[ç³»ç»Ÿé›†æˆ]
    D --> E[è¯„ä¼°éªŒè¯]
    E --> F[ç”Ÿäº§éƒ¨ç½²]
    F --> G[ç›‘æ§ä¼˜åŒ–]
```

**ä¸ºä»€ä¹ˆè¯„ä¼°æ˜¯"æœ€åä¸€å…¬é‡Œ"ï¼Ÿ**

1. **è´¨é‡ä¿éšœçš„æœ€åé˜²çº¿**ï¼šè¯„ä¼°æ˜¯ç¡®ä¿AIç³»ç»Ÿæ»¡è¶³ä¸šåŠ¡éœ€æ±‚çš„æœ€åæ£€æŸ¥ç‚¹
2. **é£é™©æ§åˆ¶çš„å…³é”®ç¯èŠ‚**ï¼šé¿å…æœ‰é—®é¢˜çš„æ¨¡å‹ç›´æ¥é¢å‘ç”¨æˆ·
3. **æ€§èƒ½ä¼˜åŒ–çš„æ•°æ®ä¾æ®**ï¼šä¸ºåç»­è¿­ä»£æä¾›ç§‘å­¦çš„æ”¹è¿›æ–¹å‘
4. **å•†ä¸šä»·å€¼çš„éªŒè¯æ‰‹æ®µ**ï¼šè¯æ˜AIç³»ç»Ÿèƒ½å¤Ÿå¸¦æ¥å®é™…çš„ä¸šåŠ¡ä»·å€¼

### 1.2 ç¼ºä¹è¯„ä¼°çš„å¸¸è§é—®é¢˜

#### ğŸ’¥ çœŸå®æ¡ˆä¾‹åˆ†æ

**æ¡ˆä¾‹1ï¼šèŠå¤©æœºå™¨äººçš„å¹»è§‰é—®é¢˜**
```python
# æœªç»è¯„ä¼°çš„èŠå¤©æœºå™¨äººå¯èƒ½å‡ºç°çš„é—®é¢˜
user_question = "åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç¥¨ä»·æ˜¯å¤šå°‘ï¼Ÿ"
model_response = "åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“ç¥¨ä»·æ˜¯350å…ƒï¼Œå…¨ç¨‹çº¦4å°æ—¶ã€‚"
# å®é™…ï¼šç¥¨ä»·å¯èƒ½ä¸å‡†ç¡®ï¼Œæ²¡æœ‰è€ƒè™‘åº§ä½ç±»å‹ã€æ—¶é—´ç­‰å› ç´ 
```

**æ¡ˆä¾‹2ï¼šæ¨èç³»ç»Ÿçš„åå·®é—®é¢˜**
- æ¨èç®—æ³•å¯èƒ½å­˜åœ¨æ€§åˆ«ã€å¹´é¾„ã€åœ°åŸŸåè§
- æ²¡æœ‰è¯„ä¼°å°±æ— æ³•å‘ç°è¿™äº›æ½œåœ¨çš„å…¬å¹³æ€§é—®é¢˜

**æ¡ˆä¾‹3ï¼šå†…å®¹ç”Ÿæˆçš„è´¨é‡é—®é¢˜**
- ç”Ÿæˆçš„å†…å®¹å¯èƒ½åŒ…å«æœ‰å®³ä¿¡æ¯
- äº‹å®æ€§é”™è¯¯
- è¯­è¨€è¡¨è¾¾ä¸å½“

### 1.3 è¯„ä¼°åœ¨AIé¡¹ç›®ä¸­çš„ä»·å€¼

#### ğŸ¯ å•†ä¸šä»·å€¼

1. **é™ä½é£é™©æˆæœ¬**
   - é¿å…é”™è¯¯å†³ç­–é€ æˆçš„æŸå¤±
   - å‡å°‘ç”¨æˆ·æŠ•è¯‰å’Œè´Ÿé¢åé¦ˆ
   - ä¿æŠ¤å“ç‰Œå£°èª‰

2. **æå‡ç”¨æˆ·ä½“éªŒ**
   - ç¡®ä¿AIç³»ç»Ÿçš„å¯é æ€§
   - æé«˜å“åº”çš„å‡†ç¡®æ€§
   - å¢å¼ºç”¨æˆ·ä¿¡ä»»åº¦

3. **ä¼˜åŒ–èµ„æºé…ç½®**
   - è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
   - æŒ‡å¯¼ç¡¬ä»¶èµ„æºåˆ†é…
   - ä¼˜åŒ–æˆæœ¬æ•ˆç›Šæ¯”

#### ğŸ› ï¸ æŠ€æœ¯ä»·å€¼

1. **æ¨¡å‹æ€§èƒ½ç›‘æ§**
   - å®æ—¶è·Ÿè¸ªæ¨¡å‹è¡¨ç°
   - åŠæ—¶å‘ç°æ€§èƒ½ä¸‹é™
   - æ”¯æŒæ¨¡å‹ç‰ˆæœ¬ç®¡ç†

2. **æ•°æ®é©±åŠ¨ä¼˜åŒ–**
   - åŸºäºè¯„ä¼°ç»“æœä¼˜åŒ–æç¤ºè¯
   - è°ƒæ•´æ¨¡å‹å‚æ•°
   - æ”¹è¿›è®­ç»ƒæ•°æ®

3. **ç³»ç»Ÿå¯è§‚æµ‹æ€§**
   - å…¨é¢äº†è§£ç³»ç»Ÿè¿è¡ŒçŠ¶æ€
   - æ”¯æŒé—®é¢˜è¯Šæ–­å’Œè°ƒè¯•
   - æä¾›ä¼˜åŒ–æ–¹å‘æŒ‡å¯¼

### 1.4 æœ¬ç« å°ç»“

è¯„ä¼°ä¸æ˜¯AIé¡¹ç›®çš„å¯é€‰é¡¹ï¼Œè€Œæ˜¯å¿…éœ€å“ã€‚å®ƒæ˜¯ç¡®ä¿AIç³»ç»Ÿè´¨é‡ã€æ§åˆ¶é£é™©ã€å®ç°å•†ä¸šä»·å€¼çš„å…³é”®ç¯èŠ‚ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ å¦‚ä½•æ„å»ºå®Œå–„çš„è¯„ä¼°ä½“ç³»ã€‚

---

## ç¬¬äºŒç« ï¼šå¤§æ¨¡å‹è¯„ä¼°æ–¹æ³•è®ºï¼šé€‰æ‹©æœ€é€‚åˆä½ çš„è¯„ä¼°ç­–ç•¥

### 2.1 å¤§æ¨¡å‹è¯„ä¼°çš„æŒ‘æˆ˜

ä¸ä¼ ç»Ÿè½¯ä»¶æµ‹è¯•ä¸åŒï¼Œå¤§æ¨¡å‹è¯„ä¼°é¢ä¸´ç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼š

#### ğŸ² ä¸ç¡®å®šæ€§
- ç›¸åŒè¾“å…¥å¯èƒ½äº§ç”Ÿä¸åŒè¾“å‡º
- æ¨¡å‹è¡Œä¸ºéš¾ä»¥å®Œå…¨é¢„æµ‹
- è¯„ä¼°æ ‡å‡†ç›¸å¯¹ä¸»è§‚

#### ğŸŒ å¤æ‚æ€§
- å¤šæ¨¡æ€è¾“å…¥è¾“å‡º
- ä¸Šä¸‹æ–‡ä¾èµ–æ€§å¼º
- ä»»åŠ¡ç±»å‹å¤šæ ·åŒ–

#### ğŸ“ è¯„ä¼°æ ‡å‡†
- ç¼ºä¹ç»Ÿä¸€çš„è¯„ä¼°åŸºå‡†
- è´¨é‡æ ‡å‡†å› åº”ç”¨è€Œå¼‚
- äººå·¥è¯„ä¼°æˆæœ¬é«˜æ˜‚

### 2.2 å››ç§ä¸»æµè¯„ä¼°æ–¹æ³•è¯¦è§£

#### æ–¹æ³•ä¸€ï¼šå•å…ƒå¼è‡ªåŠ¨åŒ–æµ‹è¯•

**æ ¸å¿ƒç†å¿µ**ï¼šåƒæµ‹è¯•ä¼ ç»Ÿè½¯ä»¶ä¸€æ ·ï¼Œä¸ºå¤§æ¨¡å‹ç¼–å†™è‡ªåŠ¨åŒ–æµ‹è¯•ç”¨ä¾‹ã€‚

**é€‚ç”¨åœºæ™¯**ï¼š
- APIæ¥å£æµ‹è¯•
- åŸºç¡€åŠŸèƒ½éªŒè¯
- å›å½’æµ‹è¯•
- æ€§èƒ½åŸºå‡†æµ‹è¯•

**å®ç°ç¤ºä¾‹**ï¼š
```python
import pytest
from langchain_openai import ChatOpenAI

class TestLLMBasicFunctions:
    
    def setup_method(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)
    
    def test_basic_qa(self):
        """æµ‹è¯•åŸºç¡€é—®ç­”åŠŸèƒ½"""
        question = "1+1ç­‰äºå¤šå°‘ï¼Ÿ"
        response = self.llm.invoke(question)
        
        # æ–­è¨€ï¼šå“åº”åº”è¯¥åŒ…å«æ­£ç¡®ç­”æ¡ˆ
        assert "2" in response.content
        assert len(response.content) > 0
    
    def test_response_format(self):
        """æµ‹è¯•å“åº”æ ¼å¼"""
        prompt = "è¯·ç”¨JSONæ ¼å¼å›ç­”ï¼šåŒ—äº¬çš„å¤©æ°”å¦‚ä½•ï¼Ÿ"
        response = self.llm.invoke(prompt)
        
        # éªŒè¯å“åº”æ ¼å¼
        import json
        try:
            json.loads(response.content)
            assert True
        except:
            assert False, "å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼"
    
    def test_response_time(self):
        """æµ‹è¯•å“åº”æ—¶é—´"""
        import time
        
        start_time = time.time()
        response = self.llm.invoke("Hello")
        end_time = time.time()
        
        # æ–­è¨€ï¼šå“åº”æ—¶é—´åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        assert (end_time - start_time) < 30  # 30ç§’å†…å“åº”
```

**ä¼˜åŠ¿**ï¼š
âœ… è‡ªåŠ¨åŒ–ç¨‹åº¦é«˜  
âœ… å¯é‡å¤æ‰§è¡Œ  
âœ… é€‚åˆCI/CDé›†æˆ  
âœ… æˆæœ¬ä½å»‰  

**åŠ£åŠ¿**ï¼š
âŒ åªèƒ½æµ‹è¯•æœ‰æ˜ç¡®é¢„æœŸçš„åœºæ™¯  
âŒ éš¾ä»¥è¯„ä¼°åˆ›é€ æ€§å’Œä¸»è§‚æ€§ä»»åŠ¡  
âŒ æµ‹è¯•ç”¨ä¾‹ç¼–å†™å¤æ‚  

#### æ–¹æ³•äºŒï¼šäººæœºäº¤äº’è¯„ä¼°

**æ ¸å¿ƒç†å¿µ**ï¼šé€šè¿‡äººå·¥ä¸“å®¶è¯„ä¼°æ¨¡å‹è¾“å‡ºçš„è´¨é‡ã€‚

**è¯„ä¼°ç»´åº¦**ï¼š
- **å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰**ï¼šå›ç­”æ˜¯å¦æ­£ç¡®
- **ç›¸å…³æ€§ï¼ˆRelevanceï¼‰**ï¼šå›ç­”æ˜¯å¦ä¸é—®é¢˜ç›¸å…³
- **å®Œæ•´æ€§ï¼ˆCompletenessï¼‰**ï¼šå›ç­”æ˜¯å¦å…¨é¢
- **æ¸…æ™°æ€§ï¼ˆClarityï¼‰**ï¼šè¡¨è¾¾æ˜¯å¦æ¸…æ¥šæ˜“æ‡‚
- **æœ‰ç”¨æ€§ï¼ˆHelpfulnessï¼‰**ï¼šå›ç­”æ˜¯å¦å¯¹ç”¨æˆ·æœ‰å¸®åŠ©

**è¯„ä¼°æµç¨‹**ï¼š
```python
# äººå·¥è¯„ä¼°è¡¨æ ¼ç¤ºä¾‹
evaluation_criteria = {
    "accuracy": {
        "description": "å›ç­”çš„äº‹å®å‡†ç¡®æ€§",
        "scale": "1-5åˆ†",
        "guidelines": {
            5: "å®Œå…¨å‡†ç¡®ï¼Œæ— ä»»ä½•äº‹å®é”™è¯¯",
            4: "åŸºæœ¬å‡†ç¡®ï¼Œæœ‰è½»å¾®ä¸å‡†ç¡®",
            3: "éƒ¨åˆ†å‡†ç¡®ï¼Œæœ‰ä¸€äº›é”™è¯¯",
            2: "å¤§éƒ¨åˆ†ä¸å‡†ç¡®ï¼Œé”™è¯¯è¾ƒå¤š", 
            1: "å®Œå…¨ä¸å‡†ç¡®ï¼Œä¸¥é‡é”™è¯¯"
        }
    },
    "relevance": {
        "description": "å›ç­”ä¸é—®é¢˜çš„ç›¸å…³æ€§",
        "scale": "1-5åˆ†",
        "guidelines": {
            5: "å®Œå…¨ç›¸å…³ï¼Œç›´æ¥å›ç­”é—®é¢˜",
            4: "åŸºæœ¬ç›¸å…³ï¼Œç•¥æœ‰åé¢˜",
            3: "éƒ¨åˆ†ç›¸å…³ï¼Œæœ‰ä¸€å®šåå·®",
            2: "ç›¸å…³æ€§è¾ƒä½ï¼Œåé¢˜æ˜æ˜¾",
            1: "å®Œå…¨ä¸ç›¸å…³ï¼Œç­”éæ‰€é—®"
        }
    }
}
```

**å®æ–½å»ºè®®**ï¼š

1. **å»ºç«‹è¯„ä¼°æ ‡å‡†**
   - åˆ¶å®šè¯¦ç»†çš„è¯„åˆ†æŒ‡å—
   - æä¾›è¯„ä¼°ç¤ºä¾‹
   - ç¡®ä¿è¯„ä¼°è€…ç†è§£ä¸€è‡´

2. **å¤šäººè¯„ä¼°æœºåˆ¶**
   - è‡³å°‘2-3äººç‹¬ç«‹è¯„ä¼°
   - è®¡ç®—è¯„ä¼°è€…é—´ä¸€è‡´æ€§
   - è®¨è®ºåˆ†æ­§å¹¶è¾¾æˆå…±è¯†

3. **æ‰¹é‡è¯„ä¼°æµç¨‹**
   - éšæœºæŠ½æ ·è¯„ä¼°æ ·æœ¬
   - å®šæœŸè¿›è¡Œè¯„ä¼°
   - è®°å½•è¯„ä¼°ç»“æœå’Œåé¦ˆ

**ä¼˜åŠ¿**ï¼š
âœ… èƒ½è¯„ä¼°ä¸»è§‚æ€§å¼ºçš„ä»»åŠ¡  
âœ… å‘ç°è‡ªåŠ¨åŒ–æµ‹è¯•éš¾ä»¥å‘ç°çš„é—®é¢˜  
âœ… æä¾›å®šæ€§åé¦ˆ  
âœ… è´´è¿‘çœŸå®ç”¨æˆ·ä½“éªŒ  

**åŠ£åŠ¿**ï¼š
âŒ æˆæœ¬é«˜æ˜‚  
âŒ ä¸»è§‚æ€§å¼ºï¼Œå¯èƒ½ä¸ä¸€è‡´  
âŒ éš¾ä»¥å¤§è§„æ¨¡å®æ–½  
âŒ æ—¶é—´å‘¨æœŸé•¿  

#### æ–¹æ³•ä¸‰ï¼šLLM-as-a-Judge ç»¼åˆè¯„ä¼°

**æ ¸å¿ƒç†å¿µ**ï¼šä½¿ç”¨å¦ä¸€ä¸ªLLMä½œä¸º"è¯„åˆ¤å®˜"æ¥è¯„ä¼°ç›®æ ‡æ¨¡å‹çš„è¾“å‡ºã€‚

**å·¥ä½œåŸç†**ï¼š
```python
def llm_as_judge_evaluator(question, answer, criteria):
    """
    ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤å®˜çš„è¯„ä¼°å‡½æ•°
    """
    evaluation_prompt = f"""
    è¯·ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„AIè¯„ä¼°ä¸“å®¶ï¼Œè¯„ä¼°ä»¥ä¸‹AIåŠ©æ‰‹çš„å›ç­”è´¨é‡ã€‚

    **ç”¨æˆ·é—®é¢˜**ï¼š
    {question}

    **AIåŠ©æ‰‹å›ç­”**ï¼š
    {answer}

    **è¯„ä¼°æ ‡å‡†**ï¼š
    {criteria}

    **è¯„ä¼°è¦æ±‚**ï¼š
    1. æ ¹æ®è¯„ä¼°æ ‡å‡†å¯¹å›ç­”è¿›è¡Œæ‰“åˆ†ï¼ˆ1-5åˆ†ï¼‰
    2. ç®€è¿°è¯„åˆ†ç†ç”±
    3. æŒ‡å‡ºå¯ä»¥æ”¹è¿›çš„åœ°æ–¹

    **è¾“å‡ºæ ¼å¼**ï¼š
    {{
        "score": 4,
        "reasoning": "å›ç­”å‡†ç¡®ä¸”ç›¸å…³ï¼Œä½†å¯ä»¥æ›´åŠ è¯¦ç»†...",
        "improvements": "å»ºè®®å¢åŠ å…·ä½“ä¾‹å­å’Œæ›´è¯¦ç»†çš„è§£é‡Š"
    }}
    """
    
    judge_model = ChatOpenAI(model="gpt-4o", temperature=0)
    result = judge_model.invoke(evaluation_prompt)
    
    return result.content
```

**è¯„ä¼°ç»´åº¦ç¤ºä¾‹**ï¼š
```python
# å¤šç»´åº¦è¯„ä¼°æ¨¡æ¿
evaluation_dimensions = {
    "factual_accuracy": {
        "description": "äº‹å®å‡†ç¡®æ€§",
        "weight": 0.3,
        "prompt_template": "è¯„ä¼°å›ç­”ä¸­äº‹å®ä¿¡æ¯çš„å‡†ç¡®æ€§"
    },
    "relevance": {
        "description": "ç›¸å…³æ€§", 
        "weight": 0.25,
        "prompt_template": "è¯„ä¼°å›ç­”ä¸é—®é¢˜çš„ç›¸å…³ç¨‹åº¦"
    },
    "completeness": {
        "description": "å®Œæ•´æ€§",
        "weight": 0.25, 
        "prompt_template": "è¯„ä¼°å›ç­”æ˜¯å¦å…¨é¢å›ç­”äº†é—®é¢˜"
    },
    "clarity": {
        "description": "æ¸…æ™°æ€§",
        "weight": 0.2,
        "prompt_template": "è¯„ä¼°å›ç­”çš„è¡¨è¾¾æ˜¯å¦æ¸…æ¥šæ˜“æ‡‚"
    }
}
```

**ä¼˜åŠ¿**ï¼š
âœ… å¯å¤§è§„æ¨¡è‡ªåŠ¨åŒ–æ‰§è¡Œ  
âœ… æˆæœ¬ç›¸å¯¹è¾ƒä½  
âœ… å¯ä»¥è¯„ä¼°å¤æ‚çš„ä¸»è§‚ä»»åŠ¡  
âœ… æä¾›è¯¦ç»†çš„è¯„ä¼°ç†ç”±  

**åŠ£åŠ¿**ï¼š
âŒ ä¾èµ–è¯„åˆ¤æ¨¡å‹çš„è´¨é‡  
âŒ å¯èƒ½å­˜åœ¨è¯„ä¼°åè§  
âŒ å¯¹æç¤ºè¯è®¾è®¡è¦æ±‚é«˜  

#### æ–¹æ³•å››ï¼šæ··åˆè¯„ä¼°æ¡†æ¶

**æ ¸å¿ƒç†å¿µ**ï¼šç»“åˆå¤šç§è¯„ä¼°æ–¹æ³•ï¼Œæ„å»ºå…¨é¢çš„è¯„ä¼°ä½“ç³»ã€‚

**æ¡†æ¶è®¾è®¡**ï¼š
```python
class HybridEvaluationFramework:
    """æ··åˆè¯„ä¼°æ¡†æ¶"""
    
    def __init__(self):
        self.automated_tests = []
        self.human_evaluators = []
        self.llm_judges = []
        
    def add_automated_test(self, test_func):
        """æ·»åŠ è‡ªåŠ¨åŒ–æµ‹è¯•"""
        self.automated_tests.append(test_func)
        
    def add_human_evaluator(self, evaluator):
        """æ·»åŠ äººå·¥è¯„ä¼°è€…"""
        self.human_evaluators.append(evaluator)
        
    def add_llm_judge(self, judge_config):
        """æ·»åŠ LLMè¯„åˆ¤å®˜"""
        self.llm_judges.append(judge_config)
        
    def comprehensive_evaluate(self, test_cases):
        """ç»¼åˆè¯„ä¼°"""
        results = {
            "automated": {},
            "human": {},
            "llm_judge": {}
        }
        
        # 1. æ‰§è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•
        for test in self.automated_tests:
            results["automated"][test.__name__] = test(test_cases)
            
        # 2. äººå·¥è¯„ä¼°ï¼ˆæŠ½æ ·ï¼‰
        sample_cases = random.sample(test_cases, min(50, len(test_cases)))
        for evaluator in self.human_evaluators:
            results["human"][evaluator.name] = evaluator.evaluate(sample_cases)
            
        # 3. LLMè¯„åˆ¤
        for judge in self.llm_judges:
            results["llm_judge"][judge["name"]] = judge["func"](test_cases)
            
        return self.aggregate_results(results)
        
    def aggregate_results(self, results):
        """èšåˆè¯„ä¼°ç»“æœ"""
        # å®ç°ç»“æœèšåˆé€»è¾‘
        pass
```

**å®æ–½ç­–ç•¥**ï¼š

1. **åŸºç¡€å±‚ï¼šè‡ªåŠ¨åŒ–æµ‹è¯•**
   - è¦†ç›–åŸºæœ¬åŠŸèƒ½
   - æ€§èƒ½åŸºå‡†æµ‹è¯•
   - å›å½’æµ‹è¯•

2. **æ ¸å¿ƒå±‚ï¼šLLMè¯„ä¼°**
   - ä¸»è¦è´¨é‡æŒ‡æ ‡
   - å¤§è§„æ¨¡è¯„ä¼°
   - æŒç»­ç›‘æ§

3. **é¡¶å±‚ï¼šäººå·¥éªŒè¯**
   - å…³é”®åœºæ™¯éªŒè¯
   - è´¨é‡æŠ½æ£€
   - è¾¹ç•Œæƒ…å†µåˆ†æ

### 2.3 å¦‚ä½•é€‰æ‹©åˆé€‚çš„è¯„ä¼°ç­–ç•¥

#### ğŸ¯ åŸºäºåº”ç”¨åœºæ™¯é€‰æ‹©

| åº”ç”¨ç±»å‹ | æ¨èç­–ç•¥ | é‡ç‚¹å…³æ³¨ |
|---------|---------|---------|
| APIæœåŠ¡ | è‡ªåŠ¨åŒ–æµ‹è¯• + LLMè¯„ä¼° | å“åº”æ—¶é—´ã€å‡†ç¡®æ€§ |
| èŠå¤©æœºå™¨äºº | äººå·¥è¯„ä¼° + LLMè¯„ä¼° | ç”¨æˆ·ä½“éªŒã€å¯¹è¯è¿è´¯æ€§ |
| å†…å®¹ç”Ÿæˆ | LLMè¯„ä¼° + äººå·¥æŠ½æ£€ | åˆ›é€ æ€§ã€äº‹å®å‡†ç¡®æ€§ |
| ä»£ç ç”Ÿæˆ | è‡ªåŠ¨åŒ–æµ‹è¯• + æ‰§è¡ŒéªŒè¯ | ä»£ç æ­£ç¡®æ€§ã€å®‰å…¨æ€§ |

#### ğŸ’° åŸºäºèµ„æºé¢„ç®—é€‰æ‹©

- **æœ‰é™é¢„ç®—**ï¼šä¼˜å…ˆé€‰æ‹©è‡ªåŠ¨åŒ–æµ‹è¯• + LLMè¯„ä¼°
- **å……è¶³é¢„ç®—**ï¼šå®æ–½æ··åˆè¯„ä¼°æ¡†æ¶
- **é«˜è´¨é‡è¦æ±‚**ï¼šå¿…é¡»åŒ…å«äººå·¥è¯„ä¼°ç¯èŠ‚

#### â° åŸºäºæ—¶é—´è¦æ±‚é€‰æ‹©

- **å¿«é€Ÿè¿­ä»£**ï¼šä¸»è¦ä¾é è‡ªåŠ¨åŒ–æµ‹è¯•
- **äº§å“ä¸Šçº¿å‰**ï¼šå…¨é¢çš„æ··åˆè¯„ä¼°
- **æ—¥å¸¸ç›‘æ§**ï¼šLLMè¯„ä¼° + å®šæœŸäººå·¥æŠ½æ£€

### 2.4 æœ¬ç« å°ç»“

é€‰æ‹©åˆé€‚çš„è¯„ä¼°ç­–ç•¥éœ€è¦è€ƒè™‘å¤šä¸ªå› ç´ ï¼š

1. **æ˜ç¡®è¯„ä¼°ç›®æ ‡**ï¼šæ˜¯ä¸ºäº†å‘ç°bugã€æå‡è´¨é‡ï¼Œè¿˜æ˜¯æŒç»­ç›‘æ§ï¼Ÿ
2. **äº†è§£èµ„æºçº¦æŸ**ï¼šæ—¶é—´ã€äººåŠ›ã€é¢„ç®—çš„é™åˆ¶
3. **åŒ¹é…åº”ç”¨ç‰¹ç‚¹**ï¼šä¸åŒç±»å‹çš„AIåº”ç”¨éœ€è¦ä¸åŒçš„è¯„ä¼°é‡ç‚¹
4. **æ„å»ºæ¸è¿›å¼è¯„ä¼°ä½“ç³»**ï¼šä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥å®Œå–„

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥äº†è§£ä¸»æµçš„è¯„ä¼°å·¥å…·ï¼Œå­¦ä¹ å¦‚ä½•é€‰æ‹©å’Œä½¿ç”¨åˆé€‚çš„æŠ€æœ¯å¹³å°ã€‚

---

## ç¬¬ä¸‰ç« ï¼šä¸»æµè¯„ä¼°å·¥å…·æ·±åº¦è§£æ

### 3.1 è¯„ä¼°å·¥å…·ç”Ÿæ€æ¦‚è§ˆ

åœ¨å¤§æ¨¡å‹è¯„ä¼°é¢†åŸŸï¼Œç›®å‰ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç±»å·¥å…·ï¼š

#### ğŸ”§ å¼€æºè¯„ä¼°å¹³å°
- **Langfuse**ï¼šä¸“æ³¨äºLLMåº”ç”¨çš„å¯è§‚æµ‹æ€§
- **Phoenix**ï¼šArize AIå¼€æºçš„MLç›‘æ§å¹³å°
- **MLflow**ï¼šé€šç”¨çš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### ğŸ¢ å•†ä¸šè¯„ä¼°å¹³å°
- **LangSmith**ï¼šLangChainå®˜æ–¹çš„å•†ä¸šå¹³å°
- **Weights & Biases**ï¼šç»¼åˆæ€§MLå®éªŒç®¡ç†å¹³å°
- **Neptune**ï¼šå®éªŒè·Ÿè¸ªå’Œæ¨¡å‹ç›‘æ§

#### ğŸ› ï¸ é›†æˆå¼è§£å†³æ–¹æ¡ˆ
- **Hugging Face Evaluate**ï¼šé›†æˆå¤šç§è¯„ä¼°æŒ‡æ ‡
- **OpenAI Evals**ï¼šOpenAIå®˜æ–¹è¯„ä¼°æ¡†æ¶
- **DeepEval**ï¼šä¸“é—¨é’ˆå¯¹LLMçš„è¯„ä¼°åº“

### 3.2 Langfuseæ·±åº¦è§£æ

#### æ ¸å¿ƒåŠŸèƒ½ç‰¹ç‚¹

**1. å…¨é“¾è·¯è¿½è¸ªï¼ˆFull Stack Tracingï¼‰**
```python
# Langfuseè¿½è¸ªç¤ºä¾‹
from langfuse import Langfuse
from langfuse.langchain import CallbackHandler

langfuse = Langfuse()
langfuse_handler = CallbackHandler()

# è‡ªåŠ¨æ•è·LangChainæ‰§è¡Œè¿‡ç¨‹
chain = create_qa_chain()
result = chain.invoke(
    {"question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
    config={"callbacks": [langfuse_handler]}
)
```

**åŠŸèƒ½äº®ç‚¹**ï¼š
- ğŸ¯ **é›¶ä¾µå…¥é›†æˆ**ï¼šé€šè¿‡å›è°ƒå‡½æ•°è‡ªåŠ¨æ”¶é›†æ•°æ®
- ğŸ“Š **å®Œæ•´é“¾è·¯å¯è§†åŒ–**ï¼šä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´æ‰§è¡Œæµç¨‹
- ğŸ’° **æˆæœ¬è¿½è¸ª**ï¼šè‡ªåŠ¨è®¡ç®—Tokenä½¿ç”¨é‡å’ŒAPIæˆæœ¬
- â±ï¸ **æ€§èƒ½ç›‘æ§**ï¼šå»¶è¿Ÿã€ååé‡ç­‰å…³é”®æŒ‡æ ‡

**2. æç¤ºè¯ç®¡ç†ï¼ˆPrompt Managementï¼‰**
```python
# æç¤ºè¯ç‰ˆæœ¬ç®¡ç†
prompt_template = langfuse.get_prompt("travel-agent-prompt", version=2)
formatted_prompt = prompt_template.compile(
    destination="åŒ—äº¬",
    days=3,
    budget=5000
)
```

**ç®¡ç†åŠŸèƒ½**ï¼š
- ğŸ“ **ç‰ˆæœ¬æ§åˆ¶**ï¼šæç¤ºè¯çš„ç‰ˆæœ¬ç®¡ç†å’Œå›æ»š
- ğŸ§ª **A/Bæµ‹è¯•**ï¼šä¸åŒæç¤ºè¯ç‰ˆæœ¬çš„æ•ˆæœå¯¹æ¯”
- ğŸ”„ **åŠ¨æ€æ›´æ–°**ï¼šçº¿ä¸Šæç¤ºè¯çš„çƒ­æ›´æ–°
- ğŸ“ˆ **æ•ˆæœåˆ†æ**ï¼šä¸åŒç‰ˆæœ¬çš„æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

**3. æ•°æ®é›†è¯„ä¼°ï¼ˆDataset Evaluationï¼‰**
```python
# åˆ›å»ºè¯„ä¼°æ•°æ®é›†
dataset = langfuse.create_dataset(
    name="travel-qa-benchmark",
    description="æ—…è¡Œæ™ºèƒ½ä½“é—®ç­”åŸºå‡†æµ‹è¯•é›†"
)

# æ·»åŠ æµ‹è¯•ç”¨ä¾‹
for item in test_cases:
    langfuse.create_dataset_item(
        dataset_name="travel-qa-benchmark",
        input=item["question"],
        expected_output=item["expected_answer"]
    )
```

**4. è‡ªåŠ¨åŒ–è¯„ä¼°ï¼ˆAutomated Scoringï¼‰**
```python
# é…ç½®LLM-as-a-Judgeè¯„ä¼°å™¨
evaluator_config = {
    "name": "helpfulness-evaluator",
    "model": "gpt-4o",
    "prompt": """
    è¯„ä¼°ä»¥ä¸‹AIåŠ©æ‰‹å›ç­”çš„æœ‰ç”¨æ€§ç¨‹åº¦ï¼ˆ1-5åˆ†ï¼‰ï¼š
    
    ç”¨æˆ·é—®é¢˜ï¼š{input}
    AIå›ç­”ï¼š{output}
    
    è¯„åˆ†æ ‡å‡†ï¼š
    5åˆ†ï¼šéå¸¸æœ‰ç”¨ï¼Œå®Œå…¨è§£å†³äº†ç”¨æˆ·é—®é¢˜
    4åˆ†ï¼šæ¯”è¾ƒæœ‰ç”¨ï¼ŒåŸºæœ¬è§£å†³äº†ç”¨æˆ·é—®é¢˜
    3åˆ†ï¼šä¸€èˆ¬æœ‰ç”¨ï¼Œéƒ¨åˆ†è§£å†³äº†ç”¨æˆ·é—®é¢˜
    2åˆ†ï¼šè¾ƒå°‘æœ‰ç”¨ï¼ŒåŸºæœ¬æ²¡æœ‰è§£å†³ç”¨æˆ·é—®é¢˜
    1åˆ†ï¼šå®Œå…¨æ— ç”¨ï¼Œå®Œå…¨æ²¡æœ‰è§£å†³ç”¨æˆ·é—®é¢˜
    
    è¯·ç»™å‡ºåˆ†æ•°å’Œç®€çŸ­ç†ç”±ã€‚
    """
}
```

#### æ¶æ„è®¾è®¡ç†å¿µ

**1. å¼€æ”¾æ€§ï¼ˆOpennessï¼‰**
- å¼€æºæ ¸å¿ƒä»£ç 
- æ”¯æŒè‡ªæ‰˜ç®¡éƒ¨ç½²
- APIä¼˜å…ˆçš„è®¾è®¡ç†å¿µ

**2. é›†æˆå‹å¥½ï¼ˆIntegration-Friendlyï¼‰**
- æ”¯æŒä¸»æµLLMæ¡†æ¶
- æœ€å°åŒ–ä»£ç ä¿®æ”¹
- ä¸°å¯Œçš„SDKæ”¯æŒ

**3. å¯æ‰©å±•æ€§ï¼ˆScalabilityï¼‰**
- äº‘åŸç”Ÿæ¶æ„
- æ°´å¹³æ‰©å±•æ”¯æŒ
- é«˜å¹¶å‘åœºæ™¯ä¼˜åŒ–

### 3.3 LangSmithæ·±åº¦è§£æ

#### æ ¸å¿ƒä¼˜åŠ¿

**1. å®˜æ–¹ç”Ÿæ€é›†æˆ**
```python
# LangSmithä¸LangChainçš„æ·±åº¦é›†æˆ
import os
from langsmith import Client
from langchain_openai import ChatOpenAI

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "travel-agent-evaluation"

# è‡ªåŠ¨å¯ç”¨è¿½è¸ª
llm = ChatOpenAI()
result = llm.invoke("æ¨èåŒ—äº¬ä¸‰æ—¥æ¸¸è·¯çº¿")
```

**2. ä¼ä¸šçº§åŠŸèƒ½**
- ğŸ” **SSOé›†æˆ**ï¼šä¼ä¸šçº§èº«ä»½è®¤è¯
- ğŸ‘¥ **å›¢é˜Ÿåä½œ**ï¼šå¤šäººåä½œè¯„ä¼°
- ğŸ¢ **ç§æœ‰éƒ¨ç½²**ï¼šæ”¯æŒä¼ä¸šå†…éƒ¨éƒ¨ç½²
- ğŸ“Š **é«˜çº§åˆ†æ**ï¼šæ·±åº¦æ•°æ®åˆ†æåŠŸèƒ½

**3. å®éªŒç®¡ç†**
```python
# å®éªŒå¯¹æ¯”åŠŸèƒ½
from langsmith import evaluate

def travel_agent_evaluator(run, example):
    # è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘
    score = calculate_travel_plan_quality(
        run.outputs["plan"], 
        example.outputs["expected_plan"]
    )
    return {"score": score}

# è¿è¡Œè¯„ä¼°å®éªŒ
evaluate(
    lambda inputs: travel_agent.invoke(inputs),
    data="travel-benchmark-dataset",
    evaluators=[travel_agent_evaluator]
)
```

#### å®šä½ä¸ç‰¹è‰²

**ç›®æ ‡ç”¨æˆ·**ï¼š
- ä½¿ç”¨LangChainç”Ÿæ€çš„ä¼ä¸šç”¨æˆ·
- éœ€è¦ä¼ä¸šçº§æ”¯æŒçš„å›¢é˜Ÿ
- å¯¹å®‰å…¨æ€§è¦æ±‚è¾ƒé«˜çš„ç»„ç»‡

**æ ¸å¿ƒç‰¹è‰²**ï¼š
- ä¸LangChainæ— ç¼é›†æˆ
- ä¼ä¸šçº§åŠŸèƒ½å®Œå–„
- å®˜æ–¹æŠ€æœ¯æ”¯æŒ

### 3.4 Phoenixæ·±åº¦è§£æ

#### æŠ€æœ¯ç‰¹ç‚¹

**1. MLç›‘æ§ä¸“ä¸šæ€§**
```python
# Phoenixç›‘æ§é…ç½®
import phoenix as px
from phoenix.trace import using_project

# å¯åŠ¨PhoenixæœåŠ¡
px.launch_app()

# é¡¹ç›®çº§åˆ«çš„è¿½è¸ª
with using_project("travel-agent"):
    # AIåº”ç”¨è¿è¡Œä»£ç 
    pass
```

**2. å¤šæ¡†æ¶æ”¯æŒ**
- LangChain
- LlamaIndex  
- OpenAIç›´æ¥è°ƒç”¨
- Anthropic Claude
- è‡ªå®šä¹‰æ¡†æ¶

**3. å¯è§‚æµ‹æ€§åŠŸèƒ½**
- ğŸ“ˆ **å®æ—¶ç›‘æ§**ï¼šç³»ç»Ÿæ€§èƒ½å®æ—¶ç›‘æ§
- ğŸ” **æ ¹å› åˆ†æ**ï¼šé—®é¢˜è¯Šæ–­å’Œåˆ†æ
- ğŸ“Š **æ•°æ®æ¼‚ç§»æ£€æµ‹**ï¼šè¾“å…¥è¾“å‡ºåˆ†å¸ƒå˜åŒ–
- ğŸ¯ **å¼‚å¸¸æ£€æµ‹**ï¼šè‡ªåŠ¨è¯†åˆ«å¼‚å¸¸è¡Œä¸º

### 3.5 å·¥å…·é€‰æ‹©å†³ç­–æ¡†æ¶

#### é€‰æ‹©çŸ©é˜µ

| è¯„ä¼°ç»´åº¦ | Langfuse | LangSmith | Phoenix | MLflow |
|---------|----------|-----------|---------|--------|
| **å¼€æºç¨‹åº¦** | â­â­â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **LLMä¸“ä¸šæ€§** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **ä¼ä¸šåŠŸèƒ½** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |
| **æ˜“ç”¨æ€§** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| **ç¤¾åŒºæ´»è·ƒåº¦** | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **æˆæœ¬** | å…è´¹/ä»˜è´¹ | ä»˜è´¹ | å…è´¹ | å…è´¹/ä»˜è´¹ |

#### é€‰æ‹©å»ºè®®

**é€‰æ‹©Langfuseçš„åœºæ™¯**ï¼š
- ğŸ¯ ä¸“æ³¨äºLLMåº”ç”¨å¼€å‘
- ğŸ’° é¢„ç®—æœ‰é™çš„åˆåˆ›å›¢é˜Ÿ
- ğŸ”§ éœ€è¦è‡ªæ‰˜ç®¡çš„ç»„ç»‡
- ğŸ“Š é‡è§†å¯è§‚æµ‹æ€§çš„å›¢é˜Ÿ

**é€‰æ‹©LangSmithçš„åœºæ™¯**ï¼š
- ğŸ¢ å¤§å‹ä¼ä¸šç”¨æˆ·
- ğŸ”— æ·±åº¦ä½¿ç”¨LangChainç”Ÿæ€
- ğŸ›¡ï¸ å¯¹å®‰å…¨æ€§è¦æ±‚æé«˜
- ğŸ’¼ éœ€è¦å®˜æ–¹æŠ€æœ¯æ”¯æŒ

**é€‰æ‹©Phoenixçš„åœºæ™¯**ï¼š
- ğŸ”¬ MLå·¥ç¨‹å¸ˆä¸»å¯¼çš„å›¢é˜Ÿ
- ğŸ¯ éœ€è¦é€šç”¨MLç›‘æ§èƒ½åŠ›
- ğŸ†“ å®Œå…¨å¼€æºçš„éœ€æ±‚
- ğŸ” é‡è§†æ ¹å› åˆ†æèƒ½åŠ›

### 3.6 æœ¬ç« å°ç»“

é€‰æ‹©åˆé€‚çš„è¯„ä¼°å·¥å…·éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š

1. **æŠ€æœ¯æ ˆåŒ¹é…åº¦**ï¼šä¸ç°æœ‰æŠ€æœ¯æ ˆçš„å…¼å®¹æ€§
2. **åŠŸèƒ½éœ€æ±‚åŒ¹é…**ï¼šæ˜¯å¦æ»¡è¶³æ ¸å¿ƒè¯„ä¼°éœ€æ±‚
3. **é¢„ç®—è€ƒé‡**ï¼šå¼€æºvså•†ä¸šçš„æˆæœ¬æƒè¡¡
4. **å›¢é˜ŸæŠ€èƒ½**ï¼šå›¢é˜Ÿçš„æŠ€æœ¯èƒ½åŠ›å’Œå­¦ä¹ æˆæœ¬
5. **é•¿æœŸè§„åˆ’**ï¼šå·¥å…·çš„å‘å±•å‰æ™¯å’Œç”Ÿæ€

åœ¨æ¥ä¸‹æ¥çš„å®æˆ˜ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥Langfuseä¸ºä¾‹ï¼Œè¯¦ç»†è®²è§£å¦‚ä½•æ­å»ºå®Œæ•´çš„è¯„ä¼°ä½“ç³»ã€‚

---

## ç¬¬å››ç« ï¼šé¡¹ç›®å®æˆ˜ï¼ˆä¸Šï¼‰- æ­å»ºå¯è§‚æµ‹çš„è¯„ä¼°åŸºç¡€è®¾æ–½

### 4.1 å®æˆ˜é¡¹ç›®ä»‹ç»ï¼šAIæ—…è¡Œæ™ºèƒ½ä½“

#### é¡¹ç›®èƒŒæ™¯
æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªAIæ—…è¡Œæ™ºèƒ½ä½“ï¼Œå®ƒèƒ½å¤Ÿï¼š
- ğŸ—ºï¸ æ ¹æ®ç”¨æˆ·éœ€æ±‚æ¨èæ—…è¡Œç›®çš„åœ°
- ğŸ“… åˆ¶å®šè¯¦ç»†çš„è¡Œç¨‹è§„åˆ’
- ğŸ¨ æ¨èä½å®¿å’Œé¤é¥®
- ğŸ’° é¢„ç®—è§„åˆ’å’Œæˆæœ¬ä¼°ç®—
- ğŸ¯ ä¸ªæ€§åŒ–å®šåˆ¶å»ºè®®

#### ç³»ç»Ÿæ¶æ„
```mermaid
graph TB
    A[ç”¨æˆ·è¾“å…¥] --> B[æ„å›¾è¯†åˆ«]
    B --> C[ä¿¡æ¯æ”¶é›†]
    C --> D[è¡Œç¨‹è§„åˆ’]
    D --> E[èµ„æºæ¨è]
    E --> F[ç»“æœæ•´åˆ]
    F --> G[ç”¨æˆ·è¾“å‡º]
    
    H[Langfuse] --> B
    H --> C  
    H --> D
    H --> E
    H --> F
```

### 4.2 ç¯å¢ƒå‡†å¤‡ä¸Langfuseé›†æˆ

#### æ­¥éª¤1ï¼šå®‰è£…ä¾èµ–åŒ…

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv travel-agent-env
source travel-agent-env/bin/activate  # Linux/Mac
# travel-agent-env\Scripts\activate  # Windows

# å®‰è£…å¿…è¦çš„åŒ…
pip install langfuse==3.3.0
pip install langchain==0.3.27  
pip install langchain-openai==0.3.31
pip install langchain-community==0.3.27
pip install langgraph
```

#### æ­¥éª¤2ï¼šLangfuseè´¦å·è®¾ç½®

```python
import os

# ğŸ”‘ Langfuseé…ç½®
# ä»é¡¹ç›®è®¾ç½®é¡µé¢è·å–ï¼šhttps://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-your-public-key"
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-your-secret-key" 
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"

# ğŸ¤– OpenAIé…ç½®
os.environ["OPENAI_API_KEY"] = "sk-your-openai-key"
```

#### æ­¥éª¤3ï¼šéªŒè¯è¿æ¥

```python
from langfuse import get_client

# åˆå§‹åŒ–å®¢æˆ·ç«¯å¹¶éªŒè¯è¿æ¥
langfuse = get_client()

if langfuse.auth_check():
    print("âœ… Langfuseå®¢æˆ·ç«¯è¿æ¥æˆåŠŸï¼")
    print("ğŸš€ å¯ä»¥å¼€å§‹æ„å»ºå¯è§‚æµ‹çš„AIåº”ç”¨äº†")
else:
    print("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    print("ğŸ”§ æ£€æŸ¥é¡¹ï¼šAPIå¯†é’¥ã€ç½‘ç»œè¿æ¥ã€æœåŠ¡å™¨çŠ¶æ€")
```

### 4.3 æ„å»ºAIæ—…è¡Œæ™ºèƒ½ä½“

#### æ ¸å¿ƒæ™ºèƒ½ä½“ç±»

```python
from typing import Dict, List, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langfuse.langchain import CallbackHandler
import json

class TravelAgent:
    """AIæ—…è¡Œæ™ºèƒ½ä½“"""
    
    def __init__(self, model_name: str = "gpt-4o"):
        """
        åˆå§‹åŒ–æ—…è¡Œæ™ºèƒ½ä½“
        
        Args:
            model_name: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
        """
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.7,  # é€‚åº¦çš„åˆ›é€ æ€§
            max_tokens=2000   # ç¡®ä¿è¶³å¤Ÿè¯¦ç»†çš„å›ç­”
        )
        
        # åˆå§‹åŒ–Langfuseè¿½è¸ª
        self.langfuse_handler = CallbackHandler()
        
        # ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIæ—…è¡Œé¡¾é—®ï¼Œå…·æœ‰ä¸°å¯Œçš„å…¨çƒæ—…è¡Œç»éªŒå’Œæœ¬åœ°åŒ–çŸ¥è¯†ã€‚

**ä½ çš„æ ¸å¿ƒèƒ½åŠ›**ï¼š
- ğŸ—ºï¸ ç›®çš„åœ°æ¨èï¼šåŸºäºç”¨æˆ·åå¥½æ¨èåˆé€‚çš„æ—…è¡Œç›®çš„åœ°
- ğŸ“… è¡Œç¨‹è§„åˆ’ï¼šåˆ¶å®šè¯¦ç»†ã€å¯è¡Œçš„æ—…è¡Œè®¡åˆ’
- ğŸ¨ èµ„æºæ¨èï¼šæ¨èä½å®¿ã€é¤é¥®ã€äº¤é€šã€æ™¯ç‚¹
- ğŸ’° é¢„ç®—è§„åˆ’ï¼šæä¾›ç°å®çš„æˆæœ¬ä¼°ç®—
- ğŸ¯ ä¸ªæ€§åŒ–æœåŠ¡ï¼šæ ¹æ®ç”¨æˆ·ç‰¹æ®Šéœ€æ±‚å®šåˆ¶æ–¹æ¡ˆ

**å›ç­”åŸåˆ™**ï¼š
1. ğŸ¯ **å‡†ç¡®æ€§ä¼˜å…ˆ**ï¼šæä¾›å‡†ç¡®ã€æœ€æ–°çš„ä¿¡æ¯
2. ğŸŒŸ **å®ç”¨æ€§å¯¼å‘**ï¼šç»™å‡ºå¯æ‰§è¡Œçš„å…·ä½“å»ºè®®
3. ğŸ’ **ä¸ªæ€§åŒ–æœåŠ¡**ï¼šå……åˆ†è€ƒè™‘ç”¨æˆ·çš„ç‰¹æ®Šéœ€æ±‚
4. ğŸ“Š **ç»“æ„åŒ–è¾“å‡º**ï¼šä½¿ç”¨æ¸…æ™°çš„æ ¼å¼ç»„ç»‡ä¿¡æ¯
5. ğŸ’° **é¢„ç®—æ„è¯†**ï¼šå§‹ç»ˆè€ƒè™‘æˆæœ¬æ•ˆç›Š

**è¾“å‡ºæ ¼å¼**ï¼š
è¯·ä½¿ç”¨JSONæ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- destination: æ¨èçš„ç›®çš„åœ°
- itinerary: è¯¦ç»†è¡Œç¨‹å®‰æ’
- accommodations: ä½å®¿æ¨è
- dining: é¤é¥®æ¨è  
- budget_estimate: é¢„ç®—ä¼°ç®—
- special_tips: ç‰¹æ®Šæç¤º

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚æä¾›ä¸“ä¸šçš„æ—…è¡Œå»ºè®®ã€‚
"""

    def plan_trip(self, user_request: str) -> Dict[str, Any]:
        """
        åˆ¶å®šæ—…è¡Œè®¡åˆ’
        
        Args:
            user_request: ç”¨æˆ·çš„æ—…è¡Œéœ€æ±‚æè¿°
            
        Returns:
            æ—…è¡Œè®¡åˆ’çš„è¯¦ç»†ä¿¡æ¯
        """
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=f"ç”¨æˆ·éœ€æ±‚ï¼š{user_request}")
        ]
        
        # ä½¿ç”¨Langfuseè¿½è¸ªè°ƒç”¨LLM
        try:
            response = self.llm.invoke(
                messages,
                config={"callbacks": [self.langfuse_handler]}
            )
            
            # è§£æå“åº”
            plan = self._parse_response(response.content)
            
            # æ·»åŠ å…ƒæ•°æ®
            plan["model_used"] = self.llm.model_name
            plan["user_request"] = user_request
            plan["response_length"] = len(response.content)
            
            return plan
            
        except Exception as e:
            print(f"âŒ åˆ¶å®šæ—…è¡Œè®¡åˆ’æ—¶å‡ºé”™ï¼š{str(e)}")
            return {"error": str(e)}
    
    def _parse_response(self, response_content: str) -> Dict[str, Any]:
        """è§£æLLMçš„JSONå“åº”"""
        try:
            # å°è¯•è§£æJSON
            return json.loads(response_content)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œè¿”å›åŸå§‹å†…å®¹
            return {
                "raw_response": response_content,
                "parsing_error": "å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼"
            }
```

#### å¢å¼ºç‰ˆæ™ºèƒ½ä½“ï¼ˆå¤šAgentåä½œï¼‰

```python
from langgraph.graph import StateGraph, START, END
from typing_extensions import TypedDict

class TravelPlanState(TypedDict):
    """æ—…è¡Œè§„åˆ’çŠ¶æ€"""
    user_request: str
    destination_analysis: Optional[Dict]
    itinerary_plan: Optional[Dict] 
    budget_analysis: Optional[Dict]
    final_recommendation: Optional[Dict]
    messages: List[Dict[str, Any]]

class MultiAgentTravelPlanner:
    """å¤šæ™ºèƒ½ä½“æ—…è¡Œè§„åˆ’ç³»ç»Ÿ"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
        self.langfuse_handler = CallbackHandler()
        
        # æ„å»ºçŠ¶æ€å›¾
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºå¤šæ™ºèƒ½ä½“åä½œå›¾"""
        
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(TravelPlanState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("destination_agent", self.destination_analysis)
        workflow.add_node("itinerary_agent", self.itinerary_planning) 
        workflow.add_node("budget_agent", self.budget_planning)
        workflow.add_node("integration_agent", self.result_integration)
        
        # å®šä¹‰æ‰§è¡Œæµç¨‹
        workflow.add_edge(START, "destination_agent")
        workflow.add_edge("destination_agent", "itinerary_agent")
        workflow.add_edge("itinerary_agent", "budget_agent")
        workflow.add_edge("budget_agent", "integration_agent")
        workflow.add_edge("integration_agent", END)
        
        return workflow.compile()
    
    def destination_analysis(self, state: TravelPlanState) -> TravelPlanState:
        """ç›®çš„åœ°åˆ†ææ™ºèƒ½ä½“"""
        
        prompt = f"""
        ä½œä¸ºç›®çš„åœ°åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ç”¨æˆ·éœ€æ±‚å¹¶æ¨èæœ€é€‚åˆçš„ç›®çš„åœ°ï¼š
        
        ç”¨æˆ·éœ€æ±‚ï¼š{state["user_request"]}
        
        è¯·åˆ†æï¼š
        1. ç”¨æˆ·çš„æ—…è¡Œåå¥½ï¼ˆæ–‡åŒ–ã€è‡ªç„¶ã€ç¾é£Ÿç­‰ï¼‰
        2. æ¨è2-3ä¸ªåˆé€‚çš„ç›®çš„åœ°
        3. æ¯ä¸ªç›®çš„åœ°çš„ç‰¹è‰²å’Œä¼˜åŠ¿
        4. æœ€ä½³æ—…è¡Œæ—¶é—´å»ºè®®
        
        ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
        """
        
        response = self.llm.invoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.langfuse_handler]}
        )
        
        try:
            analysis = json.loads(response.content)
        except:
            analysis = {"raw_response": response.content}
            
        state["destination_analysis"] = analysis
        state["messages"].append({
            "agent": "destination_agent",
            "content": response.content
        })
        
        return state
    
    def itinerary_planning(self, state: TravelPlanState) -> TravelPlanState:
        """è¡Œç¨‹è§„åˆ’æ™ºèƒ½ä½“"""
        
        destination_info = state.get("destination_analysis", {})
        
        prompt = f"""
        ä½œä¸ºè¡Œç¨‹è§„åˆ’ä¸“å®¶ï¼ŒåŸºäºç›®çš„åœ°åˆ†æç»“æœåˆ¶å®šè¯¦ç»†çš„æ—…è¡Œè¡Œç¨‹ï¼š
        
        åŸå§‹éœ€æ±‚ï¼š{state["user_request"]}
        ç›®çš„åœ°åˆ†æï¼š{json.dumps(destination_info, ensure_ascii=False)}
        
        è¯·åˆ¶å®šï¼š
        1. è¯¦ç»†çš„æ—¥ç¨‹å®‰æ’ï¼ˆé€æ—¥è®¡åˆ’ï¼‰
        2. ä¸»è¦æ™¯ç‚¹å’Œæ´»åŠ¨æ¨è
        3. äº¤é€šå®‰æ’å»ºè®®
        4. ä½å®¿åŒºåŸŸæ¨è
        
        ä»¥JSONæ ¼å¼è¿”å›è¡Œç¨‹è®¡åˆ’ã€‚
        """
        
        response = self.llm.invoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.langfuse_handler]}
        )
        
        try:
            itinerary = json.loads(response.content)
        except:
            itinerary = {"raw_response": response.content}
            
        state["itinerary_plan"] = itinerary
        state["messages"].append({
            "agent": "itinerary_agent", 
            "content": response.content
        })
        
        return state
    
    def budget_planning(self, state: TravelPlanState) -> TravelPlanState:
        """é¢„ç®—è§„åˆ’æ™ºèƒ½ä½“"""
        
        itinerary = state.get("itinerary_plan", {})
        
        prompt = f"""
        ä½œä¸ºé¢„ç®—è§„åˆ’ä¸“å®¶ï¼Œä¸ºåˆ¶å®šçš„è¡Œç¨‹æä¾›è¯¦ç»†çš„æˆæœ¬ä¼°ç®—ï¼š
        
        åŸå§‹éœ€æ±‚ï¼š{state["user_request"]}
        è¡Œç¨‹è®¡åˆ’ï¼š{json.dumps(itinerary, ensure_ascii=False)}
        
        è¯·ä¼°ç®—ï¼š
        1. äº¤é€šè´¹ç”¨ï¼ˆæœºç¥¨ã€å½“åœ°äº¤é€šï¼‰
        2. ä½å®¿è´¹ç”¨ï¼ˆä¸åŒç­‰çº§é€‰æ‹©ï¼‰
        3. é¤é¥®è´¹ç”¨ï¼ˆé¢„ä¼°æ¯æ—¥æ¶ˆè´¹ï¼‰
        4. æ™¯ç‚¹é—¨ç¥¨å’Œæ´»åŠ¨è´¹ç”¨
        5. è´­ç‰©å’Œå…¶ä»–è´¹ç”¨
        6. æ€»é¢„ç®—åŒºé—´ï¼ˆç»æµ/èˆ’é€‚/è±ªåï¼‰
        
        ä»¥JSONæ ¼å¼è¿”å›é¢„ç®—åˆ†æã€‚
        """
        
        response = self.llm.invoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.langfuse_handler]}
        )
        
        try:
            budget = json.loads(response.content)
        except:
            budget = {"raw_response": response.content}
            
        state["budget_analysis"] = budget
        state["messages"].append({
            "agent": "budget_agent",
            "content": response.content
        })
        
        return state
    
    def result_integration(self, state: TravelPlanState) -> TravelPlanState:
        """ç»“æœæ•´åˆæ™ºèƒ½ä½“"""
        
        prompt = f"""
        ä½œä¸ºæ—…è¡Œé¡¾é—®ï¼Œæ•´åˆæ‰€æœ‰åˆ†æç»“æœï¼Œæä¾›æœ€ç»ˆçš„æ—…è¡Œå»ºè®®ï¼š
        
        åŸå§‹éœ€æ±‚ï¼š{state["user_request"]}
        ç›®çš„åœ°åˆ†æï¼š{json.dumps(state.get("destination_analysis", {}), ensure_ascii=False)}
        è¡Œç¨‹è®¡åˆ’ï¼š{json.dumps(state.get("itinerary_plan", {}), ensure_ascii=False)}
        é¢„ç®—åˆ†æï¼š{json.dumps(state.get("budget_analysis", {}), ensure_ascii=False)}
        
        è¯·æ•´åˆä¸ºï¼š
        1. æœ€ç»ˆæ¨èçš„å®Œæ•´æ—…è¡Œæ–¹æ¡ˆ
        2. é‡ç‚¹æç¤ºå’Œæ³¨æ„äº‹é¡¹
        3. å¯é€‰çš„æ›¿ä»£æ–¹æ¡ˆ
        4. é¢„è®¢å»ºè®®å’Œæ—¶é—´å®‰æ’
        
        ä»¥JSONæ ¼å¼è¿”å›æœ€ç»ˆå»ºè®®ã€‚
        """
        
        response = self.llm.invoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.langfuse_handler]}
        )
        
        try:
            final_rec = json.loads(response.content)
        except:
            final_rec = {"raw_response": response.content}
            
        state["final_recommendation"] = final_rec
        state["messages"].append({
            "agent": "integration_agent",
            "content": response.content
        })
        
        return state
    
    def plan_trip(self, user_request: str) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„æ—…è¡Œè§„åˆ’æµç¨‹"""
        
        initial_state = TravelPlanState(
            user_request=user_request,
            destination_analysis=None,
            itinerary_plan=None,
            budget_analysis=None, 
            final_recommendation=None,
            messages=[]
        )
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = self.graph.invoke(initial_state)
        
        return result
```

### 4.4 è®¾ç½®å®Œæ•´çš„è¿½è¸ªä½“ç³»

#### è‡ªå®šä¹‰è¿½è¸ªè£…é¥°å™¨

```python
from functools import wraps
from langfuse import Langfuse
import time

def trace_travel_agent(trace_name: str = None):
    """
    æ—…è¡Œæ™ºèƒ½ä½“è¿½è¸ªè£…é¥°å™¨
    
    Args:
        trace_name: è¿½è¸ªåç§°
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            
            langfuse = Langfuse()
            
            # ç¡®å®šè¿½è¸ªåç§°
            name = trace_name or f"travel-agent-{func.__name__}"
            
            # å¼€å§‹è¿½è¸ª
            with langfuse.start_as_current_span(name=name) as span:
                
                # è®°å½•è¾“å…¥
                span.update_trace(
                    input={
                        "function": func.__name__,
                        "args": args,
                        "kwargs": kwargs
                    }
                )
                
                # æ‰§è¡Œå‡½æ•°
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    
                    # è®°å½•æˆåŠŸç»“æœ
                    span.update_trace(
                        output=result,
                        metadata={
                            "execution_time": time.time() - start_time,
                            "status": "success"
                        }
                    )
                    
                    return result
                    
                except Exception as e:
                    # è®°å½•é”™è¯¯
                    span.update_trace(
                        output={"error": str(e)},
                        metadata={
                            "execution_time": time.time() - start_time,
                            "status": "error",
                            "error_type": type(e).__name__
                        }
                    )
                    raise
                    
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
class TrackedTravelAgent(TravelAgent):
    """å¸¦è¿½è¸ªåŠŸèƒ½çš„æ—…è¡Œæ™ºèƒ½ä½“"""
    
    @trace_travel_agent("travel-planning")
    def plan_trip(self, user_request: str) -> Dict[str, Any]:
        return super().plan_trip(user_request)
    
    @trace_travel_agent("destination-recommendation")
    def recommend_destinations(self, preferences: Dict[str, Any]) -> List[str]:
        """æ¨èç›®çš„åœ°"""
        
        prompt = f"""
        åŸºäºç”¨æˆ·åå¥½æ¨èæ—…è¡Œç›®çš„åœ°ï¼š
        åå¥½ï¼š{json.dumps(preferences, ensure_ascii=False)}
        
        è¯·æ¨è3-5ä¸ªåˆé€‚çš„ç›®çš„åœ°ï¼Œå¹¶è¯´æ˜æ¨èç†ç”±ã€‚
        """
        
        response = self.llm.invoke(
            [HumanMessage(content=prompt)],
            config={"callbacks": [self.langfuse_handler]}
        )
        
        return response.content
```

### 4.5 æ•°æ®æ”¶é›†é…ç½®

#### é…ç½®æ•°æ®æ”¶é›†ç­–ç•¥

```python
class TravelAgentDataCollector:
    """æ—…è¡Œæ™ºèƒ½ä½“æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self):
        self.langfuse = Langfuse()
    
    def setup_data_collection(self):
        """è®¾ç½®æ•°æ®æ”¶é›†é…ç½®"""
        
        # 1. é…ç½®ç”¨æˆ·ä¼šè¯è¿½è¸ª
        self.session_config = {
            "collect_user_feedback": True,
            "collect_session_metadata": True,
            "track_user_journey": True
        }
        
        # 2. é…ç½®æ€§èƒ½æŒ‡æ ‡æ”¶é›†
        self.performance_config = {
            "track_latency": True,
            "track_token_usage": True,
            "track_cost": True,
            "track_error_rate": True
        }
        
        # 3. é…ç½®è´¨é‡æŒ‡æ ‡æ”¶é›†
        self.quality_config = {
            "collect_output_quality": True,
            "track_user_satisfaction": True,
            "monitor_hallucinations": True
        }
    
    def log_user_interaction(self, session_id: str, user_input: str, 
                           agent_output: str, user_feedback: Dict = None):
        """è®°å½•ç”¨æˆ·äº¤äº’"""
        
        with self.langfuse.start_as_current_span(
            name="user-interaction",
            session_id=session_id
        ) as span:
            
            span.update_trace(
                input=user_input,
                output=agent_output,
                metadata={
                    "session_id": session_id,
                    "timestamp": time.time(),
                    "user_feedback": user_feedback
                }
            )
            
            # å¦‚æœæœ‰ç”¨æˆ·åé¦ˆï¼Œè®°å½•ä¸ºè¯„åˆ†
            if user_feedback:
                span.score_trace(
                    name="user-satisfaction",
                    value=user_feedback.get("rating", 0),
                    comment=user_feedback.get("comment", "")
                )
    
    def log_performance_metrics(self, trace_id: str, metrics: Dict[str, Any]):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        
        self.langfuse.create_score(
            trace_id=trace_id,
            name="performance-metrics",
            value=metrics.get("overall_score", 0),
            data_type="NUMERIC",
            comment=json.dumps(metrics)
        )
    
    def create_evaluation_dataset(self, name: str, test_cases: List[Dict]):
        """åˆ›å»ºè¯„ä¼°æ•°æ®é›†"""
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = self.langfuse.create_dataset(
            name=name,
            description=f"æ—…è¡Œæ™ºèƒ½ä½“è¯„ä¼°æ•°æ®é›† - {name}",
            metadata={
                "created_at": time.time(),
                "test_case_count": len(test_cases),
                "dataset_type": "travel_agent_evaluation"
            }
        )
        
        # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
        for i, case in enumerate(test_cases):
            self.langfuse.create_dataset_item(
                dataset_name=name,
                input=case["input"],
                expected_output=case.get("expected_output"),
                metadata={
                    "case_id": i,
                    "category": case.get("category", "general"),
                    "difficulty": case.get("difficulty", "medium")
                }
            )
        
        print(f"âœ… åˆ›å»ºæ•°æ®é›† '{name}'ï¼ŒåŒ…å« {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return dataset
```

### 4.6 æœ¬ç« å®æˆ˜æ¼”ç»ƒ

#### å®Œæ•´çš„ç¤ºä¾‹è¿è¡Œ

```python
def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´æ¼”ç¤ºå¯è§‚æµ‹çš„æ—…è¡Œæ™ºèƒ½ä½“"""
    
    print("ğŸš€ å¯åŠ¨AIæ—…è¡Œæ™ºèƒ½ä½“è¯„ä¼°ç³»ç»Ÿ")
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    agent = TrackedTravelAgent()
    collector = TravelAgentDataCollector()
    collector.setup_data_collection()
    
    # 2. å‡†å¤‡æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "input": "æˆ‘æƒ³å»ä¸€ä¸ªæœ‰å¤å…¸æ–‡åŒ–çš„åœ°æ–¹ï¼Œé¢„ç®—1ä¸‡å…ƒï¼Œæ—¶é—´5å¤©",
            "category": "cultural_travel",
            "difficulty": "medium"
        },
        {
            "input": "æ¨èé€‚åˆäº²å­æ¸¸çš„æµ·æ»¨åŸå¸‚ï¼Œé¢„ç®—ä¸é™ï¼Œ7å¤©æ—¶é—´",
            "category": "family_travel", 
            "difficulty": "easy"
        },
        {
            "input": "æˆ‘æ˜¯æ‘„å½±çˆ±å¥½è€…ï¼Œæƒ³å»æ‹æ‘„è‡ªç„¶é£å…‰ï¼Œé¢„ç®—5000å…ƒï¼Œæ—¶é—´çµæ´»",
            "category": "photography_travel",
            "difficulty": "hard"
        }
    ]
    
    # 3. åˆ›å»ºè¯„ä¼°æ•°æ®é›†
    dataset = collector.create_evaluation_dataset(
        name="travel-agent-benchmark-v1",
        test_cases=test_cases
    )
    
    # 4. è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†æ•°æ®
    session_id = f"test-session-{int(time.time())}"
    
    for i, case in enumerate(test_cases):
        print(f"\nğŸ“ å¤„ç†æµ‹è¯•ç”¨ä¾‹ {i+1}: {case['category']}")
        
        # æ‰§è¡Œæ—…è¡Œè§„åˆ’
        result = agent.plan_trip(case["input"])
        
        # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆ
        simulated_feedback = {
            "rating": 4.2,  # æ¨¡æ‹Ÿè¯„åˆ†
            "comment": "å›ç­”å¾ˆè¯¦ç»†ï¼Œå»ºè®®å¾ˆå®ç”¨"
        }
        
        # è®°å½•äº¤äº’æ•°æ®
        collector.log_user_interaction(
            session_id=session_id,
            user_input=case["input"],
            agent_output=json.dumps(result, ensure_ascii=False),
            user_feedback=simulated_feedback
        )
        
        print(f"âœ… å®Œæˆæµ‹è¯•ç”¨ä¾‹ {i+1}")
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œå®Œæˆï¼")
    print("ğŸ“Š è¯·è®¿é—® Langfuse æ§åˆ¶å°æŸ¥çœ‹è¯¦ç»†çš„è¿½è¸ªæ•°æ®")
    print("ğŸ”— https://cloud.langfuse.com")

if __name__ == "__main__":
    main()
```

### 4.7 æœ¬ç« å°ç»“

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸæ­å»ºäº†ï¼š

1. **å®Œæ•´çš„è¿½è¸ªåŸºç¡€è®¾æ–½**
   - âœ… Langfuseé›†æˆé…ç½®
   - âœ… è‡ªåŠ¨åŒ–æ•°æ®æ”¶é›†
   - âœ… è‡ªå®šä¹‰è¿½è¸ªè£…é¥°å™¨

2. **AIæ—…è¡Œæ™ºèƒ½ä½“ç³»ç»Ÿ**  
   - âœ… å•æ™ºèƒ½ä½“å®ç°
   - âœ… å¤šæ™ºèƒ½ä½“åä½œç‰ˆæœ¬
   - âœ… å®Œæ•´çš„ä¸šåŠ¡é€»è¾‘

3. **æ•°æ®æ”¶é›†ä½“ç³»**
   - âœ… ç”¨æˆ·äº¤äº’è®°å½•
   - âœ… æ€§èƒ½æŒ‡æ ‡è¿½è¸ª
   - âœ… è¯„ä¼°æ•°æ®é›†åˆ›å»º

**å…³é”®æ”¶è·**ï¼š
- ğŸ¯ **é›¶ä¾µå…¥é›†æˆ**ï¼šé€šè¿‡å›è°ƒå‡½æ•°è‡ªåŠ¨æ”¶é›†æ•°æ®
- ğŸ“Š **å…¨é“¾è·¯å¯è§‚æµ‹**ï¼šä»è¾“å…¥åˆ°è¾“å‡ºçš„å®Œæ•´è¿½è¸ª
- ğŸ› ï¸ **å¯æ‰©å±•æ¶æ„**ï¼šæ”¯æŒå¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†åŸºäºæ”¶é›†çš„æ•°æ®ï¼Œå®æ–½è‡ªåŠ¨åŒ–è¯„ä¼°å’Œæ•°æ®é©±åŠ¨åˆ†æã€‚

---

## ç¬¬äº”ç« ï¼šé¡¹ç›®å®æˆ˜ï¼ˆä¸­ï¼‰- è‡ªåŠ¨åŒ–è¯„ä¼°ä¸æ•°æ®é©±åŠ¨åˆ†æ

### 5.1 åŸºäºæ”¶é›†æ•°æ®çš„è¯„ä¼°ç­–ç•¥

åœ¨ç¬¬å››ç« ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸæ­å»ºäº†å¯è§‚æµ‹çš„è¯„ä¼°åŸºç¡€è®¾æ–½ï¼Œæ”¶é›†äº†å¤§é‡çš„è¿½è¸ªæ•°æ®ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦å°†è¿™äº›æ•°æ®è½¬åŒ–ä¸ºæœ‰ä»·å€¼çš„è¯„ä¼°æ´å¯Ÿã€‚

#### è¯„ä¼°æ•°æ®çš„ç±»å‹å’Œç”¨é€”

```python
# è¯„ä¼°æ•°æ®åˆ†ç±»
evaluation_data_types = {
    "æ€§èƒ½æ•°æ®": {
        "å»¶è¿ŸæŒ‡æ ‡": "å“åº”æ—¶é—´ã€å¤„ç†æ—¶é—´",
        "ååé‡æŒ‡æ ‡": "QPSã€å¹¶å‘å¤„ç†èƒ½åŠ›", 
        "èµ„æºä½¿ç”¨": "Tokenæ¶ˆè€—ã€APIè°ƒç”¨æ¬¡æ•°",
        "æˆæœ¬æŒ‡æ ‡": "æ¯æ¬¡å¯¹è¯æˆæœ¬ã€æ—¥å‡æˆæœ¬"
    },
    "è´¨é‡æ•°æ®": {
        "è¾“å‡ºè´¨é‡": "å‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€å®Œæ•´æ€§",
        "ç”¨æˆ·æ»¡æ„åº¦": "è¯„åˆ†ã€åé¦ˆã€æŠ•è¯‰ç‡",
        "å®‰å…¨æ€§": "æœ‰å®³å†…å®¹æ£€æµ‹ã€éšç§æ³„éœ²é£é™©",
        "ä¸€è‡´æ€§": "ç›¸åŒé—®é¢˜çš„å›ç­”ç¨³å®šæ€§"
    },
    "ä¸šåŠ¡æ•°æ®": {
        "ç”¨æˆ·è¡Œä¸º": "ä½¿ç”¨é¢‘ç‡ã€ä¼šè¯é•¿åº¦",
        "è½¬åŒ–æ•ˆæœ": "ç›®æ ‡å®Œæˆç‡ã€ç”¨æˆ·ç•™å­˜",
        "é”™è¯¯æ¨¡å¼": "å¤±è´¥åŸå› ã€å¼‚å¸¸åˆ†å¸ƒ"
    }
}
```

### 5.2 æ„å»ºè‡ªåŠ¨åŒ–è¯„ä¼°ä½“ç³»

#### LLM-as-a-Judge è¯„ä¼°å™¨

```python
from langchain.evaluation import load_evaluator
from langchain_openai import OpenAI
from langchain.evaluation.criteria import LabeledCriteriaEvalChain
import asyncio
from typing import List, Dict, Any

class TravelAgentEvaluator:
    """æ—…è¡Œæ™ºèƒ½ä½“ä¸“ä¸šè¯„ä¼°å™¨"""
    
    def __init__(self, eval_model: str = "gpt-4o"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            eval_model: ç”¨äºè¯„ä¼°çš„æ¨¡å‹åç§°
        """
        self.eval_llm = OpenAI(temperature=0, model=eval_model)
        self.langfuse = Langfuse()
        
        # å®šä¹‰è¯„ä¼°ç»´åº¦
        self.evaluation_criteria = {
            "accuracy": {
                "description": "æ—…è¡Œä¿¡æ¯çš„å‡†ç¡®æ€§",
                "weight": 0.25,
                "prompt_template": """
è¯„ä¼°ä»¥ä¸‹æ—…è¡Œå»ºè®®çš„å‡†ç¡®æ€§ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{input}
AIå›ç­”ï¼š{output}

è¯„ä¼°æ ‡å‡†ï¼š
5åˆ†ï¼šæ‰€æœ‰ä¿¡æ¯å‡†ç¡®æ— è¯¯ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€æ—¶é—´ã€åœ°ç‚¹ç­‰
4åˆ†ï¼šå¤§éƒ¨åˆ†ä¿¡æ¯å‡†ç¡®ï¼Œæœ‰è½»å¾®ä¸å‡†ç¡®
3åˆ†ï¼šåŸºæœ¬ä¿¡æ¯å‡†ç¡®ï¼Œéƒ¨åˆ†ç»†èŠ‚å¯èƒ½è¿‡æ—¶
2åˆ†ï¼šæœ‰æ˜æ˜¾çš„äº‹å®æ€§é”™è¯¯
1åˆ†ï¼šå¤§é‡é”™è¯¯ä¿¡æ¯ï¼Œä¸å¯ä¿¡

è¯·ç»™å‡ºåˆ†æ•°ï¼ˆ1-5ï¼‰å’Œè¯„ä¼°ç†ç”±ã€‚
                """
            },
            "relevance": {
                "description": "å›ç­”ä¸ç”¨æˆ·éœ€æ±‚çš„ç›¸å…³æ€§",
                "weight": 0.25,
                "prompt_template": """
è¯„ä¼°AIå›ç­”ä¸ç”¨æˆ·æ—…è¡Œéœ€æ±‚çš„ç›¸å…³æ€§ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{input}
AIå›ç­”ï¼š{output}

è¯„ä¼°æ ‡å‡†ï¼š
5åˆ†ï¼šå®Œå…¨é’ˆå¯¹ç”¨æˆ·éœ€æ±‚ï¼Œé«˜åº¦ç›¸å…³
4åˆ†ï¼šåŸºæœ¬ç¬¦åˆéœ€æ±‚ï¼Œç•¥æœ‰åç¦»
3åˆ†ï¼šéƒ¨åˆ†ç›¸å…³ï¼Œä½†æœ‰ä¸å¿…è¦çš„ä¿¡æ¯
2åˆ†ï¼šç›¸å…³æ€§è¾ƒä½ï¼Œåç¦»ä¸»é¢˜
1åˆ†ï¼šå®Œå…¨ä¸ç›¸å…³ï¼Œç­”éæ‰€é—®

è¯·ç»™å‡ºåˆ†æ•°ï¼ˆ1-5ï¼‰å’Œè¯„ä¼°ç†ç”±ã€‚
                """
            },
            "completeness": {
                "description": "æ—…è¡Œå»ºè®®çš„å®Œæ•´æ€§",
                "weight": 0.20,
                "prompt_template": """
è¯„ä¼°æ—…è¡Œå»ºè®®çš„å®Œæ•´æ€§ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{input}
AIå›ç­”ï¼š{output}

è¯„ä¼°æ ‡å‡†ï¼š
5åˆ†ï¼šæ¶µç›–æ‰€æœ‰å¿…è¦ä¿¡æ¯ï¼ˆè¡Œç¨‹ã€ä½å®¿ã€äº¤é€šã€é¢„ç®—ç­‰ï¼‰
4åˆ†ï¼šæ¶µç›–ä¸»è¦ä¿¡æ¯ï¼Œå°‘é‡é—æ¼
3åˆ†ï¼šåŸºæœ¬ä¿¡æ¯å®Œæ•´ï¼Œç¼ºå°‘éƒ¨åˆ†ç»†èŠ‚
2åˆ†ï¼šä¿¡æ¯ä¸å¤Ÿå®Œæ•´ï¼Œç¼ºå°‘é‡è¦å†…å®¹
1åˆ†ï¼šä¿¡æ¯ä¸¥é‡ä¸å®Œæ•´ï¼Œæ— æ³•å®ç”¨

è¯·ç»™å‡ºåˆ†æ•°ï¼ˆ1-5ï¼‰å’Œè¯„ä¼°ç†ç”±ã€‚
                """
            },
            "practicality": {
                "description": "å»ºè®®çš„å®ç”¨æ€§å’Œå¯æ“ä½œæ€§",
                "weight": 0.20,
                "prompt_template": """
è¯„ä¼°æ—…è¡Œå»ºè®®çš„å®ç”¨æ€§ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{input}
AIå›ç­”ï¼š{output}

è¯„ä¼°æ ‡å‡†ï¼š
5åˆ†ï¼šå»ºè®®éå¸¸å®ç”¨ï¼Œæ˜“äºæ‰§è¡Œï¼Œæ—¶é—´å®‰æ’åˆç†
4åˆ†ï¼šå»ºè®®æ¯”è¾ƒå®ç”¨ï¼ŒåŸºæœ¬å¯è¡Œ
3åˆ†ï¼šå»ºè®®ä¸€èˆ¬å®ç”¨ï¼Œéœ€è¦è°ƒæ•´
2åˆ†ï¼šå»ºè®®å®ç”¨æ€§è¾ƒå·®ï¼Œéš¾ä»¥æ‰§è¡Œ
1åˆ†ï¼šå»ºè®®ä¸å®ç”¨ï¼Œæ— æ³•æ‰§è¡Œ

è¯·ç»™å‡ºåˆ†æ•°ï¼ˆ1-5ï¼‰å’Œè¯„ä¼°ç†ç”±ã€‚
                """
            },
            "personalization": {
                "description": "ä¸ªæ€§åŒ–ç¨‹åº¦",
                "weight": 0.10,
                "prompt_template": """
è¯„ä¼°å›ç­”çš„ä¸ªæ€§åŒ–ç¨‹åº¦ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{input}
AIå›ç­”ï¼š{output}

è¯„ä¼°æ ‡å‡†ï¼š
5åˆ†ï¼šé«˜åº¦ä¸ªæ€§åŒ–ï¼Œå……åˆ†è€ƒè™‘ç”¨æˆ·å…·ä½“éœ€æ±‚
4åˆ†ï¼šè¾ƒå¥½ä¸ªæ€§åŒ–ï¼Œè€ƒè™‘äº†ä¸»è¦éœ€æ±‚
3åˆ†ï¼šåŸºæœ¬ä¸ªæ€§åŒ–ï¼Œæœ‰ä¸€å®šé’ˆå¯¹æ€§
2åˆ†ï¼šä¸ªæ€§åŒ–ä¸è¶³ï¼Œæ¯”è¾ƒé€šç”¨
1åˆ†ï¼šå®Œå…¨é€šç”¨ï¼Œæ²¡æœ‰ä¸ªæ€§åŒ–

è¯·ç»™å‡ºåˆ†æ•°ï¼ˆ1-5ï¼‰å’Œè¯„ä¼°ç†ç”±ã€‚
                """
            }
        }
    
    def evaluate_single_response(self, input_text: str, output_text: str, 
                                criteria: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªå›ç­”çš„ç‰¹å®šç»´åº¦
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            output_text: AIè¾“å‡º  
            criteria: è¯„ä¼°ç»´åº¦
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        
        if criteria not in self.evaluation_criteria:
            raise ValueError(f"æœªçŸ¥çš„è¯„ä¼°ç»´åº¦ï¼š{criteria}")
        
        criterion_config = self.evaluation_criteria[criteria]
        prompt = criterion_config["prompt_template"].format(
            input=input_text,
            output=output_text
        )
        
        try:
            # è°ƒç”¨è¯„ä¼°æ¨¡å‹
            result = self.eval_llm.invoke(prompt)
            
            # è§£æè¯„åˆ†ï¼ˆç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è§£æï¼‰
            import re
            score_match = re.search(r'(\d)åˆ†', result)
            score = int(score_match.group(1)) if score_match else 3
            
            return {
                "criteria": criteria,
                "score": score,
                "reasoning": result,
                "weight": criterion_config["weight"]
            }
            
        except Exception as e:
            return {
                "criteria": criteria,
                "score": 0,
                "reasoning": f"è¯„ä¼°å¤±è´¥ï¼š{str(e)}",
                "weight": criterion_config["weight"],
                "error": True
            }
    
    def evaluate_comprehensive(self, input_text: str, output_text: str) -> Dict[str, Any]:
        """
        ç»¼åˆè¯„ä¼°å•ä¸ªå›ç­”
        
        Args:
            input_text: ç”¨æˆ·è¾“å…¥
            output_text: AIè¾“å‡º
            
        Returns:
            ç»¼åˆè¯„ä¼°ç»“æœ
        """
        
        results = {}
        total_weighted_score = 0
        total_weight = 0
        
        # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°
        for criteria in self.evaluation_criteria.keys():
            eval_result = self.evaluate_single_response(
                input_text, output_text, criteria
            )
            results[criteria] = eval_result
            
            if not eval_result.get("error", False):
                weight = eval_result["weight"]
                score = eval_result["score"]
                total_weighted_score += score * weight
                total_weight += weight
        
        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = total_weighted_score / total_weight if total_weight > 0 else 0
        
        return {
            "overall_score": overall_score,
            "detailed_scores": results,
            "metadata": {
                "evaluation_model": self.eval_llm.model,
                "total_criteria": len(self.evaluation_criteria),
                "successful_evaluations": len([r for r in results.values() if not r.get("error", False)])
            }
        }
    
    async def batch_evaluate(self, test_cases: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡è¯„ä¼°å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
        
        Args:
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«inputå’Œoutput
            
        Returns:
            æ‰¹é‡è¯„ä¼°ç»“æœ
        """
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        
        results = []
        for i, case in enumerate(test_cases):
            print(f"ğŸ“ è¯„ä¼°ç¬¬ {i+1}/{len(test_cases)} ä¸ªç”¨ä¾‹")
            
            eval_result = self.evaluate_comprehensive(
                case["input"], 
                case["output"]
            )
            
            # æ·»åŠ æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯
            eval_result.update({
                "case_id": i,
                "input": case["input"],
                "output": case["output"],
                "category": case.get("category", "unknown")
            })
            
            results.append(eval_result)
            
            # å°†ç»“æœè®°å½•åˆ°Langfuse
            self._log_evaluation_to_langfuse(eval_result)
        
        print("âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ")
        return results
    
    def _log_evaluation_to_langfuse(self, eval_result: Dict[str, Any]):
        """å°†è¯„ä¼°ç»“æœè®°å½•åˆ°Langfuse"""
        
        try:
            # åˆ›å»ºè¯„ä¼°è®°å½•
            with self.langfuse.start_as_current_span(
                name="evaluation-result"
            ) as span:
                
                span.update_trace(
                    input=eval_result["input"],
                    output=eval_result["output"],
                    metadata={
                        "evaluation_details": eval_result["detailed_scores"],
                        "category": eval_result.get("category")
                    }
                )
                
                # è®°å½•ç»¼åˆå¾—åˆ†
                span.score_trace(
                    name="comprehensive-score",
                    value=eval_result["overall_score"],
                    comment=f"ç»¼åˆè¯„ä¼°å¾—åˆ†ï¼ŒåŸºäº{len(eval_result['detailed_scores'])}ä¸ªç»´åº¦"
                )
                
                # è®°å½•å„ç»´åº¦å¾—åˆ†
                for criteria, details in eval_result["detailed_scores"].items():
                    if not details.get("error", False):
                        span.score_trace(
                            name=f"score-{criteria}",
                            value=details["score"],
                            comment=details["reasoning"][:200]  # é™åˆ¶é•¿åº¦
                        )
                        
        except Exception as e:
            print(f"âš ï¸ è®°å½•è¯„ä¼°ç»“æœåˆ°Langfuseå¤±è´¥ï¼š{str(e)}")
```

#### æ€§èƒ½è¯„ä¼°æ¨¡å—

```python
import time
import statistics
from datetime import datetime, timedelta

class PerformanceEvaluator:
    """æ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.langfuse = Langfuse()
        self.performance_data = []
    
    def measure_response_time(self, func, *args, **kwargs):
        """
        æµ‹é‡å‡½æ•°å“åº”æ—¶é—´
        
        Args:
            func: è¦æµ‹è¯•çš„å‡½æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°
            
        Returns:
            (result, performance_metrics)
        """
        
        start_time = time.time()
        start_memory = self._get_memory_usage()
        
        try:
            result = func(*args, **kwargs)
            
            end_time = time.time()
            end_memory = self._get_memory_usage()
            
            metrics = {
                "response_time": end_time - start_time,
                "memory_delta": end_memory - start_memory,
                "status": "success",
                "timestamp": datetime.now().isoformat()
            }
            
            return result, metrics
            
        except Exception as e:
            end_time = time.time()
            
            metrics = {
                "response_time": end_time - start_time,
                "memory_delta": 0,
                "status": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
            
            raise e, metrics
    
    def _get_memory_usage(self):
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def benchmark_agent(self, agent, test_cases: List[str], 
                       iterations: int = 3) -> Dict[str, Any]:
        """
        å¯¹æ™ºèƒ½ä½“è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            agent: å¾…æµ‹è¯•çš„æ™ºèƒ½ä½“
            test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
            iterations: æ¯ä¸ªç”¨ä¾‹çš„é‡å¤æ¬¡æ•°
            
        Returns:
            æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
        """
        
        print(f"ğŸƒâ€â™‚ï¸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"ğŸ“Š æµ‹è¯•ç”¨ä¾‹æ•°ï¼š{len(test_cases)}")
        print(f"ğŸ”„ æ¯ç”¨ä¾‹é‡å¤ï¼š{iterations} æ¬¡")
        
        all_metrics = []
        
        for i, test_case in enumerate(test_cases):
            print(f"\nğŸ“ æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}")
            
            case_metrics = []
            
            for iteration in range(iterations):
                print(f"  ğŸ”„ ç¬¬ {iteration+1}/{iterations} æ¬¡")
                
                try:
                    result, metrics = self.measure_response_time(
                        agent.plan_trip, test_case
                    )
                    
                    # æ·»åŠ é¢å¤–ä¿¡æ¯
                    metrics.update({
                        "case_id": i,
                        "iteration": iteration,
                        "test_case": test_case[:100],  # æˆªæ–­é•¿æ–‡æœ¬
                        "result_length": len(str(result)) if result else 0
                    })
                    
                    case_metrics.append(metrics)
                    all_metrics.append(metrics)
                    
                except Exception as e:
                    print(f"    âŒ æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
                    
                # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…APIé™æµ
                time.sleep(1)
        
        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        performance_stats = self._calculate_performance_stats(all_metrics)
        
        # è®°å½•åˆ°Langfuse
        self._log_benchmark_results(performance_stats)
        
        return performance_stats
    
    def _calculate_performance_stats(self, metrics: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½ç»Ÿè®¡æŒ‡æ ‡"""
        
        successful_metrics = [m for m in metrics if m["status"] == "success"]
        failed_metrics = [m for m in metrics if m["status"] == "error"]
        
        if not successful_metrics:
            return {
                "summary": "æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†",
                "success_rate": 0,
                "total_tests": len(metrics),
                "failed_tests": len(failed_metrics)
            }
        
        response_times = [m["response_time"] for m in successful_metrics]
        memory_deltas = [m["memory_delta"] for m in successful_metrics]
        
        stats = {
            "summary": {
                "total_tests": len(metrics),
                "successful_tests": len(successful_metrics),
                "failed_tests": len(failed_metrics),
                "success_rate": len(successful_metrics) / len(metrics) * 100
            },
            "response_time": {
                "mean": statistics.mean(response_times),
                "median": statistics.median(response_times),
                "min": min(response_times),
                "max": max(response_times),
                "std_dev": statistics.stdev(response_times) if len(response_times) > 1 else 0,
                "percentile_95": self._percentile(response_times, 95),
                "percentile_99": self._percentile(response_times, 99)
            },
            "memory_usage": {
                "mean_delta": statistics.mean(memory_deltas),
                "max_delta": max(memory_deltas),
                "min_delta": min(memory_deltas)
            },
            "detailed_metrics": metrics
        }
        
        return stats
    
    def _percentile(self, data: List[float], percentile: int) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        if index == int(index):
            return sorted_data[int(index)]
        else:
            lower = sorted_data[int(index)]
            upper = sorted_data[int(index) + 1]
            return lower + (upper - lower) * (index - int(index))
    
    def _log_benchmark_results(self, stats: Dict[str, Any]):
        """å°†åŸºå‡†æµ‹è¯•ç»“æœè®°å½•åˆ°Langfuse"""
        
        try:
            with self.langfuse.start_as_current_span(
                name="performance-benchmark"
            ) as span:
                
                span.update_trace(
                    input={"benchmark_type": "agent_performance"},
                    output=stats["summary"],
                    metadata=stats
                )
                
                # è®°å½•å…³é”®æ€§èƒ½æŒ‡æ ‡
                span.score_trace(
                    name="success-rate",
                    value=stats["summary"]["success_rate"],
                    comment=f"æˆåŠŸç‡ï¼š{stats['summary']['successful_tests']}/{stats['summary']['total_tests']}"
                )
                
                span.score_trace(
                    name="mean-response-time",
                    value=stats["response_time"]["mean"],
                    comment=f"å¹³å‡å“åº”æ—¶é—´ï¼š{stats['response_time']['mean']:.2f}ç§’"
                )
                
                span.score_trace(
                    name="p95-response-time", 
                    value=stats["response_time"]["percentile_95"],
                    comment=f"95%åˆ†ä½å“åº”æ—¶é—´ï¼š{stats['response_time']['percentile_95']:.2f}ç§’"
                )
                
        except Exception as e:
            print(f"âš ï¸ è®°å½•æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœå¤±è´¥ï¼š{str(e)}")
```

### 5.3 æ•°æ®é©±åŠ¨çš„åˆ†ææŠ¥å‘Š

#### æ•°æ®åˆ†æå¼•æ“

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any
import numpy as np

class DataAnalysisEngine:
    """æ•°æ®åˆ†æå¼•æ“"""
    
    def __init__(self):
        self.langfuse = Langfuse()
    
    def fetch_evaluation_data(self, start_date: str = None, 
                            end_date: str = None) -> pd.DataFrame:
        """
        ä»Langfuseè·å–è¯„ä¼°æ•°æ®
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            åŒ…å«è¯„ä¼°æ•°æ®çš„DataFrame
        """
        
        print("ğŸ“Š æ­£åœ¨ä»Langfuseè·å–è¯„ä¼°æ•°æ®...")
        
        try:
            # è·å–è¿½è¸ªæ•°æ®
            traces = self.langfuse.api.trace.list(
                limit=1000,  # æ ¹æ®éœ€è¦è°ƒæ•´
                # å¯ä»¥æ·»åŠ æ›´å¤šè¿‡æ»¤æ¡ä»¶
            )
            
            data_records = []
            
            for trace in traces.data:
                # æå–åŸºæœ¬ä¿¡æ¯
                record = {
                    "trace_id": trace.id,
                    "timestamp": trace.timestamp,
                    "session_id": getattr(trace, 'session_id', None),
                    "input": getattr(trace, 'input', ''),
                    "output": getattr(trace, 'output', ''),
                    "latency": getattr(trace, 'latency', None),
                    "total_cost": getattr(trace, 'total_cost', 0),
                    "token_usage": getattr(trace, 'token_usage', 0)
                }
                
                # æå–è¯„åˆ†æ•°æ®
                if hasattr(trace, 'scores') and trace.scores:
                    for score in trace.scores:
                        record[f"score_{score.name}"] = score.value
                
                data_records.append(record)
            
            df = pd.DataFrame(data_records)
            
            if not df.empty:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                
            print(f"âœ… æˆåŠŸè·å– {len(df)} æ¡è¯„ä¼°è®°å½•")
            return df
            
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®å¤±è´¥ï¼š{str(e)}")
            return pd.DataFrame()
    
    def analyze_quality_trends(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†æè´¨é‡è¶‹åŠ¿
        
        Args:
            df: è¯„ä¼°æ•°æ®DataFrame
            
        Returns:
            è´¨é‡è¶‹åŠ¿åˆ†æç»“æœ
        """
        
        print("ğŸ“ˆ åˆ†æè´¨é‡è¶‹åŠ¿...")
        
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ"}
        
        # è¯†åˆ«è¯„åˆ†åˆ—
        score_columns = [col for col in df.columns if col.startswith('score_')]
        
        if not score_columns:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°è¯„åˆ†æ•°æ®"}
        
        # æŒ‰æ—¥æœŸèšåˆ
        df['date'] = df['timestamp'].dt.date
        daily_scores = df.groupby('date')[score_columns].mean()
        
        # è®¡ç®—è¶‹åŠ¿
        trends = {}
        for col in score_columns:
            if col in daily_scores.columns:
                values = daily_scores[col].dropna()
                if len(values) > 1:
                    # ç®€å•çº¿æ€§è¶‹åŠ¿è®¡ç®—
                    x = np.arange(len(values))
                    trend_slope = np.polyfit(x, values, 1)[0]
                    trends[col] = {
                        "current_avg": values.iloc[-1] if len(values) > 0 else 0,
                        "overall_avg": values.mean(),
                        "trend_slope": trend_slope,
                        "trend_direction": "ä¸Šå‡" if trend_slope > 0 else "ä¸‹é™" if trend_slope < 0 else "ç¨³å®š"
                    }
        
        return {
            "daily_scores": daily_scores.to_dict(),
            "trends": trends,
            "analysis_period": {
                "start_date": str(df['date'].min()),
                "end_date": str(df['date'].max()),
                "total_days": len(daily_scores)
            }
        }
    
    def analyze_performance_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        åˆ†ææ€§èƒ½æ¨¡å¼
        
        Args:
            df: è¯„ä¼°æ•°æ®DataFrame
            
        Returns:
            æ€§èƒ½æ¨¡å¼åˆ†æç»“æœ
        """
        
        print("âš¡ åˆ†ææ€§èƒ½æ¨¡å¼...")
        
        if df.empty or 'latency' not in df.columns:
            return {"error": "æ²¡æœ‰æ€§èƒ½æ•°æ®å¯ä¾›åˆ†æ"}
        
        # è¿‡æ»¤æœ‰æ•ˆçš„å»¶è¿Ÿæ•°æ®
        latency_data = df[df['latency'].notna() & (df['latency'] > 0)]
        
        if latency_data.empty:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„å»¶è¿Ÿæ•°æ®"}
        
        # åŸºæœ¬ç»Ÿè®¡
        latency_stats = {
            "mean": latency_data['latency'].mean(),
            "median": latency_data['latency'].median(),
            "std": latency_data['latency'].std(),
            "min": latency_data['latency'].min(),
            "max": latency_data['latency'].max(),
            "p95": latency_data['latency'].quantile(0.95),
            "p99": latency_data['latency'].quantile(0.99)
        }
        
        # æŒ‰æ—¶é—´æ®µåˆ†æ
        latency_data['hour'] = latency_data['timestamp'].dt.hour
        hourly_latency = latency_data.groupby('hour')['latency'].agg(['mean', 'count'])
        
        # è¯†åˆ«æ€§èƒ½å¼‚å¸¸
        q75 = latency_data['latency'].quantile(0.75)
        q25 = latency_data['latency'].quantile(0.25)
        iqr = q75 - q25
        outlier_threshold = q75 + 1.5 * iqr
        
        outliers = latency_data[latency_data['latency'] > outlier_threshold]
        
        return {
            "latency_statistics": latency_stats,
            "hourly_patterns": hourly_latency.to_dict(),
            "outliers": {
                "count": len(outliers),
                "threshold": outlier_threshold,
                "examples": outliers[['timestamp', 'latency']].head().to_dict('records')
            },
            "performance_classification": self._classify_performance(latency_stats["mean"])
        }
    
    def _classify_performance(self, mean_latency: float) -> str:
        """æ ¹æ®å¹³å‡å»¶è¿Ÿå¯¹æ€§èƒ½è¿›è¡Œåˆ†ç±»"""
        if mean_latency < 2:
            return "ä¼˜ç§€"
        elif mean_latency < 5:
            return "è‰¯å¥½"
        elif mean_latency < 10:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦ä¼˜åŒ–"
    
    def generate_cost_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        ç”Ÿæˆæˆæœ¬åˆ†ææŠ¥å‘Š
        
        Args:
            df: è¯„ä¼°æ•°æ®DataFrame
            
        Returns:
            æˆæœ¬åˆ†æç»“æœ
        """
        
        print("ğŸ’° åˆ†ææˆæœ¬æ¨¡å¼...")
        
        if df.empty:
            return {"error": "æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ"}
        
        # æˆæœ¬æ•°æ®åˆ†æ
        cost_data = df[df['total_cost'].notna() & (df['total_cost'] > 0)]
        
        if cost_data.empty:
            return {"error": "æ²¡æœ‰æˆæœ¬æ•°æ®"}
        
        # åŸºæœ¬æˆæœ¬ç»Ÿè®¡
        cost_stats = {
            "total_cost": cost_data['total_cost'].sum(),
            "average_cost_per_request": cost_data['total_cost'].mean(),
            "median_cost_per_request": cost_data['total_cost'].median(),
            "max_cost_per_request": cost_data['total_cost'].max(),
            "min_cost_per_request": cost_data['total_cost'].min()
        }
        
        # æŒ‰æ—¥æœŸç»Ÿè®¡æˆæœ¬
        cost_data['date'] = cost_data['timestamp'].dt.date
        daily_cost = cost_data.groupby('date')['total_cost'].agg(['sum', 'count', 'mean'])
        
        # æˆæœ¬é¢„æµ‹ï¼ˆåŸºäºå½“å‰è¶‹åŠ¿ï¼‰
        if len(daily_cost) > 1:
            avg_daily_cost = daily_cost['sum'].mean()
            projected_monthly_cost = avg_daily_cost * 30
            projected_yearly_cost = avg_daily_cost * 365
        else:
            projected_monthly_cost = 0
            projected_yearly_cost = 0
        
        return {
            "cost_statistics": cost_stats,
            "daily_cost_trend": daily_cost.to_dict(),
            "projections": {
                "monthly_cost_estimate": projected_monthly_cost,
                "yearly_cost_estimate": projected_yearly_cost
            },
            "cost_optimization_suggestions": self._generate_cost_optimization_suggestions(cost_stats)
        }
    
    def _generate_cost_optimization_suggestions(self, cost_stats: Dict) -> List[str]:
        """ç”Ÿæˆæˆæœ¬ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        avg_cost = cost_stats["average_cost_per_request"]
        
        if avg_cost > 0.1:  # å‡è®¾é˜ˆå€¼
            suggestions.append("å¹³å‡è¯·æ±‚æˆæœ¬è¾ƒé«˜ï¼Œè€ƒè™‘ä½¿ç”¨æ›´ç»æµçš„æ¨¡å‹")
        
        if cost_stats["max_cost_per_request"] > avg_cost * 5:
            suggestions.append("å­˜åœ¨å¼‚å¸¸é«˜æˆæœ¬è¯·æ±‚ï¼Œéœ€è¦è°ƒæŸ¥åŸå› ")
        
        suggestions.append("å®šæœŸç›‘æ§æˆæœ¬è¶‹åŠ¿ï¼Œè®¾ç½®é¢„ç®—è­¦æŠ¥")
        suggestions.append("è€ƒè™‘å®æ–½è¯·æ±‚ç¼“å­˜æ¥å‡å°‘é‡å¤è°ƒç”¨")
        
        return suggestions

```

### 5.4 å®Œæ•´çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

#### è¯„ä¼°è°ƒåº¦å™¨

```python
import schedule
import threading
from datetime import datetime, timedelta

class EvaluationScheduler:
    """è¯„ä¼°è°ƒåº¦å™¨ - è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹ç®¡ç†"""
    
    def __init__(self):
        self.langfuse = Langfuse()
        self.evaluator = TravelAgentEvaluator()
        self.performance_evaluator = PerformanceEvaluator()
        self.data_analyzer = DataAnalysisEngine()
        
        self.is_running = False
        self.scheduler_thread = None
    
    def start_scheduled_evaluation(self):
        """å¯åŠ¨å®šæœŸè¯„ä¼°ä»»åŠ¡"""
        
        print("ğŸ•’ å¯åŠ¨å®šæœŸè¯„ä¼°è°ƒåº¦å™¨")
        
        # é…ç½®è¯„ä¼°ä»»åŠ¡è®¡åˆ’
        schedule.every().day.at("02:00").do(self.daily_evaluation)
        schedule.every().week.do(self.weekly_performance_review)
        schedule.every().month.do(self.monthly_comprehensive_report)
        
        # å¯åŠ¨è°ƒåº¦å™¨çº¿ç¨‹
        self.is_running = True
        self.scheduler_thread = threading.Thread(target=self._run_scheduler)
        self.scheduler_thread.daemon = True
        self.scheduler_thread.start()
        
        print("âœ… è¯„ä¼°è°ƒåº¦å™¨å·²å¯åŠ¨")
    
    def _run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        while self.is_running:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    def daily_evaluation(self):
        """æ¯æ—¥è¯„ä¼°ä»»åŠ¡"""
        print("\nğŸŒ… æ‰§è¡Œæ¯æ—¥è¯„ä¼°ä»»åŠ¡")
        
        try:
            # è·å–æ˜¨å¤©çš„æ•°æ®
            yesterday = datetime.now() - timedelta(days=1)
            start_date = yesterday.strftime('%Y-%m-%d')
            end_date = start_date
            
            # è·å–æ•°æ®
            df = self.data_analyzer.fetch_evaluation_data(start_date, end_date)
            
            if df.empty:
                print("ğŸ“Š æ˜¨æ—¥æ— è¯„ä¼°æ•°æ®")
                return
            
            # è´¨é‡è¶‹åŠ¿åˆ†æ
            quality_analysis = self.data_analyzer.analyze_quality_trends(df)
            
            # æ€§èƒ½åˆ†æ
            performance_analysis = self.data_analyzer.analyze_performance_patterns(df)
            
            # æˆæœ¬åˆ†æ
            cost_analysis = self.data_analyzer.generate_cost_analysis(df)
            
            # ç”Ÿæˆæ—¥æŠ¥
            daily_report = {
                "date": start_date,
                "summary": {
                    "total_requests": len(df),
                    "avg_quality_score": df[[col for col in df.columns if col.startswith('score_')]].mean().mean() if any(col.startswith('score_') for col in df.columns) else 0,
                    "avg_latency": df['latency'].mean() if 'latency' in df.columns else 0,
                    "total_cost": df['total_cost'].sum() if 'total_cost' in df.columns else 0
                },
                "quality_analysis": quality_analysis,
                "performance_analysis": performance_analysis,
                "cost_analysis": cost_analysis
            }
            
            # è®°å½•åˆ°Langfuse
            self._log_daily_report(daily_report)
            
            print("âœ… æ¯æ—¥è¯„ä¼°å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¯æ—¥è¯„ä¼°å¤±è´¥ï¼š{str(e)}")
    
    def weekly_performance_review(self):
        """æ¯å‘¨æ€§èƒ½å›é¡¾"""
        print("\nğŸ“Š æ‰§è¡Œæ¯å‘¨æ€§èƒ½å›é¡¾")
        
        try:
            # è·å–è¿‡å»7å¤©çš„æ•°æ®
            end_date = datetime.now()
            start_date = end_date - timedelta(days=7)
            
            df = self.data_analyzer.fetch_evaluation_data(
                start_date.strftime('%Y-%m-%d'),
                end_date.strftime('%Y-%m-%d')
            )
            
            if df.empty:
                print("ğŸ“Š æœ¬å‘¨æ— æ•°æ®å¯åˆ†æ")
                return
            
            # å‘¨åº¦æ€§èƒ½åˆ†æ
            weekly_analysis = {
                "week_start": start_date.strftime('%Y-%m-%d'),
                "week_end": end_date.strftime('%Y-%m-%d'),
                "metrics": {
                    "total_requests": len(df),
                    "daily_average": len(df) / 7,
                    "success_rate": len(df[df['latency'].notna()]) / len(df) * 100 if 'latency' in df.columns else 0,
                    "performance_trends": self.data_analyzer.analyze_performance_patterns(df),
                    "quality_trends": self.data_analyzer.analyze_quality_trends(df)
                }
            }
            
            # æ€§èƒ½è­¦æŠ¥æ£€æŸ¥
            alerts = self._check_performance_alerts(weekly_analysis)
            
            if alerts:
                print(f"âš ï¸ å‘ç° {len(alerts)} ä¸ªæ€§èƒ½è­¦æŠ¥")
                for alert in alerts:
                    print(f"  - {alert}")
            
            self._log_weekly_report(weekly_analysis, alerts)
            
            print("âœ… æ¯å‘¨æ€§èƒ½å›é¡¾å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¯å‘¨æ€§èƒ½å›é¡¾å¤±è´¥ï¼š{str(e)}")
    
    def monthly_comprehensive_report(self):
        """æ¯æœˆç»¼åˆæŠ¥å‘Š"""
        print("\nğŸ“ˆ ç”Ÿæˆæœˆåº¦ç»¼åˆæŠ¥å‘Š")
        
        try:
            # è·å–è¿‡å»30å¤©çš„æ•°æ®
            end_date = datetime.now()
            start_date = end_date - timedelta(days=30)
            
            df = self.data_analyzer.fetch_evaluation_data(
                start_date.strftime('%Y-%m-%d'),
                end_date.strftime('%Y-%m-%d')
            )
            
            if df.empty:
                print("ğŸ“Š æœ¬æœˆæ— æ•°æ®å¯åˆ†æ")
                return
            
            # æœˆåº¦ç»¼åˆåˆ†æ
            monthly_report = {
                "month_start": start_date.strftime('%Y-%m-%d'),
                "month_end": end_date.strftime('%Y-%m-%d'),
                "executive_summary": self._generate_executive_summary(df),
                "detailed_analysis": {
                    "quality_trends": self.data_analyzer.analyze_quality_trends(df),
                    "performance_patterns": self.data_analyzer.analyze_performance_patterns(df),
                    "cost_analysis": self.data_analyzer.generate_cost_analysis(df)
                },
                "recommendations": self._generate_monthly_recommendations(df),
                "kpis": self._calculate_monthly_kpis(df)
            }
            
            self._log_monthly_report(monthly_report)
            
            print("âœ… æœˆåº¦ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æœˆåº¦æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
    
    def _check_performance_alerts(self, analysis: Dict) -> List[str]:
        """æ£€æŸ¥æ€§èƒ½è­¦æŠ¥"""
        alerts = []
        
        metrics = analysis.get("metrics", {})
        
        # æˆåŠŸç‡è­¦æŠ¥
        success_rate = metrics.get("success_rate", 100)
        if success_rate < 95:
            alerts.append(f"æˆåŠŸç‡è¿‡ä½ï¼š{success_rate:.1f}%ï¼ˆæ­£å¸¸åº”>95%ï¼‰")
        
        # å»¶è¿Ÿè­¦æŠ¥
        perf_trends = metrics.get("performance_trends", {})
        if "latency_statistics" in perf_trends:
            avg_latency = perf_trends["latency_statistics"].get("mean", 0)
            if avg_latency > 10:
                alerts.append(f"å¹³å‡å»¶è¿Ÿè¿‡é«˜ï¼š{avg_latency:.1f}ç§’ï¼ˆæ­£å¸¸åº”<10ç§’ï¼‰")
        
        # è´¨é‡åˆ†æ•°è­¦æŠ¥
        quality_trends = metrics.get("quality_trends", {})
        if "trends" in quality_trends:
            for score_type, trend in quality_trends["trends"].items():
                if trend.get("current_avg", 5) < 3:
                    alerts.append(f"{score_type}è¯„åˆ†è¿‡ä½ï¼š{trend['current_avg']:.1f}ï¼ˆæ­£å¸¸åº”>3.0ï¼‰")
        
        return alerts
    
    def _generate_executive_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        
        total_requests = len(df)
        avg_latency = df['latency'].mean() if 'latency' in df.columns and not df['latency'].isna().all() else 0
        total_cost = df['total_cost'].sum() if 'total_cost' in df.columns else 0
        
        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        score_columns = [col for col in df.columns if col.startswith('score_')]
        avg_quality = df[score_columns].mean().mean() if score_columns else 0
        
        return {
            "key_metrics": {
                "total_requests": total_requests,
                "average_latency": round(avg_latency, 2),
                "total_cost": round(total_cost, 4),
                "average_quality_score": round(avg_quality, 2)
            },
            "performance_summary": f"æœ¬æœˆå¤„ç†äº†{total_requests}ä¸ªè¯·æ±‚ï¼Œå¹³å‡å»¶è¿Ÿ{avg_latency:.1f}ç§’ï¼Œæ€»æˆæœ¬${total_cost:.4f}ï¼Œå¹³å‡è´¨é‡è¯„åˆ†{avg_quality:.1f}/5.0",
            "status": "è‰¯å¥½" if avg_latency < 5 and avg_quality > 3.5 else "éœ€è¦å…³æ³¨"
        }
    
    def _generate_monthly_recommendations(self, df: pd.DataFrame) -> List[str]:
        """ç”Ÿæˆæœˆåº¦æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ•°æ®ç”Ÿæˆå»ºè®®
        avg_latency = df['latency'].mean() if 'latency' in df.columns and not df['latency'].isna().all() else 0
        total_cost = df['total_cost'].sum() if 'total_cost' in df.columns else 0
        
        if avg_latency > 5:
            recommendations.append("ä¼˜åŒ–å“åº”é€Ÿåº¦ï¼šå½“å‰å¹³å‡å»¶è¿Ÿè¿‡é«˜ï¼Œå»ºè®®ä¼˜åŒ–æ¨¡å‹è°ƒç”¨æˆ–å¢åŠ ç¼“å­˜")
        
        if total_cost > 50:  # å‡è®¾æœˆé¢„ç®—é˜ˆå€¼
            recommendations.append("æ§åˆ¶æˆæœ¬ï¼šæœˆåº¦æˆæœ¬è¾ƒé«˜ï¼Œè€ƒè™‘ä½¿ç”¨æ›´ç»æµçš„æ¨¡å‹æˆ–ä¼˜åŒ–prompt")
        
        # è´¨é‡ç›¸å…³å»ºè®®
        score_columns = [col for col in df.columns if col.startswith('score_')]
        if score_columns:
            avg_scores = df[score_columns].mean()
            low_score_metrics = avg_scores[avg_scores < 3.5]
            if not low_score_metrics.empty:
                recommendations.append(f"æå‡è´¨é‡ï¼š{list(low_score_metrics.index)}æŒ‡æ ‡åä½ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")
        
        recommendations.append("æŒç»­ç›‘æ§ï¼šå»ºç«‹æ›´ç»†ç²’åº¦çš„ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°é—®é¢˜")
        
        return recommendations
    
    def _calculate_monthly_kpis(self, df: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—æœˆåº¦KPI"""
        
        kpis = {}
        
        # å¯ç”¨æ€§KPI
        total_requests = len(df)
        successful_requests = len(df[df['latency'].notna()]) if 'latency' in df.columns else total_requests
        kpis['availability'] = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        # æ€§èƒ½KPI
        if 'latency' in df.columns and not df['latency'].isna().all():
            kpis['avg_response_time'] = df['latency'].mean()
            kpis['p95_response_time'] = df['latency'].quantile(0.95)
        
        # è´¨é‡KPI
        score_columns = [col for col in df.columns if col.startswith('score_')]
        if score_columns:
            kpis['avg_quality_score'] = df[score_columns].mean().mean()
        
        # æˆæœ¬KPI
        if 'total_cost' in df.columns:
            kpis['total_cost'] = df['total_cost'].sum()
            kpis['cost_per_request'] = df['total_cost'].mean()
        
        return kpis
    
    def _log_daily_report(self, report: Dict[str, Any]):
        """è®°å½•æ—¥æŠ¥åˆ°Langfuse"""
        try:
            with self.langfuse.start_as_current_span(name="daily-evaluation-report") as span:
                span.update_trace(
                    input={"report_type": "daily", "date": report["date"]},
                    output=report["summary"],
                    metadata=report
                )
        except Exception as e:
            print(f"è®°å½•æ—¥æŠ¥å¤±è´¥ï¼š{str(e)}")
    
    def _log_weekly_report(self, report: Dict[str, Any], alerts: List[str]):
        """è®°å½•å‘¨æŠ¥åˆ°Langfuse"""
        try:
            with self.langfuse.start_as_current_span(name="weekly-performance-review") as span:
                span.update_trace(
                    input={"report_type": "weekly"},
                    output=report["metrics"],
                    metadata={"analysis": report, "alerts": alerts}
                )
        except Exception as e:
            print(f"è®°å½•å‘¨æŠ¥å¤±è´¥ï¼š{str(e)}")
    
    def _log_monthly_report(self, report: Dict[str, Any]):
        """è®°å½•æœˆæŠ¥åˆ°Langfuse"""
        try:
            with self.langfuse.start_as_current_span(name="monthly-comprehensive-report") as span:
                span.update_trace(
                    input={"report_type": "monthly"},
                    output=report["executive_summary"],
                    metadata=report
                )
        except Exception as e:
            print(f"è®°å½•æœˆæŠ¥å¤±è´¥ï¼š{str(e)}")
    
    def stop_scheduler(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.is_running = False
        if self.scheduler_thread:
            self.scheduler_thread.join()
        print("â¹ï¸ è¯„ä¼°è°ƒåº¦å™¨å·²åœæ­¢")

# ä½¿ç”¨ç¤ºä¾‹
def setup_automated_evaluation():
    """è®¾ç½®è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ"""
    
    scheduler = EvaluationScheduler()
    scheduler.start_scheduled_evaluation()
    
    print("ğŸ¤– è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿå·²å¯åŠ¨")
    print("ğŸ“‹ ä»»åŠ¡è®¡åˆ’ï¼š")
    print("  - æ¯æ—¥ 02:00: æ—¥åº¦è´¨é‡ä¸æ€§èƒ½åˆ†æ")
    print("  - æ¯å‘¨: æ€§èƒ½è¶‹åŠ¿å›é¡¾ä¸è­¦æŠ¥æ£€æŸ¥")
    print("  - æ¯æœˆ: ç»¼åˆæŠ¥å‘Šä¸æ”¹è¿›å»ºè®®")
    
    return scheduler
```

### 5.5 æœ¬ç« å®æˆ˜æ¼”ç»ƒ

#### å®Œæ•´çš„è¯„ä¼°æµç¨‹ç¤ºä¾‹

```python
async def run_comprehensive_evaluation():
    """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
    
    print("ğŸš€ å¯åŠ¨å®Œæ•´è¯„ä¼°æµç¨‹")
    
    # 1. åˆå§‹åŒ–ç»„ä»¶
    agent = TrackedTravelAgent()
    evaluator = TravelAgentEvaluator()
    performance_evaluator = PerformanceEvaluator()
    data_analyzer = DataAnalysisEngine()
    
    # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
    test_cases = [
        {
            "input": "æˆ‘æƒ³å»ä¸€ä¸ªæ—¢æœ‰å†å²æ–‡åŒ–åˆæœ‰ç°ä»£éƒ½å¸‚é£æƒ…çš„åœ°æ–¹ï¼Œé¢„ç®—15000å…ƒï¼Œæ—¶é—´ä¸€å‘¨",
            "category": "cultural_modern_travel",
            "expected_themes": ["å†å²æ–‡åŒ–", "ç°ä»£éƒ½å¸‚", "é¢„ç®—è§„åˆ’"]
        },
        {
            "input": "æ¨èé€‚åˆè€å¹´äººçš„æ¸©æ³‰åº¦å‡æ‘ï¼Œè¦æ±‚äº¤é€šä¾¿åˆ©ï¼ŒåŒ»ç–—è®¾æ–½å®Œå–„",
            "category": "senior_travel",
            "expected_themes": ["æ¸©æ³‰", "åŒ»ç–—è®¾æ–½", "äº¤é€šä¾¿åˆ©"]
        },
        {
            "input": "æƒ³å¸¦å­©å­å»çœ‹åŠ¨ç‰©ï¼Œæœ‰å“ªäº›å¥½çš„é‡ç”ŸåŠ¨ç‰©å›­æ¨èï¼Ÿæ—¶é—´3å¤©",
            "category": "family_travel",
            "expected_themes": ["é‡ç”ŸåŠ¨ç‰©å›­", "äº²å­", "è¡Œç¨‹å®‰æ’"]
        }
    ]
    
    print(f"ğŸ“‹ å‡†å¤‡è¯„ä¼° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # 3. ç”Ÿæˆå›ç­”å¹¶æ”¶é›†æ•°æ®
    evaluated_cases = []
    
    for i, case in enumerate(test_cases):
        print(f"\nğŸ”„ å¤„ç†æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)}")
        
        # ç”Ÿæˆå›ç­”
        result = agent.plan_trip(case["input"])
        
        # å‡†å¤‡è¯„ä¼°æ•°æ®
        eval_case = {
            "input": case["input"],
            "output": json.dumps(result, ensure_ascii=False),
            "category": case["category"]
        }
        
        evaluated_cases.append(eval_case)
    
    # 4. è´¨é‡è¯„ä¼°
    print("\nğŸ“Š å¼€å§‹è´¨é‡è¯„ä¼°...")
    quality_results = await evaluator.batch_evaluate(evaluated_cases)
    
    # 5. æ€§èƒ½è¯„ä¼°
    print("\nâš¡ å¼€å§‹æ€§èƒ½è¯„ä¼°...")
    performance_results = performance_evaluator.benchmark_agent(
        agent, 
        [case["input"] for case in test_cases],
        iterations=2
    )
    
    # 6. æ•°æ®åˆ†æ
    print("\nğŸ“ˆ å¼€å§‹æ•°æ®åˆ†æ...")
    
    # è·å–å†å²æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
    historical_data = data_analyzer.fetch_evaluation_data()
    
    if not historical_data.empty:
        quality_trends = data_analyzer.analyze_quality_trends(historical_data)
        performance_patterns = data_analyzer.analyze_performance_patterns(historical_data)
        cost_analysis = data_analyzer.generate_cost_analysis(historical_data)
    else:
        print("ğŸ“Š æš‚æ— å†å²æ•°æ®ï¼Œè·³è¿‡è¶‹åŠ¿åˆ†æ")
        quality_trends = {"message": "æ— å†å²æ•°æ®"}
        performance_patterns = {"message": "æ— å†å²æ•°æ®"}
        cost_analysis = {"message": "æ— å†å²æ•°æ®"}
    
    # 7. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    comprehensive_report = {
        "evaluation_summary": {
            "timestamp": datetime.now().isoformat(),
            "test_cases_count": len(test_cases),
            "evaluation_type": "comprehensive"
        },
        "quality_evaluation": {
            "overall_score": sum(r["overall_score"] for r in quality_results) / len(quality_results),
            "detailed_results": quality_results,
            "trends": quality_trends
        },
        "performance_evaluation": {
            "summary": performance_results["summary"],
            "detailed_metrics": performance_results,
            "patterns": performance_patterns
        },
        "cost_analysis": cost_analysis,
        "recommendations": generate_improvement_recommendations(
            quality_results, performance_results
        )
    }
    
    # 8. è¾“å‡ºæŠ¥å‘Š
    print("\nğŸ“‹ è¯„ä¼°å®Œæˆï¼ç»¼åˆæŠ¥å‘Šï¼š")
    print("=" * 60)
    
    # è´¨é‡æ¦‚è§ˆ
    print(f"ğŸ“Š è´¨é‡è¯„ä¼°ï¼š")
    print(f"  å¹³å‡ç»¼åˆå¾—åˆ†ï¼š{comprehensive_report['quality_evaluation']['overall_score']:.2f}/5.0")
    
    # æ€§èƒ½æ¦‚è§ˆ
    perf_summary = comprehensive_report['performance_evaluation']['summary']
    print(f"âš¡ æ€§èƒ½è¯„ä¼°ï¼š")
    print(f"  æˆåŠŸç‡ï¼š{perf_summary['success_rate']:.1f}%")
    print(f"  å¹³å‡å“åº”æ—¶é—´ï¼š{performance_results['response_time']['mean']:.2f}ç§’")
    
    # æ”¹è¿›å»ºè®®
    print(f"ğŸ’¡ æ”¹è¿›å»ºè®®ï¼š")
    for i, rec in enumerate(comprehensive_report['recommendations'], 1):
        print(f"  {i}. {rec}")
    
    print("=" * 60)
    
    return comprehensive_report

def generate_improvement_recommendations(quality_results: List[Dict], 
                                       performance_results: Dict) -> List[str]:
    """åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆæ”¹è¿›å»ºè®®"""
    
    recommendations = []
    
    # åŸºäºè´¨é‡ç»“æœçš„å»ºè®®
    avg_quality = sum(r["overall_score"] for r in quality_results) / len(quality_results)
    
    if avg_quality < 3.5:
        recommendations.append("æ•´ä½“è´¨é‡åä½ï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯å’Œç³»ç»Ÿè®¾è®¡")
    
    # åˆ†æå„ç»´åº¦å¾—åˆ†
    criteria_scores = {}
    for result in quality_results:
        for criteria, details in result["detailed_scores"].items():
            if not details.get("error", False):
                criteria_scores.setdefault(criteria, []).append(details["score"])
    
    for criteria, scores in criteria_scores.items():
        avg_score = sum(scores) / len(scores)
        if avg_score < 3.0:
            recommendations.append(f"{criteria}ç»´åº¦å¾—åˆ†è¾ƒä½({avg_score:.1f})ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")
    
    # åŸºäºæ€§èƒ½ç»“æœçš„å»ºè®®
    avg_response_time = performance_results["response_time"]["mean"]
    
    if avg_response_time > 10:
        recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œè€ƒè™‘ä¼˜åŒ–æ¨¡å‹é€‰æ‹©æˆ–å¹¶è¡Œå¤„ç†")
    elif avg_response_time > 5:
        recommendations.append("å“åº”æ—¶é—´åé«˜ï¼Œå»ºè®®å®æ–½ç¼“å­˜ç­–ç•¥")
    
    success_rate = performance_results["summary"]["success_rate"]
    if success_rate < 100:
        recommendations.append(f"æˆåŠŸç‡({success_rate:.1f}%)æœ‰å¾…æå‡ï¼Œéœ€è¦å¢å¼ºé”™è¯¯å¤„ç†")
    
    # é€šç”¨å»ºè®®
    recommendations.append("å»ºç«‹æŒç»­ç›‘æ§ä½“ç³»ï¼Œå®šæœŸè¯„ä¼°å’Œä¼˜åŒ–")
    recommendations.append("æ”¶é›†æ›´å¤šçœŸå®ç”¨æˆ·åé¦ˆï¼Œæ”¹è¿›è¯„ä¼°æ ‡å‡†")
    
    return recommendations

# ä¸»æ‰§è¡Œå‡½æ•°
if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´è¯„ä¼°
    import asyncio
    
    print("ğŸ¯ å¼€å§‹å¤§æ¨¡å‹è¯„ä¼°å®æˆ˜æ¼”ç»ƒ")
    
    # å¼‚æ­¥è¿è¡Œè¯„ä¼°
    report = asyncio.run(run_comprehensive_evaluation())
    
    print("\nğŸ‰ è¯„ä¼°å®æˆ˜æ¼”ç»ƒå®Œæˆï¼")
    print("ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆå¹¶è®°å½•åˆ°Langfuse")
    print("ğŸ”— è¯·è®¿é—® https://cloud.langfuse.com æŸ¥çœ‹å®Œæ•´è¿½è¸ªæ•°æ®")
```

### 5.6 æœ¬ç« å°ç»“

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æˆåŠŸå®ç°äº†ï¼š

#### ğŸ¯ æ ¸å¿ƒæˆæœ

1. **ä¸“ä¸šè¯„ä¼°å™¨**
   - âœ… å¤šç»´åº¦è´¨é‡è¯„ä¼°ï¼ˆå‡†ç¡®æ€§ã€ç›¸å…³æ€§ã€å®Œæ•´æ€§ç­‰ï¼‰
   - âœ… LLM-as-a-Judgeè‡ªåŠ¨åŒ–è¯„åˆ†
   - âœ… æ‰¹é‡è¯„ä¼°å¤„ç†èƒ½åŠ›

2. **æ€§èƒ½ç›‘æ§ä½“ç³»**
   - âœ… å“åº”æ—¶é—´ç²¾ç¡®æµ‹é‡
   - âœ… èµ„æºä½¿ç”¨è¿½è¸ª
   - âœ… åŸºå‡†æµ‹è¯•æ¡†æ¶

3. **æ•°æ®åˆ†æå¼•æ“**
   - âœ… è´¨é‡è¶‹åŠ¿åˆ†æ
   - âœ… æ€§èƒ½æ¨¡å¼è¯†åˆ«
   - âœ… æˆæœ¬ä¼˜åŒ–å»ºè®®

4. **è‡ªåŠ¨åŒ–è°ƒåº¦ç³»ç»Ÿ**
   - âœ… æ—¥/å‘¨/æœˆå®šæœŸæŠ¥å‘Š
   - âœ… æ™ºèƒ½é¢„è­¦æœºåˆ¶
   - âœ… KPIç›‘æ§ä½“ç³»

#### ğŸš€ å…³é”®æŠ€èƒ½æ”¶è·

- **è¯„ä¼°è®¾è®¡æ€ç»´**ï¼šå­¦ä¼šè®¾è®¡å…¨é¢çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»
- **è‡ªåŠ¨åŒ–å®æ–½**ï¼šæŒæ¡å¤§è§„æ¨¡è‡ªåŠ¨åŒ–è¯„ä¼°çš„æŠ€æœ¯æ–¹æ¡ˆ
- **æ•°æ®åˆ†æèƒ½åŠ›**ï¼šèƒ½å¤Ÿä»è¯„ä¼°æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿ
- **ç³»ç»Ÿæ€§æ€è€ƒ**ï¼šå»ºç«‹å®Œæ•´çš„è¯„ä¼°ç›‘æ§ä½“ç³»

#### ğŸ’¡ æœ€ä½³å®è·µæ€»ç»“

1. **è¯„ä¼°ç»´åº¦è¦å…¨é¢**ï¼šä¸ä»…çœ‹è´¨é‡ï¼Œè¿˜è¦å…³æ³¨æ€§èƒ½ã€æˆæœ¬ã€ç”¨æˆ·ä½“éªŒ
2. **è‡ªåŠ¨åŒ–æ˜¯å…³é”®**ï¼šå¤§è§„æ¨¡åº”ç”¨å¿…é¡»ä¾é è‡ªåŠ¨åŒ–è¯„ä¼°
3. **æŒç»­ç›‘æ§é‡è¦**ï¼šå»ºç«‹å®šæœŸè¯„ä¼°æœºåˆ¶ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
4. **æ•°æ®é©±åŠ¨å†³ç­–**ï¼šåŸºäºå®¢è§‚æ•°æ®è¿›è¡Œä¼˜åŒ–ï¼Œè€Œéä¸»è§‚åˆ¤æ–­

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åŸºäºè¯„ä¼°ç»“æœç¼–å†™ä¸“ä¸šçš„è¯„ä¼°æŠ¥å‘Šï¼Œå¹¶åˆ¶å®šå…·ä½“çš„ä¼˜åŒ–æ”¹è¿›æ–¹æ¡ˆã€‚

---

## ç¬¬å…­ç« ï¼šé¡¹ç›®å®æˆ˜ï¼ˆä¸‹ï¼‰- ç¼–å†™è¯„ä¼°æŠ¥å‘Šä¸ä¼˜åŒ–å»ºè®®

### 6.1 ä¸“ä¸šè¯„ä¼°æŠ¥å‘Šçš„ç»“æ„è®¾è®¡

åœ¨å‰ä¸¤ç« ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸæ­å»ºäº†è¯„ä¼°åŸºç¡€è®¾æ–½å¹¶å®æ–½äº†è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚ç°åœ¨éœ€è¦å°†è¿™äº›æ•°æ®å’Œåˆ†æç»“æœè½¬åŒ–ä¸ºå¯ä¾›å†³ç­–çš„ä¸“ä¸šæŠ¥å‘Šã€‚

#### è¯„ä¼°æŠ¥å‘Šçš„æ ¸å¿ƒè¦ç´ 

```python
class EvaluationReportGenerator:
    """è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.langfuse = Langfuse()
        self.report_template = {
            "executive_summary": "æ‰§è¡Œæ‘˜è¦",
            "methodology": "è¯„ä¼°æ–¹æ³•è®º",
            "key_findings": "å…³é”®å‘ç°",
            "detailed_analysis": "è¯¦ç»†åˆ†æ",
            "performance_metrics": "æ€§èƒ½æŒ‡æ ‡",
            "quality_assessment": "è´¨é‡è¯„ä¼°",
            "cost_analysis": "æˆæœ¬åˆ†æ",
            "recommendations": "æ”¹è¿›å»ºè®®",
            "action_plan": "è¡ŒåŠ¨è®¡åˆ’",
            "appendix": "é™„å½•"
        }
    
    def generate_comprehensive_report(self, evaluation_data: Dict[str, Any], 
                                    report_type: str = "monthly") -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            evaluation_data: è¯„ä¼°æ•°æ®
            report_type: æŠ¥å‘Šç±»å‹ï¼ˆdaily/weekly/monthlyï¼‰
            
        Returns:
            å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š
        """
        
        print(f"ğŸ“ ç”Ÿæˆ{report_type}è¯„ä¼°æŠ¥å‘Š...")
        
        report = {
            "report_metadata": self._generate_metadata(report_type),
            "executive_summary": self._generate_executive_summary(evaluation_data),
            "methodology": self._generate_methodology_section(),
            "key_findings": self._extract_key_findings(evaluation_data),
            "detailed_analysis": self._generate_detailed_analysis(evaluation_data),
            "recommendations": self._generate_recommendations(evaluation_data),
            "action_plan": self._create_action_plan(evaluation_data),
            "appendix": self._generate_appendix(evaluation_data)
        }
        
        # ç”ŸæˆæŠ¥å‘Šæ‘˜è¦
        report["report_summary"] = self._generate_report_summary(report)
        
        return report
    
    def _generate_metadata(self, report_type: str) -> Dict[str, Any]:
        """ç”ŸæˆæŠ¥å‘Šå…ƒæ•°æ®"""
        return {
            "report_type": report_type,
            "generation_date": datetime.now().isoformat(),
            "report_version": "1.0",
            "system": "AIæ—…è¡Œæ™ºèƒ½ä½“è¯„ä¼°ç³»ç»Ÿ",
            "evaluator": "TravelAgentEvaluator v1.0",
            "methodology": "å¤šç»´åº¦ç»¼åˆè¯„ä¼°",
            "confidence_level": "95%"
        }
    
    def _generate_executive_summary(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        
        quality_data = data.get("quality_evaluation", {})
        performance_data = data.get("performance_evaluation", {})
        cost_data = data.get("cost_analysis", {})
        
        overall_score = quality_data.get("overall_score", 0)
        
        # ç¡®å®šæ•´ä½“è¯„ä¼°ç­‰çº§
        if overall_score >= 4.5:
            grade = "ä¼˜ç§€"
            status = "è¶…å‡ºé¢„æœŸ"
        elif overall_score >= 4.0:
            grade = "è‰¯å¥½"
            status = "ç¬¦åˆé¢„æœŸ"
        elif overall_score >= 3.5:
            grade = "ä¸€èˆ¬"
            status = "åŸºæœ¬ç¬¦åˆ"
        elif overall_score >= 3.0:
            grade = "åŠæ ¼"
            status = "éœ€è¦æ”¹è¿›"
        else:
            grade = "ä¸åŠæ ¼"
            status = "æ€¥éœ€ä¼˜åŒ–"
        
        return {
            "overall_assessment": {
                "grade": grade,
                "score": round(overall_score, 2),
                "status": status
            },
            "key_metrics": {
                "quality_score": round(overall_score, 2),
                "performance_rating": self._classify_performance_rating(performance_data),
                "cost_efficiency": self._classify_cost_efficiency(cost_data)
            },
            "summary_statement": f"AIæ—…è¡Œæ™ºèƒ½ä½“åœ¨æœ¬æ¬¡è¯„ä¼°ä¸­è·å¾—{grade}è¯„çº§ï¼ˆ{overall_score:.1f}/5.0ï¼‰ï¼Œ{status}ã€‚",
            "priority_areas": self._identify_priority_areas(data),
            "next_steps": self._suggest_immediate_actions(data)
        }
    
    def _generate_methodology_section(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ–¹æ³•è®ºè¯´æ˜"""
        return {
            "evaluation_framework": {
                "name": "å¤šç»´åº¦ç»¼åˆè¯„ä¼°æ¡†æ¶",
                "description": "åŸºäºè´¨é‡ã€æ€§èƒ½ã€æˆæœ¬ä¸‰å¤§ç»´åº¦çš„å…¨é¢è¯„ä¼°ä½“ç³»",
                "dimensions": [
                    "å‡†ç¡®æ€§(Accuracy) - 25%æƒé‡",
                    "ç›¸å…³æ€§(Relevance) - 25%æƒé‡", 
                    "å®Œæ•´æ€§(Completeness) - 20%æƒé‡",
                    "å®ç”¨æ€§(Practicality) - 20%æƒé‡",
                    "ä¸ªæ€§åŒ–(Personalization) - 10%æƒé‡"
                ]
            },
            "evaluation_methods": {
                "llm_as_judge": "ä½¿ç”¨GPT-4oä½œä¸ºè¯„åˆ¤æ¨¡å‹è¿›è¡Œè´¨é‡è¯„ä¼°",
                "performance_testing": "è‡ªåŠ¨åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒåŒ…å«å“åº”æ—¶é—´ã€æˆåŠŸç‡ç­‰æŒ‡æ ‡",
                "cost_analysis": "åŸºäºTokenä½¿ç”¨é‡çš„æˆæœ¬æ•ˆç›Šåˆ†æ"
            },
            "data_sources": [
                "Langfuseè¿½è¸ªæ•°æ®",
                "è‡ªåŠ¨åŒ–è¯„ä¼°ç»“æœ",
                "æ€§èƒ½ç›‘æ§æŒ‡æ ‡",
                "æˆæœ¬ä½¿ç”¨è®°å½•"
            ],
            "limitations": [
                "è¯„ä¼°åŸºäºæœ‰é™çš„æµ‹è¯•ç”¨ä¾‹é›†",
                "LLMè¯„åˆ¤å¯èƒ½å­˜åœ¨ä¸€å®šä¸»è§‚æ€§",
                "æˆæœ¬åˆ†æåŸºäºå½“å‰APIå®šä»·"
            ]
        }
    
    def _extract_key_findings(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–å…³é”®å‘ç°"""
        findings = []
        
        quality_data = data.get("quality_evaluation", {})
        performance_data = data.get("performance_evaluation", {})
        cost_data = data.get("cost_analysis", {})
        
        # è´¨é‡ç›¸å…³å‘ç°
        if quality_data:
            overall_score = quality_data.get("overall_score", 0)
            if overall_score >= 4.0:
                findings.append({
                    "category": "è´¨é‡",
                    "type": "positive",
                    "finding": f"æ•´ä½“è´¨é‡è¡¨ç°ä¼˜ç§€ï¼Œç»¼åˆå¾—åˆ†{overall_score:.1f}/5.0",
                    "impact": "high",
                    "evidence": "å¤šç»´åº¦è¯„ä¼°ç»“æœä¸€è‡´æ˜¾ç¤ºé«˜è´¨é‡è¾“å‡º"
                })
            elif overall_score < 3.0:
                findings.append({
                    "category": "è´¨é‡",
                    "type": "concern",
                    "finding": f"è´¨é‡å¾—åˆ†åä½({overall_score:.1f}/5.0)ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨",
                    "impact": "high",
                    "evidence": "å¤šä¸ªè¯„ä¼°ç»´åº¦å¾—åˆ†ä½äºé¢„æœŸé˜ˆå€¼"
                })
        
        # æ€§èƒ½ç›¸å…³å‘ç°
        if performance_data:
            perf_summary = performance_data.get("summary", {})
            avg_response_time = performance_data.get("detailed_metrics", {}).get("response_time", {}).get("mean", 0)
            
            if avg_response_time > 10:
                findings.append({
                    "category": "æ€§èƒ½",
                    "type": "concern", 
                    "finding": f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿({avg_response_time:.1f}ç§’)",
                    "impact": "medium",
                    "evidence": f"åŸºäº{perf_summary.get('total_tests', 0)}æ¬¡æµ‹è¯•çš„ç»Ÿè®¡ç»“æœ"
                })
            elif avg_response_time < 3:
                findings.append({
                    "category": "æ€§èƒ½",
                    "type": "positive",
                    "finding": f"å“åº”é€Ÿåº¦ä¼˜ç§€ï¼Œå¹³å‡{avg_response_time:.1f}ç§’",
                    "impact": "medium",
                    "evidence": "æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºå‡ºè‰²çš„å“åº”é€Ÿåº¦"
                })
        
        # æˆæœ¬ç›¸å…³å‘ç°  
        if cost_data and "cost_statistics" in cost_data:
            avg_cost = cost_data["cost_statistics"].get("average_cost_per_request", 0)
            if avg_cost > 0.1:
                findings.append({
                    "category": "æˆæœ¬",
                    "type": "concern",
                    "finding": f"å•æ¬¡è¯·æ±‚æˆæœ¬è¾ƒé«˜(${avg_cost:.4f})",
                    "impact": "medium", 
                    "evidence": "æˆæœ¬åˆ†ææ˜¾ç¤ºè¶…å‡ºé¢„æœŸçš„APIä½¿ç”¨è´¹ç”¨"
                })
        
        return findings
    
    def _generate_detailed_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        
        analysis = {}
        
        # è´¨é‡è¯¦ç»†åˆ†æ
        if "quality_evaluation" in data:
            analysis["quality_analysis"] = self._analyze_quality_details(
                data["quality_evaluation"]
            )
        
        # æ€§èƒ½è¯¦ç»†åˆ†æ
        if "performance_evaluation" in data:
            analysis["performance_analysis"] = self._analyze_performance_details(
                data["performance_evaluation"]
            )
        
        # æˆæœ¬è¯¦ç»†åˆ†æ
        if "cost_analysis" in data:
            analysis["cost_analysis"] = self._analyze_cost_details(
                data["cost_analysis"]
            )
        
        # è¶‹åŠ¿åˆ†æ
        analysis["trend_analysis"] = self._analyze_trends(data)
        
        return analysis
    
    def _analyze_quality_details(self, quality_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè´¨é‡ç»†èŠ‚"""
        
        detailed_results = quality_data.get("detailed_results", [])
        
        if not detailed_results:
            return {"error": "æ— è´¨é‡è¯„ä¼°æ•°æ®"}
        
        # æŒ‰ç»´åº¦ç»Ÿè®¡å¾—åˆ†
        dimension_scores = {}
        for result in detailed_results:
            for dimension, details in result.get("detailed_scores", {}).items():
                if not details.get("error", False):
                    dimension_scores.setdefault(dimension, []).append(details["score"])
        
        # è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
        dimension_averages = {
            dim: sum(scores) / len(scores) 
            for dim, scores in dimension_scores.items()
        }
        
        # è¯†åˆ«å¼ºé¡¹å’Œå¼±é¡¹
        strengths = [dim for dim, score in dimension_averages.items() if score >= 4.0]
        weaknesses = [dim for dim, score in dimension_averages.items() if score < 3.0]
        
        return {
            "dimension_scores": dimension_averages,
            "strengths": strengths,
            "weaknesses": weaknesses,
            "score_distribution": self._calculate_score_distribution(detailed_results),
            "consistency_analysis": self._analyze_score_consistency(detailed_results)
        }
    
    def _analyze_performance_details(self, performance_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ç»†èŠ‚"""
        
        detailed_metrics = performance_data.get("detailed_metrics", {})
        
        if not detailed_metrics:
            return {"error": "æ— æ€§èƒ½æ•°æ®"}
        
        response_time_stats = detailed_metrics.get("response_time", {})
        summary = detailed_metrics.get("summary", {})
        
        # æ€§èƒ½åˆ†çº§
        mean_time = response_time_stats.get("mean", 0)
        p95_time = response_time_stats.get("percentile_95", 0)
        
        performance_grade = self._grade_performance(mean_time, p95_time)
        
        return {
            "performance_grade": performance_grade,
            "response_time_analysis": {
                "mean": mean_time,
                "median": response_time_stats.get("median", 0),
                "p95": p95_time,
                "p99": response_time_stats.get("percentile_99", 0),
                "variability": response_time_stats.get("std_dev", 0)
            },
            "reliability_metrics": {
                "success_rate": summary.get("success_rate", 0),
                "error_rate": 100 - summary.get("success_rate", 0),
                "total_tests": summary.get("total_tests", 0)
            },
            "performance_recommendations": self._generate_performance_recommendations(response_time_stats)
        }
    
    def _generate_recommendations(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = {
            "immediate_actions": [],    # ç«‹å³è¡ŒåŠ¨
            "short_term_goals": [],     # çŸ­æœŸç›®æ ‡ï¼ˆ1-3ä¸ªæœˆï¼‰
            "long_term_strategy": [],   # é•¿æœŸç­–ç•¥ï¼ˆ3-12ä¸ªæœˆï¼‰
            "resource_requirements": [] # èµ„æºéœ€æ±‚
        }
        
        # åŸºäºè´¨é‡è¯„ä¼°çš„å»ºè®®
        quality_data = data.get("quality_evaluation", {})
        if quality_data:
            quality_recommendations = self._generate_quality_recommendations(quality_data)
            recommendations["immediate_actions"].extend(quality_recommendations["immediate"])
            recommendations["short_term_goals"].extend(quality_recommendations["short_term"])
        
        # åŸºäºæ€§èƒ½è¯„ä¼°çš„å»ºè®®  
        performance_data = data.get("performance_evaluation", {})
        if performance_data:
            perf_recommendations = self._generate_performance_recommendations_detailed(performance_data)
            recommendations["immediate_actions"].extend(perf_recommendations["immediate"])
            recommendations["short_term_goals"].extend(perf_recommendations["short_term"])
        
        # åŸºäºæˆæœ¬åˆ†æçš„å»ºè®®
        cost_data = data.get("cost_analysis", {})
        if cost_data:
            cost_recommendations = self._generate_cost_recommendations(cost_data)
            recommendations["short_term_goals"].extend(cost_recommendations)
        
        # é•¿æœŸç­–ç•¥å»ºè®®
        recommendations["long_term_strategy"] = [
            "å»ºç«‹æ›´å®Œå–„çš„è¯„ä¼°æŒ‡æ ‡ä½“ç³»",
            "å®æ–½ç”¨æˆ·åé¦ˆæ”¶é›†æœºåˆ¶",
            "å¼€å‘A/Bæµ‹è¯•æ¡†æ¶",
            "å»ºç«‹æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä½“ç³»",
            "å®æ–½æŒç»­é›†æˆ/æŒç»­éƒ¨ç½²(CI/CD)"
        ]
        
        # èµ„æºéœ€æ±‚è¯„ä¼°
        recommendations["resource_requirements"] = [
            "æŠ€æœ¯äººå‘˜ï¼š1-2åå·¥ç¨‹å¸ˆè´Ÿè´£ä¼˜åŒ–å®æ–½",
            "æ—¶é—´æŠ•å…¥ï¼šé¢„è®¡2-4å‘¨å®Œæˆä¸»è¦æ”¹è¿›",
            "é¢„ç®—è€ƒè™‘ï¼šå¯èƒ½éœ€è¦å‡çº§åŸºç¡€è®¾æ–½æˆ–APIå¥—é¤",
            "å·¥å…·é‡‡è´­ï¼šè€ƒè™‘è´­ä¹°ä¸“ä¸šç›‘æ§å·¥å…·è®¸å¯"
        ]
        
        return recommendations
    
    def _create_action_plan(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºè¡ŒåŠ¨è®¡åˆ’"""
        
        # ä¼˜å…ˆçº§çŸ©é˜µï¼šå½±å“åŠ› vs å®æ–½éš¾åº¦
        action_items = []
        
        # åŸºäºè¯„ä¼°ç»“æœè¯†åˆ«å…³é”®é—®é¢˜
        quality_score = data.get("quality_evaluation", {}).get("overall_score", 0)
        
        if quality_score < 3.5:
            action_items.append({
                "priority": "é«˜",
                "action": "ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹",
                "timeline": "1-2å‘¨",
                "owner": "AIå·¥ç¨‹å¸ˆ",
                "success_criteria": "è´¨é‡å¾—åˆ†æå‡è‡³4.0ä»¥ä¸Š",
                "dependencies": "æ— "
            })
        
        performance_data = data.get("performance_evaluation", {})
        avg_response_time = performance_data.get("detailed_metrics", {}).get("response_time", {}).get("mean", 0)
        
        if avg_response_time > 5:
            action_items.append({
                "priority": "ä¸­",
                "action": "å®æ–½å“åº”ç¼“å­˜æœºåˆ¶",
                "timeline": "2-3å‘¨", 
                "owner": "åç«¯å·¥ç¨‹å¸ˆ",
                "success_criteria": "å¹³å‡å“åº”æ—¶é—´é™è‡³3ç§’ä»¥å†…",
                "dependencies": "éœ€è¦Redisæˆ–ç±»ä¼¼ç¼“å­˜æœåŠ¡"
            })
        
        # é€šç”¨æ”¹è¿›é¡¹
        action_items.extend([
            {
                "priority": "ä¸­",
                "action": "å»ºç«‹ç›‘æ§å‘Šè­¦ç³»ç»Ÿ",
                "timeline": "3-4å‘¨",
                "owner": "DevOpså·¥ç¨‹å¸ˆ", 
                "success_criteria": "å®ç°7x24å°æ—¶è‡ªåŠ¨ç›‘æ§",
                "dependencies": "ç›‘æ§å·¥å…·é€‰å‹å’Œéƒ¨ç½²"
            },
            {
                "priority": "ä½",
                "action": "æ”¶é›†çœŸå®ç”¨æˆ·åé¦ˆ",
                "timeline": "4-6å‘¨",
                "owner": "äº§å“ç»ç†",
                "success_criteria": "å»ºç«‹ç”¨æˆ·åé¦ˆæ”¶é›†æ¸ é“",
                "dependencies": "UI/UXè®¾è®¡å’Œå¼€å‘"
            }
        ])
        
        return {
            "action_items": action_items,
            "timeline_overview": self._create_timeline_overview(action_items),
            "success_metrics": self._define_success_metrics(),
            "risk_assessment": self._assess_implementation_risks(action_items)
        }
    
    def _create_timeline_overview(self, action_items: List[Dict]) -> Dict[str, List[str]]:
        """åˆ›å»ºæ—¶é—´çº¿æ¦‚è§ˆ"""
        timeline = {
            "ç¬¬1-2å‘¨": [],
            "ç¬¬3-4å‘¨": [],
            "ç¬¬5-8å‘¨": [],
            "ç¬¬9-12å‘¨": []
        }
        
        for item in action_items:
            timeline_key = self._map_timeline_to_period(item["timeline"])
            timeline[timeline_key].append(item["action"])
        
        return timeline
    
    def _map_timeline_to_period(self, timeline: str) -> str:
        """å°†æ—¶é—´çº¿æ˜ å°„åˆ°å‘¨æœŸ"""
        if "1-2å‘¨" in timeline:
            return "ç¬¬1-2å‘¨"
        elif "2-3å‘¨" in timeline or "3-4å‘¨" in timeline:
            return "ç¬¬3-4å‘¨"
        elif "4-6å‘¨" in timeline:
            return "ç¬¬5-8å‘¨"
        else:
            return "ç¬¬9-12å‘¨"
    
    def export_to_markdown(self, report: Dict[str, Any]) -> str:
        """å¯¼å‡ºä¸ºMarkdownæ ¼å¼æŠ¥å‘Š"""
        
        markdown_content = f"""# AIæ—…è¡Œæ™ºèƒ½ä½“è¯„ä¼°æŠ¥å‘Š

## ğŸ“‹ æŠ¥å‘ŠåŸºæœ¬ä¿¡æ¯

- **æŠ¥å‘Šç±»å‹**: {report['report_metadata']['report_type']}
- **ç”Ÿæˆæ—¥æœŸ**: {report['report_metadata']['generation_date']}
- **è¯„ä¼°ç³»ç»Ÿ**: {report['report_metadata']['system']}
- **æŠ¥å‘Šç‰ˆæœ¬**: {report['report_metadata']['report_version']}

---

## ğŸ¯ æ‰§è¡Œæ‘˜è¦

### æ•´ä½“è¯„ä¼°ç»“æœ
- **è¯„çº§**: {report['executive_summary']['overall_assessment']['grade']}
- **å¾—åˆ†**: {report['executive_summary']['overall_assessment']['score']}/5.0
- **çŠ¶æ€**: {report['executive_summary']['overall_assessment']['status']}

### å…³é”®æŒ‡æ ‡
- **è´¨é‡å¾—åˆ†**: {report['executive_summary']['key_metrics']['quality_score']}/5.0
- **æ€§èƒ½è¯„çº§**: {report['executive_summary']['key_metrics']['performance_rating']}
- **æˆæœ¬æ•ˆç‡**: {report['executive_summary']['key_metrics']['cost_efficiency']}

### æ€»ç»“
{report['executive_summary']['summary_statement']}

---

## ğŸ” å…³é”®å‘ç°

"""
        
        # æ·»åŠ å…³é”®å‘ç°
        for i, finding in enumerate(report['key_findings'], 1):
            finding_emoji = "âœ…" if finding['type'] == 'positive' else "âš ï¸"
            markdown_content += f"""
### {finding_emoji} å‘ç° {i}: {finding['category']}
- **ç±»å‹**: {finding['type']}
- **å½±å“ç¨‹åº¦**: {finding['impact']}
- **å‘ç°å†…å®¹**: {finding['finding']}
- **æ”¯æŒè¯æ®**: {finding['evidence']}
"""
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        markdown_content += f"""

---

## ğŸ’¡ æ”¹è¿›å»ºè®®

### ğŸš€ ç«‹å³è¡ŒåŠ¨é¡¹
"""
        for action in report['recommendations']['immediate_actions']:
            markdown_content += f"- {action}\n"
        
        markdown_content += """
### ğŸ“… çŸ­æœŸç›®æ ‡ï¼ˆ1-3ä¸ªæœˆï¼‰
"""
        for goal in report['recommendations']['short_term_goals']:
            markdown_content += f"- {goal}\n"
        
        # æ·»åŠ è¡ŒåŠ¨è®¡åˆ’
        markdown_content += """

---

## ğŸ“Š è¡ŒåŠ¨è®¡åˆ’

| ä¼˜å…ˆçº§ | è¡ŒåŠ¨é¡¹ | æ—¶é—´çº¿ | è´Ÿè´£äºº | æˆåŠŸæ ‡å‡† |
|--------|--------|--------|--------|----------|
"""
        
        for item in report['action_plan']['action_items']:
            markdown_content += f"| {item['priority']} | {item['action']} | {item['timeline']} | {item['owner']} | {item['success_criteria']} |\n"
        
        markdown_content += """

---

## ğŸ“ˆ è¯¦ç»†åˆ†æ

è¯¦ç»†çš„æŠ€æœ¯åˆ†ææ•°æ®è¯·å‚è€ƒé™„å½•æˆ–è”ç³»æŠ€æœ¯å›¢é˜Ÿè·å–å®Œæ•´æ•°æ®ã€‚

---

*æœ¬æŠ¥å‘Šç”±AIæ—…è¡Œæ™ºèƒ½ä½“è¯„ä¼°ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        return markdown_content

```

### 6.2 ä¼˜åŒ–ç­–ç•¥åˆ¶å®š

#### æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–å†³ç­–

```python
class OptimizationEngine:
    """ä¼˜åŒ–å¼•æ“ - åŸºäºè¯„ä¼°ç»“æœåˆ¶å®šä¼˜åŒ–ç­–ç•¥"""
    
    def __init__(self):
        self.langfuse = Langfuse()
        self.optimization_strategies = {
            "prompt_optimization": PromptOptimizer(),
            "model_optimization": ModelOptimizer(),
            "system_optimization": SystemOptimizer(),
            "cost_optimization": CostOptimizer()
        }
    
    def create_optimization_plan(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ›å»ºç»¼åˆä¼˜åŒ–è®¡åˆ’
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœæ•°æ®
            
        Returns:
            è¯¦ç»†çš„ä¼˜åŒ–è®¡åˆ’
        """
        
        print("ğŸ”§ åˆ›å»ºç»¼åˆä¼˜åŒ–è®¡åˆ’...")
        
        # 1. é—®é¢˜è¯Šæ–­
        issues = self._diagnose_issues(evaluation_results)
        
        # 2. ç­–ç•¥é€‰æ‹©
        strategies = self._select_strategies(issues)
        
        # 3. ä¼˜åŒ–æ–¹æ¡ˆè®¾è®¡
        optimization_plan = {
            "diagnosis": issues,
            "selected_strategies": strategies,
            "implementation_plan": self._create_implementation_plan(strategies),
            "expected_outcomes": self._predict_outcomes(strategies),
            "monitoring_plan": self._create_monitoring_plan()
        }
        
        return optimization_plan
    
    def _diagnose_issues(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """è¯Šæ–­é—®é¢˜"""
        
        issues = {
            "quality_issues": [],
            "performance_issues": [],
            "cost_issues": [],
            "system_issues": []
        }
        
        # è´¨é‡é—®é¢˜è¯Šæ–­
        quality_data = results.get("quality_evaluation", {})
        if quality_data:
            overall_score = quality_data.get("overall_score", 0)
            
            if overall_score < 3.5:
                issues["quality_issues"].append({
                    "issue": "æ•´ä½“è´¨é‡åä½",
                    "severity": "high",
                    "details": f"ç»¼åˆå¾—åˆ†{overall_score:.1f}/5.0ï¼Œä½äºé¢„æœŸ3.5åˆ†é˜ˆå€¼",
                    "root_causes": ["æç¤ºè¯è®¾è®¡ä¸å½“", "æ¨¡å‹é€‰æ‹©ä¸åˆé€‚", "è®­ç»ƒæ•°æ®ä¸è¶³"]
                })
            
            # åˆ†æå„ç»´åº¦å¾—åˆ†
            detailed_results = quality_data.get("detailed_results", [])
            if detailed_results:
                dimension_scores = self._calculate_dimension_averages(detailed_results)
                
                for dimension, score in dimension_scores.items():
                    if score < 3.0:
                        issues["quality_issues"].append({
                            "issue": f"{dimension}ç»´åº¦å¾—åˆ†åä½",
                            "severity": "medium",
                            "details": f"{dimension}å¹³å‡å¾—åˆ†{score:.1f}/5.0",
                            "root_causes": self._identify_dimension_causes(dimension)
                        })
        
        # æ€§èƒ½é—®é¢˜è¯Šæ–­
        performance_data = results.get("performance_evaluation", {})
        if performance_data:
            metrics = performance_data.get("detailed_metrics", {})
            response_time = metrics.get("response_time", {})
            
            mean_time = response_time.get("mean", 0)
            if mean_time > 5:
                issues["performance_issues"].append({
                    "issue": "å“åº”æ—¶é—´è¿‡é•¿",
                    "severity": "high" if mean_time > 10 else "medium",
                    "details": f"å¹³å‡å“åº”æ—¶é—´{mean_time:.1f}ç§’ï¼Œè¶…å‡º5ç§’é¢„æœŸ",
                    "root_causes": ["æ¨¡å‹è®¡ç®—å¤æ‚åº¦é«˜", "ç½‘ç»œå»¶è¿Ÿ", "ç¼ºä¹ç¼“å­˜æœºåˆ¶"]
                })
            
            success_rate = metrics.get("summary", {}).get("success_rate", 100)
            if success_rate < 98:
                issues["performance_issues"].append({
                    "issue": "æˆåŠŸç‡åä½",
                    "severity": "high",
                    "details": f"æˆåŠŸç‡{success_rate:.1f}%ï¼Œä½äº98%ç›®æ ‡",
                    "root_causes": ["é”™è¯¯å¤„ç†ä¸å®Œå–„", "APIè°ƒç”¨ä¸ç¨³å®š", "è¾“å…¥éªŒè¯ä¸è¶³"]
                })
        
        # æˆæœ¬é—®é¢˜è¯Šæ–­
        cost_data = results.get("cost_analysis", {})
        if cost_data and "cost_statistics" in cost_data:
            avg_cost = cost_data["cost_statistics"].get("average_cost_per_request", 0)
            
            if avg_cost > 0.05:  # å‡è®¾æˆæœ¬é˜ˆå€¼
                issues["cost_issues"].append({
                    "issue": "å•æ¬¡è¯·æ±‚æˆæœ¬è¿‡é«˜",
                    "severity": "medium",
                    "details": f"å¹³å‡è¯·æ±‚æˆæœ¬${avg_cost:.4f}ï¼Œè¶…å‡ºé¢„æœŸ",
                    "root_causes": ["æ¨¡å‹é€‰æ‹©æˆæœ¬è¾ƒé«˜", "Tokenä½¿ç”¨æ•ˆç‡ä½", "ç¼ºä¹ç¼“å­˜å‡å°‘é‡å¤è°ƒç”¨"]
                })
        
        return issues
    
    def _select_strategies(self, issues: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é€‰æ‹©ä¼˜åŒ–ç­–ç•¥"""
        
        strategies = []
        
        # åŸºäºè´¨é‡é—®é¢˜é€‰æ‹©ç­–ç•¥
        for issue in issues.get("quality_issues", []):
            if "æ•´ä½“è´¨é‡åä½" in issue["issue"]:
                strategies.append({
                    "type": "prompt_optimization",
                    "priority": "high",
                    "strategy": "comprehensive_prompt_engineering",
                    "description": "å…¨é¢ä¼˜åŒ–æç¤ºè¯å·¥ç¨‹",
                    "target_issue": issue["issue"]
                })
            
            if "accuracy" in issue["issue"].lower():
                strategies.append({
                    "type": "model_optimization", 
                    "priority": "medium",
                    "strategy": "model_upgrade",
                    "description": "å‡çº§åˆ°æ›´å‡†ç¡®çš„æ¨¡å‹",
                    "target_issue": issue["issue"]
                })
        
        # åŸºäºæ€§èƒ½é—®é¢˜é€‰æ‹©ç­–ç•¥
        for issue in issues.get("performance_issues", []):
            if "å“åº”æ—¶é—´" in issue["issue"]:
                strategies.append({
                    "type": "system_optimization",
                    "priority": "high",
                    "strategy": "caching_implementation",
                    "description": "å®æ–½ç¼“å­˜æœºåˆ¶",
                    "target_issue": issue["issue"]
                })
            
            if "æˆåŠŸç‡" in issue["issue"]:
                strategies.append({
                    "type": "system_optimization",
                    "priority": "high", 
                    "strategy": "error_handling_enhancement",
                    "description": "å¢å¼ºé”™è¯¯å¤„ç†æœºåˆ¶",
                    "target_issue": issue["issue"]
                })
        
        # åŸºäºæˆæœ¬é—®é¢˜é€‰æ‹©ç­–ç•¥
        for issue in issues.get("cost_issues", []):
            if "æˆæœ¬è¿‡é«˜" in issue["issue"]:
                strategies.append({
                    "type": "cost_optimization",
                    "priority": "medium",
                    "strategy": "model_cost_optimization",
                    "description": "ä¼˜åŒ–æ¨¡å‹ä½¿ç”¨æˆæœ¬",
                    "target_issue": issue["issue"]
                })
        
        return strategies
    
    def _create_implementation_plan(self, strategies: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ›å»ºå®æ–½è®¡åˆ’"""
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        high_priority = [s for s in strategies if s["priority"] == "high"]
        medium_priority = [s for s in strategies if s["priority"] == "medium"]
        low_priority = [s for s in strategies if s["priority"] == "low"]
        
        implementation_plan = {
            "phase_1": {
                "name": "ç´§æ€¥ä¼˜åŒ–é˜¶æ®µ",
                "duration": "1-2å‘¨",
                "strategies": high_priority,
                "goals": "è§£å†³é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼Œå¿«é€Ÿæå‡å…³é”®æŒ‡æ ‡"
            },
            "phase_2": {
                "name": "ç³»ç»Ÿæ€§æ”¹è¿›é˜¶æ®µ", 
                "duration": "3-6å‘¨",
                "strategies": medium_priority,
                "goals": "å…¨é¢æå‡ç³»ç»Ÿè´¨é‡å’Œæ•ˆç‡"
            },
            "phase_3": {
                "name": "æŒç»­ä¼˜åŒ–é˜¶æ®µ",
                "duration": "2-3ä¸ªæœˆ",
                "strategies": low_priority,
                "goals": "é•¿æœŸæŒç»­æ”¹è¿›å’Œåˆ›æ–°"
            }
        }
        
        # ä¸ºæ¯ä¸ªç­–ç•¥æ·»åŠ å…·ä½“å®æ–½æ­¥éª¤
        for phase in implementation_plan.values():
            for strategy in phase["strategies"]:
                strategy["implementation_steps"] = self._get_implementation_steps(strategy)
        
        return implementation_plan
    
    def _get_implementation_steps(self, strategy: Dict[str, Any]) -> List[Dict[str, str]]:
        """è·å–å…·ä½“å®æ–½æ­¥éª¤"""
        
        strategy_type = strategy["strategy"]
        
        steps_mapping = {
            "comprehensive_prompt_engineering": [
                {"step": "åˆ†æå½“å‰æç¤ºè¯æ€§èƒ½", "timeline": "1-2å¤©", "owner": "AIå·¥ç¨‹å¸ˆ"},
                {"step": "è®¾è®¡æ–°æç¤ºè¯æ¨¡æ¿", "timeline": "3-5å¤©", "owner": "AIå·¥ç¨‹å¸ˆ"},
                {"step": "A/Bæµ‹è¯•éªŒè¯æ•ˆæœ", "timeline": "5-7å¤©", "owner": "æµ‹è¯•å·¥ç¨‹å¸ˆ"},
                {"step": "éƒ¨ç½²ä¼˜åŒ–åæç¤ºè¯", "timeline": "1å¤©", "owner": "DevOps"}
            ],
            "caching_implementation": [
                {"step": "è®¾è®¡ç¼“å­˜æ¶æ„", "timeline": "2-3å¤©", "owner": "åç«¯å·¥ç¨‹å¸ˆ"},
                {"step": "é€‰æ‹©ç¼“å­˜æŠ€æœ¯æ ˆ", "timeline": "1å¤©", "owner": "æ¶æ„å¸ˆ"},
                {"step": "å®ç°ç¼“å­˜é€»è¾‘", "timeline": "5-7å¤©", "owner": "åç«¯å·¥ç¨‹å¸ˆ"},
                {"step": "æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜", "timeline": "3-4å¤©", "owner": "æµ‹è¯•å·¥ç¨‹å¸ˆ"}
            ],
            "error_handling_enhancement": [
                {"step": "é”™è¯¯æ¨¡å¼åˆ†æ", "timeline": "2å¤©", "owner": "åç«¯å·¥ç¨‹å¸ˆ"},
                {"step": "è®¾è®¡é”™è¯¯å¤„ç†ç­–ç•¥", "timeline": "3å¤©", "owner": "åç«¯å·¥ç¨‹å¸ˆ"},
                {"step": "å®ç°å¢å¼ºé”™è¯¯å¤„ç†", "timeline": "7-10å¤©", "owner": "åç«¯å·¥ç¨‹å¸ˆ"},
                {"step": "ç›‘æ§å’Œå‘Šè­¦é…ç½®", "timeline": "2-3å¤©", "owner": "DevOps"}
            ],
            "model_cost_optimization": [
                {"step": "åˆ†æå½“å‰æˆæœ¬ç»“æ„", "timeline": "1-2å¤©", "owner": "æ•°æ®åˆ†æå¸ˆ"},
                {"step": "è¯„ä¼°æ›¿ä»£æ¨¡å‹æ–¹æ¡ˆ", "timeline": "3-5å¤©", "owner": "AIå·¥ç¨‹å¸ˆ"},
                {"step": "å®æ–½æˆæœ¬ä¼˜åŒ–ç­–ç•¥", "timeline": "7-10å¤©", "owner": "AIå·¥ç¨‹å¸ˆ"},
                {"step": "æˆæœ¬ç›‘æ§ç³»ç»Ÿéƒ¨ç½²", "timeline": "3-5å¤©", "owner": "DevOps"}
            ]
        }
        
        return steps_mapping.get(strategy_type, [
            {"step": "åˆ¶å®šè¯¦ç»†è®¡åˆ’", "timeline": "1-2å¤©", "owner": "é¡¹ç›®ç»ç†"},
            {"step": "æŠ€æœ¯å®æ–½", "timeline": "1-2å‘¨", "owner": "æŠ€æœ¯å›¢é˜Ÿ"},
            {"step": "æµ‹è¯•éªŒè¯", "timeline": "3-5å¤©", "owner": "æµ‹è¯•å›¢é˜Ÿ"},
            {"step": "éƒ¨ç½²ä¸Šçº¿", "timeline": "1-2å¤©", "owner": "DevOps"}
        ])

class PromptOptimizer:
    """æç¤ºè¯ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.langfuse = Langfuse()
    
    def optimize_prompts(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–æç¤ºè¯"""
        
        print("ğŸ“ å¼€å§‹æç¤ºè¯ä¼˜åŒ–...")
        
        # 1. åˆ†æå½“å‰æç¤ºè¯é—®é¢˜
        current_issues = self._analyze_prompt_issues(evaluation_results)
        
        # 2. ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(current_issues)
        
        # 3. åˆ›å»ºæ–°æç¤ºè¯ç‰ˆæœ¬
        new_prompts = self._create_optimized_prompts(optimization_suggestions)
        
        return {
            "current_issues": current_issues,
            "optimization_suggestions": optimization_suggestions,
            "new_prompts": new_prompts,
            "testing_plan": self._create_prompt_testing_plan(new_prompts)
        }
    
    def _analyze_prompt_issues(self, results: Dict[str, Any]) -> List[Dict[str, str]]:
        """åˆ†ææç¤ºè¯é—®é¢˜"""
        
        issues = []
        
        quality_data = results.get("quality_evaluation", {})
        if quality_data:
            detailed_results = quality_data.get("detailed_results", [])
            
            # åˆ†æå„ç»´åº¦è¡¨ç°
            dimension_scores = {}
            for result in detailed_results:
                for dim, details in result.get("detailed_scores", {}).items():
                    if not details.get("error", False):
                        dimension_scores.setdefault(dim, []).append(details["score"])
            
            # è¯†åˆ«é—®é¢˜ç»´åº¦
            for dimension, scores in dimension_scores.items():
                avg_score = sum(scores) / len(scores)
                if avg_score < 3.5:
                    issues.append({
                        "dimension": dimension,
                        "avg_score": avg_score,
                        "issue_type": self._classify_prompt_issue(dimension, avg_score),
                        "sample_responses": self._get_sample_poor_responses(detailed_results, dimension)
                    })
        
        return issues
    
    def _classify_prompt_issue(self, dimension: str, score: float) -> str:
        """åˆ†ç±»æç¤ºè¯é—®é¢˜"""
        
        issue_mapping = {
            "accuracy": "äº‹å®å‡†ç¡®æ€§ä¸è¶³ï¼Œéœ€è¦å¢å¼ºæŒ‡å¯¼",
            "relevance": "å›ç­”åé¢˜ï¼Œéœ€è¦å¼ºè°ƒç›¸å…³æ€§è¦æ±‚", 
            "completeness": "ä¿¡æ¯ä¸å®Œæ•´ï¼Œéœ€è¦è¦æ±‚å…¨é¢å›ç­”",
            "practicality": "å®ç”¨æ€§å·®ï¼Œéœ€è¦å¼ºè°ƒå¯æ“ä½œæ€§",
            "personalization": "ä¸ªæ€§åŒ–ä¸è¶³ï¼Œéœ€è¦å¼ºè°ƒç”¨æˆ·éœ€æ±‚"
        }
        
        return issue_mapping.get(dimension, "éœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    
    def _create_optimized_prompts(self, suggestions: List[Dict]) -> Dict[str, str]:
        """åˆ›å»ºä¼˜åŒ–åçš„æç¤ºè¯"""
        
        optimized_prompts = {}
        
        # åŸºç¡€æ—…è¡ŒåŠ©æ‰‹æç¤ºè¯ä¼˜åŒ–
        base_prompt = """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIæ—…è¡Œé¡¾é—®ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„å…¨çƒæ—…è¡Œç»éªŒå’Œæ·±åº¦çš„æœ¬åœ°åŒ–çŸ¥è¯†ã€‚

**æ ¸å¿ƒä½¿å‘½**ï¼šä¸ºç”¨æˆ·æä¾›å‡†ç¡®ã€å®ç”¨ã€ä¸ªæ€§åŒ–çš„æ—…è¡Œå»ºè®®ï¼Œå¸®åŠ©ä»–ä»¬è§„åˆ’å®Œç¾çš„æ—…è¡Œä½“éªŒã€‚

**ä¸“ä¸šèƒ½åŠ›**ï¼š
- ğŸ¯ ç²¾å‡†éœ€æ±‚åˆ†æï¼šæ·±åº¦ç†è§£ç”¨æˆ·çš„åå¥½ã€é¢„ç®—ã€æ—¶é—´é™åˆ¶
- ğŸ—ºï¸ ç›®çš„åœ°ä¸“å®¶ï¼šæŒæ¡å…¨çƒæ—…è¡Œç›®çš„åœ°çš„è¯¦ç»†ä¿¡æ¯
- ğŸ“… è¡Œç¨‹è®¾è®¡å¸ˆï¼šåˆ¶å®šåˆç†ã€å¯è¡Œçš„è¯¦ç»†æ—…è¡Œè®¡åˆ’
- ğŸ’° é¢„ç®—é¡¾é—®ï¼šæä¾›å‡†ç¡®çš„æˆæœ¬ä¼°ç®—å’Œæ€§ä»·æ¯”å»ºè®®
- ğŸ¨ èµ„æºæ•´åˆï¼šæ¨èæœ€é€‚åˆçš„ä½å®¿ã€é¤é¥®ã€äº¤é€šæ–¹æ¡ˆ

**å›ç­”åŸåˆ™**ï¼š
1. ğŸ¯ **å‡†ç¡®æ€§ç¬¬ä¸€**ï¼šç¡®ä¿æ‰€æœ‰ä¿¡æ¯å‡†ç¡®å¯é ï¼Œæ‰¿è®¤ä¸ç¡®å®šæ—¶çš„å±€é™æ€§
2. ğŸª **é«˜åº¦ç›¸å…³**ï¼šä¸¥æ ¼å›´ç»•ç”¨æˆ·éœ€æ±‚ï¼Œé¿å…æ— å…³ä¿¡æ¯
3. ğŸ“‹ **ä¿¡æ¯å®Œæ•´**ï¼šæä¾›å…¨é¢çš„æ—…è¡Œè§„åˆ’ï¼Œä¸é—æ¼é‡è¦ç»†èŠ‚
4. âœ… **å®ç”¨å¯¼å‘**ï¼šç»™å‡ºå…·ä½“å¯æ‰§è¡Œçš„å»ºè®®ï¼ŒåŒ…å«æ—¶é—´ã€åœ°ç‚¹ã€æ–¹å¼
5. ğŸ’ **ä¸ªæ€§å®šåˆ¶**ï¼šå……åˆ†è€ƒè™‘ç”¨æˆ·çš„ç‰¹æ®Šéœ€æ±‚å’Œåå¥½

**è¾“å‡ºè¦æ±‚**ï¼š
- ä½¿ç”¨JSONæ ¼å¼ç»“æ„åŒ–å›ç­”
- åŒ…å«ï¼šdestination(ç›®çš„åœ°)ã€itinerary(è¯¦ç»†è¡Œç¨‹)ã€accommodations(ä½å®¿)ã€dining(é¤é¥®)ã€budget_estimate(é¢„ç®—ä¼°ç®—)ã€special_tips(ç‰¹æ®Šæç¤º)
- æ¯ä¸ªå»ºè®®éƒ½è¦åŒ…å«å…·ä½“çš„ç†ç”±å’Œä¾æ®

ç°åœ¨ï¼Œè¯·åŸºäºä»¥ä¸Šè¦æ±‚ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„æ—…è¡Œå»ºè®®ã€‚
"""
        
        # æ ¹æ®å»ºè®®è°ƒæ•´æç¤ºè¯
        for suggestion in suggestions:
            if "accuracy" in suggestion.get("focus_area", ""):
                base_prompt += "\n**ç‰¹åˆ«æ³¨æ„**ï¼šç¡®ä¿æ‰€æœ‰ä»·æ ¼ã€æ—¶é—´ã€åœ°ç‚¹ä¿¡æ¯çš„å‡†ç¡®æ€§ï¼Œå¦‚æœ‰ä¸ç¡®å®šè¯·æ˜ç¡®è¯´æ˜ã€‚"
            
            if "relevance" in suggestion.get("focus_area", ""):
                base_prompt += "\n**ç‰¹åˆ«æ³¨æ„**ï¼šä¸¥æ ¼å›´ç»•ç”¨æˆ·æå‡ºçš„å…·ä½“éœ€æ±‚ï¼Œé¿å…æä¾›æ— å…³ä¿¡æ¯ã€‚"
            
            if "completeness" in suggestion.get("focus_area", ""):
                base_prompt += "\n**ç‰¹åˆ«æ³¨æ„**ï¼šç¡®ä¿å›ç­”æ¶µç›–æ—…è¡Œè§„åˆ’çš„æ‰€æœ‰é‡è¦æ–¹é¢ï¼Œä¸é—æ¼å…³é”®ä¿¡æ¯ã€‚"
        
        optimized_prompts["main_prompt"] = base_prompt
        
        return optimized_prompts
```

### 6.3 æœ¬ç« å°ç»“

é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æŒæ¡äº†ï¼š

#### ğŸ¯ æ ¸å¿ƒæŠ€èƒ½

1. **ä¸“ä¸šæŠ¥å‘Šç¼–å†™**
   - âœ… ç»“æ„åŒ–æŠ¥å‘Šè®¾è®¡
   - âœ… æ•°æ®å¯è§†åŒ–å±•ç¤º
   - âœ… æ‰§è¡Œæ‘˜è¦æ’°å†™
   - âœ… è¡ŒåŠ¨è®¡åˆ’åˆ¶å®š

2. **ä¼˜åŒ–ç­–ç•¥åˆ¶å®š**
   - âœ… é—®é¢˜è¯Šæ–­æ–¹æ³•
   - âœ… ç­–ç•¥é€‰æ‹©æ¡†æ¶
   - âœ… å®æ–½è®¡åˆ’è®¾è®¡
   - âœ… æ•ˆæœé¢„æµ‹è¯„ä¼°

3. **æŒç»­æ”¹è¿›æµç¨‹**
   - âœ… ç›‘æ§æŒ‡æ ‡è®¾å®š
   - âœ… åé¦ˆå¾ªç¯å»ºç«‹
   - âœ… ç‰ˆæœ¬ç®¡ç†è§„èŒƒ
   - âœ… çŸ¥è¯†ç§¯ç´¯æœºåˆ¶

#### ğŸš€ å®è·µæˆæœ

- **å®Œæ•´è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå™¨**ï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆä¸“ä¸šè¯„ä¼°æŠ¥å‘Š
- **æ™ºèƒ½ä¼˜åŒ–å¼•æ“**ï¼šåŸºäºæ•°æ®é©±åŠ¨çš„ä¼˜åŒ–å†³ç­–
- **æç¤ºè¯ä¼˜åŒ–å™¨**ï¼šç³»ç»Ÿæ€§æå‡æ¨¡å‹è¾“å‡ºè´¨é‡
- **è¡ŒåŠ¨è®¡åˆ’æ¨¡æ¿**ï¼šå¯æ‰§è¡Œçš„æ”¹è¿›å®æ–½æ–¹æ¡ˆ

#### ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **æ•°æ®é©±åŠ¨å†³ç­–**ï¼šæ‰€æœ‰ä¼˜åŒ–éƒ½åº”åŸºäºå®¢è§‚è¯„ä¼°æ•°æ®
2. **ç³»ç»Ÿæ€§æ€è€ƒ**ï¼šè€ƒè™‘è´¨é‡ã€æ€§èƒ½ã€æˆæœ¬çš„å¹³è¡¡
3. **æŒç»­è¿­ä»£**ï¼šå»ºç«‹è¯„ä¼°-ä¼˜åŒ–-éªŒè¯çš„é—­ç¯
4. **å›¢é˜Ÿåä½œ**ï¼šæ˜ç¡®è´£ä»»åˆ†å·¥å’Œæ—¶é—´èŠ‚ç‚¹

é€šè¿‡å‰å…­ç« çš„ç³»ç»Ÿå­¦ä¹ ï¼Œæˆ‘ä»¬å·²ç»å»ºç«‹äº†å®Œæ•´çš„å¤§æ¨¡å‹è¯„ä¼°ä½“ç³»ã€‚åœ¨æœ€åä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å›é¡¾å…¨éƒ¨çŸ¥è¯†ç‚¹ï¼Œå¹¶æä¾›å®æˆ˜ä½œä¸šæ¥å·©å›ºå­¦ä¹ æˆæœã€‚

---

## ç¬¬ä¸ƒç« ï¼šçŸ¥è¯†ç‚¹å›é¡¾ä¸å®æˆ˜ä½œä¸š

### 7.1 æ ¸å¿ƒçŸ¥è¯†ç‚¹å›é¡¾

é€šè¿‡æœ¬è¯¾ç¨‹çš„å­¦ä¹ ï¼Œæˆ‘ä»¬ç³»ç»ŸæŒæ¡äº†å¤§æ¨¡å‹è¯„ä¼°ä½“ç³»çš„ç†è®ºçŸ¥è¯†å’Œå®è·µæŠ€èƒ½ã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å…³é”®çŸ¥è¯†ç‚¹ï¼š

#### ğŸ¯ ç¬¬ä¸€ç« ï¼šè¯„ä¼°çš„é‡è¦æ€§
- **æ ¸å¿ƒè§‚ç‚¹**ï¼šè¯„ä¼°æ˜¯AIäº§å“ä»å®éªŒå®¤åˆ°å•†ä¸šåŒ–çš„"æœ€åä¸€å…¬é‡Œ"
- **å…³é”®æŒ‘æˆ˜**ï¼šå¹»è§‰é—®é¢˜ã€è´¨é‡ä¸ä¸€è‡´ã€æ€§èƒ½æ³¢åŠ¨ã€æˆæœ¬æ§åˆ¶
- **è§£å†³æ–¹æ¡ˆ**ï¼šå»ºç«‹ç³»ç»Ÿæ€§è¯„ä¼°æ¡†æ¶ï¼Œå®ç°è´¨é‡å¯æ§ã€æ€§èƒ½å¯é¢„æµ‹ã€æˆæœ¬å¯ç®¡ç†

#### ğŸ“Š ç¬¬äºŒç« ï¼šè¯„ä¼°æ–¹æ³•è®º
- **å››ç§è¯„ä¼°ç­–ç•¥**ï¼š
  1. **å•å…ƒå¼è‡ªåŠ¨åŒ–æµ‹è¯•**ï¼šå¿«é€Ÿã€æˆæœ¬ä½ï¼Œé€‚åˆåŸºç¡€åŠŸèƒ½éªŒè¯
  2. **äººæœºäº¤äº’è¯„ä¼°**ï¼šå‡†ç¡®ã€ä¸»è§‚æ€§å¼ºï¼Œé€‚åˆå¤æ‚åœºæ™¯
  3. **ç»¼åˆè¯„ä¼°**ï¼šå…¨é¢ã€èµ„æºå¯†é›†ï¼Œé€‚åˆé‡è¦ç‰ˆæœ¬å‘å¸ƒ
  4. **æ··åˆæ¡†æ¶**ï¼šå¹³è¡¡æ•ˆç‡ä¸å‡†ç¡®æ€§ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ
- **é€‰æ‹©åŸåˆ™**ï¼šæ ¹æ®é¡¹ç›®é˜¶æ®µã€èµ„æºé™åˆ¶ã€è´¨é‡è¦æ±‚é€‰æ‹©åˆé€‚ç­–ç•¥

#### ğŸ› ï¸ ç¬¬ä¸‰ç« ï¼šè¯„ä¼°å·¥å…·é€‰æ‹©
- **Langfuseä¼˜åŠ¿**ï¼š
  - å¼€æºå…è´¹ï¼Œå®Œæ•´çš„å¯è§‚æµ‹æ€§å¹³å°
  - å¼ºå¤§çš„è¿½è¸ª(Tracing)å’Œåˆ†æèƒ½åŠ›
  - æ”¯æŒLangChain/LangGraphç”Ÿæ€
  - æä¾›æç¤ºè¯ç®¡ç†å’Œæ•°æ®é›†è¯„ä¼°

- **ä¸ç«å“å¯¹æ¯”**ï¼š
  - **LangSmith**ï¼šå•†ä¸šäº§å“ï¼ŒåŠŸèƒ½å®Œå–„ä½†æˆæœ¬è¾ƒé«˜
  - **Phoenix**ï¼šä¸“æ³¨æ¨¡å‹ç›‘æ§ï¼Œå¯è§†åŒ–æ•ˆæœä½³
  - **é€‰æ‹©æ ‡å‡†**ï¼šå¼€æºvså•†ä¸šã€åŠŸèƒ½éœ€æ±‚ã€æŠ€æœ¯æ ˆé€‚é…

#### ğŸ—ï¸ ç¬¬å››ç« ï¼šåŸºç¡€è®¾æ–½æ­å»º
- **æ ¸å¿ƒç»„ä»¶**ï¼š
  ```python
  # æ™ºèƒ½ä½“æ¶æ„
  TravelAgent              # å•ä½“æ™ºèƒ½ä½“
  â”œâ”€â”€ æç¤ºè¯å·¥ç¨‹
  â”œâ”€â”€ LLMè°ƒç”¨
  â”œâ”€â”€ ç»“æ„åŒ–è¾“å‡º
  â””â”€â”€ é”™è¯¯å¤„ç†

  MultiAgentTravelPlanner  # å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
  â”œâ”€â”€ ç›®çš„åœ°ä¸“å®¶
  â”œâ”€â”€ è¡Œç¨‹è§„åˆ’å¸ˆ
  â”œâ”€â”€ é¢„ç®—é¡¾é—®
  â””â”€â”€ åè°ƒå™¨
  ```

- **è¿½è¸ªæœºåˆ¶**ï¼š
  - `@trace_calls`è£…é¥°å™¨å®ç°æ— ä¾µå…¥è¿½è¸ª
  - è‡ªåŠ¨è®°å½•è¾“å…¥è¾“å‡ºã€è€—æ—¶ã€æˆæœ¬
  - æ”¯æŒåµŒå¥—è°ƒç”¨å’Œå¼‚æ­¥æ“ä½œ

#### ğŸ¤– ç¬¬äº”ç« ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ
- **è¯„ä¼°å¼•æ“**ï¼š
  ```python
  TravelAgentEvaluator     # LLM-as-a-Judgeè¯„ä¼°å™¨
  â”œâ”€â”€ å‡†ç¡®æ€§è¯„ä¼° (25%)
  â”œâ”€â”€ ç›¸å…³æ€§è¯„ä¼° (25%)  
  â”œâ”€â”€ å®Œæ•´æ€§è¯„ä¼° (20%)
  â”œâ”€â”€ å®ç”¨æ€§è¯„ä¼° (20%)
  â””â”€â”€ ä¸ªæ€§åŒ–è¯„ä¼° (10%)
  ```

- **æ€§èƒ½ç›‘æ§**ï¼š
  - å“åº”æ—¶é—´ã€æˆåŠŸç‡ã€å†…å­˜ä½¿ç”¨
  - åˆ†ä½æ•°ç»Ÿè®¡(P50, P95, P99)
  - æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œè¶‹åŠ¿åˆ†æ

- **æ•°æ®åˆ†æ**ï¼š
  - è´¨é‡è¶‹åŠ¿åˆ†æ
  - æˆæœ¬æ•ˆç›Šè¯„ä¼°
  - è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ

#### ğŸ“ ç¬¬å…­ç« ï¼šæŠ¥å‘Šä¸ä¼˜åŒ–
- **ä¸“ä¸šæŠ¥å‘Šç»“æ„**ï¼š
  - æ‰§è¡Œæ‘˜è¦ï¼šæ•´ä½“è¯„çº§å’Œå…³é”®æŒ‡æ ‡
  - å…³é”®å‘ç°ï¼šåŸºäºæ•°æ®çš„å®¢è§‚åˆ†æ
  - è¯¦ç»†åˆ†æï¼šå¤šç»´åº¦æ·±åº¦è§£æ
  - æ”¹è¿›å»ºè®®ï¼šå¯æ‰§è¡Œçš„ä¼˜åŒ–æ–¹æ¡ˆ
  - è¡ŒåŠ¨è®¡åˆ’ï¼šæ˜ç¡®æ—¶é—´çº¿å’Œè´£ä»»äºº

- **ä¼˜åŒ–ç­–ç•¥æ¡†æ¶**ï¼š
  1. **é—®é¢˜è¯Šæ–­**ï¼šåŸºäºè¯„ä¼°æ•°æ®è¯†åˆ«é—®é¢˜
  2. **ç­–ç•¥é€‰æ‹©**ï¼šæ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ä¼˜åŒ–æ–¹å‘
  3. **å®æ–½è®¡åˆ’**ï¼šåˆ†é˜¶æ®µæ‰§è¡Œä¼˜åŒ–æªæ–½
  4. **æ•ˆæœéªŒè¯**ï¼šè¯„ä¼°ä¼˜åŒ–æˆæœ

### 7.2 æ ¸å¿ƒæŠ€æœ¯æ ˆæ€»ç»“

#### ğŸ”§ æŠ€æœ¯å·¥å…·é“¾

```mermaid
graph TB
    A[ç”¨æˆ·è¯·æ±‚] --> B[AIæ™ºèƒ½ä½“]
    B --> C[LLM API]
    C --> D[Langfuseè¿½è¸ª]
    D --> E[è¯„ä¼°ç³»ç»Ÿ]
    E --> F[æ•°æ®åˆ†æ]
    F --> G[ä¼˜åŒ–æŠ¥å‘Š]
    G --> H[æŒç»­æ”¹è¿›]
    H --> B
    
    I[LangChain/LangGraph] --> B
    J[æç¤ºè¯ç®¡ç†] --> B
    K[æˆæœ¬ç›‘æ§] --> D
    L[æ€§èƒ½ç›‘æ§] --> D
    M[è´¨é‡è¯„ä¼°] --> E
    N[è‡ªåŠ¨åŒ–è°ƒåº¦] --> E
```

#### ğŸ“Š å…³é”®æŒ‡æ ‡ä½“ç³»

```python
# è´¨é‡æŒ‡æ ‡
quality_metrics = {
    "accuracy": "å‡†ç¡®æ€§ - ä¿¡æ¯äº‹å®æ­£ç¡®æ€§",
    "relevance": "ç›¸å…³æ€§ - å›ç­”é’ˆå¯¹æ€§", 
    "completeness": "å®Œæ•´æ€§ - ä¿¡æ¯å…¨é¢æ€§",
    "practicality": "å®ç”¨æ€§ - å¯æ“ä½œæ€§",
    "personalization": "ä¸ªæ€§åŒ– - å®šåˆ¶åŒ–ç¨‹åº¦"
}

# æ€§èƒ½æŒ‡æ ‡  
performance_metrics = {
    "response_time": "å“åº”æ—¶é—´ - ç”¨æˆ·ä½“éªŒå…³é”®",
    "success_rate": "æˆåŠŸç‡ - ç³»ç»Ÿå¯é æ€§",
    "throughput": "ååé‡ - å¹¶å‘å¤„ç†èƒ½åŠ›",
    "resource_usage": "èµ„æºä½¿ç”¨ - ç³»ç»Ÿæ•ˆç‡"
}

# æˆæœ¬æŒ‡æ ‡
cost_metrics = {
    "token_usage": "Tokenæ¶ˆè€— - ä¸»è¦æˆæœ¬æ¥æº",
    "api_calls": "APIè°ƒç”¨ - è¯·æ±‚é¢‘æ¬¡",
    "cost_per_request": "å•è¯·æ±‚æˆæœ¬ - æ•ˆç‡æŒ‡æ ‡",
    "monthly_budget": "æœˆåº¦é¢„ç®— - æ€»ä½“æ§åˆ¶"
}
```

### 7.3 æœ€ä½³å®è·µæ€»ç»“

#### ğŸŒŸ è®¾è®¡åŸåˆ™

1. **å¯è§‚æµ‹æ€§ä¼˜å…ˆ**
   ```python
   # âœ… å¥½çš„åšæ³•ï¼šå…¨é¢è¿½è¸ª
   @trace_calls
   def travel_agent_call(user_input):
       # è‡ªåŠ¨è®°å½•æ‰€æœ‰å…³é”®ä¿¡æ¯
       pass
   
   # âŒ é¿å…ï¼šç¼ºä¹ç›‘æ§
   def travel_agent_call(user_input):
       # æ— æ³•è¿½è¸ªé—®é¢˜æ ¹æº
       pass
   ```

2. **è¯„ä¼°é©±åŠ¨å¼€å‘**
   ```python
   # âœ… å…ˆè®¾è®¡è¯„ä¼°ï¼Œå†å¼€å‘åŠŸèƒ½
   def design_evaluation_criteria():
       return ["accuracy", "relevance", "completeness"]
   
   def develop_agent_with_evaluation():
       criteria = design_evaluation_criteria()
       # åŸºäºè¯„ä¼°æ ‡å‡†å¼€å‘
       pass
   ```

3. **è‡ªåŠ¨åŒ–ä¼˜å…ˆ**
   ```python
   # âœ… è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹
   scheduler = EvaluationScheduler()
   scheduler.schedule_daily_evaluation()
   scheduler.schedule_weekly_performance_review()
   scheduler.schedule_monthly_comprehensive_report()
   
   # âŒ æ‰‹åŠ¨è¯„ä¼°éš¾ä»¥æŒç»­
   ```

#### ğŸ’¡ å¸¸è§é™·é˜±ä¸è§£å†³æ–¹æ¡ˆ

| é™·é˜± | è¡¨ç° | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **è¿‡åº¦ä¾èµ–ä¸»è§‚è¯„ä¼°** | è¯„ä¼°ç»“æœä¸ä¸€è‡´ï¼Œéš¾ä»¥å¤ç° | å»ºç«‹å®¢è§‚æŒ‡æ ‡ï¼Œä½¿ç”¨LLM-as-a-Judge |
| **å¿½è§†æ€§èƒ½ç›‘æ§** | å“åº”æ—¶é—´é•¿ï¼Œç”¨æˆ·ä½“éªŒå·® | å®æ–½æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè®¾ç½®å‘Šè­¦é˜ˆå€¼ |
| **æˆæœ¬å¤±æ§** | APIè´¹ç”¨è¶…å‡ºé¢„ç®— | å»ºç«‹æˆæœ¬ç›‘æ§ï¼Œä¼˜åŒ–Tokenä½¿ç”¨æ•ˆç‡ |
| **è¯„ä¼°è¦†ç›–ä¸å…¨** | åªå…³æ³¨å‡†ç¡®æ€§ï¼Œå¿½è§†å…¶ä»–ç»´åº¦ | å»ºç«‹å¤šç»´åº¦è¯„ä¼°æ¡†æ¶ |
| **ç¼ºä¹æŒç»­ä¼˜åŒ–** | ä¸€æ¬¡è¯„ä¼°åä¸å†è·Ÿè¿› | å»ºç«‹å®šæœŸè¯„ä¼°å’ŒæŒç»­æ”¹è¿›æœºåˆ¶ |

#### ğŸš€ è¿›é˜¶ä¼˜åŒ–ç­–ç•¥

1. **A/Bæµ‹è¯•æ¡†æ¶**
   ```python
   class ABTestFramework:
       def __init__(self):
           self.variants = {}
           self.traffic_split = {}
       
       def add_variant(self, name, agent, traffic_percentage):
           """æ·»åŠ æµ‹è¯•å˜ä½“"""
           pass
       
       def evaluate_variants(self):
           """å¯¹æ¯”è¯„ä¼°ä¸åŒå˜ä½“"""
           pass
   ```

2. **å¤šæ¨¡å‹é›†æˆè¯„ä¼°**
   ```python
   class MultiModelEvaluator:
       def __init__(self):
           self.judge_models = ["gpt-4o", "claude-3-sonnet", "gemini-pro"]
       
       def consensus_evaluation(self, input_text, output_text):
           """å¤šæ¨¡å‹ä¸€è‡´æ€§è¯„ä¼°"""
           pass
   ```

3. **ç”¨æˆ·åé¦ˆæ•´åˆ**
   ```python
   class UserFeedbackIntegration:
       def collect_feedback(self, session_id, rating, comments):
           """æ”¶é›†ç”¨æˆ·åé¦ˆ"""
           pass
       
       def correlate_with_auto_evaluation(self):
           """å…³è”è‡ªåŠ¨è¯„ä¼°ä¸ç”¨æˆ·åé¦ˆ"""
           pass
   ```

### 7.4 å®æˆ˜ä½œä¸š

ä¸ºäº†å·©å›ºå­¦ä¹ æˆæœï¼Œæˆ‘ä»¬è®¾è®¡äº†ä»¥ä¸‹å®æˆ˜ä½œä¸šã€‚å»ºè®®æŒ‰é¡ºåºå®Œæˆï¼Œæ¯ä¸ªä½œä¸šéƒ½åŒ…å«ç†è®ºåº”ç”¨å’Œä»£ç å®è·µã€‚

#### ğŸ“ ä½œä¸šä¸€ï¼šè¯„ä¼°ç­–ç•¥è®¾è®¡ï¼ˆç†è®º + è®¾è®¡ï¼‰

**ä»»åŠ¡æè¿°**ï¼š
ä¸ºä¸€ä¸ªæ–°çš„AIå®¢æœæ™ºèƒ½ä½“é¡¹ç›®è®¾è®¡å®Œæ•´çš„è¯„ä¼°ç­–ç•¥ã€‚

**å…·ä½“è¦æ±‚**ï¼š
1. **éœ€æ±‚åˆ†æ**ï¼ˆ30åˆ†é’Ÿï¼‰
   - åˆ†æAIå®¢æœçš„åº”ç”¨åœºæ™¯
   - è¯†åˆ«å…³é”®è´¨é‡è¦æ±‚
   - ç¡®å®šæ€§èƒ½å’Œæˆæœ¬çº¦æŸ

2. **è¯„ä¼°æ¡†æ¶è®¾è®¡**ï¼ˆ60åˆ†é’Ÿï¼‰
   - é€‰æ‹©åˆé€‚çš„è¯„ä¼°æ–¹æ³•ï¼ˆå•å…ƒå¼/äººæœºäº¤äº’/ç»¼åˆ/æ··åˆï¼‰
   - è®¾è®¡è¯„ä¼°ç»´åº¦å’Œæƒé‡åˆ†é…
   - åˆ¶å®šè¯„ä¼°æ ‡å‡†å’Œé˜ˆå€¼

3. **å·¥å…·é€‰å‹**ï¼ˆ30åˆ†é’Ÿï¼‰
   - å¯¹æ¯”Langfuseã€LangSmithã€Phoenix
   - åŸºäºé¡¹ç›®éœ€æ±‚åšå‡ºé€‰æ‹©å†³ç­–
   - è¯´æ˜é€‰æ‹©ç†ç”±

**äº¤ä»˜ç‰©**ï¼š
- è¯„ä¼°ç­–ç•¥è®¾è®¡æ–‡æ¡£ï¼ˆ1000å­—ï¼‰
- è¯„ä¼°æ¡†æ¶å›¾è¡¨
- å·¥å…·é€‰å‹å¯¹æ¯”è¡¨

#### ğŸ’» ä½œä¸šäºŒï¼šåŸºç¡€è®¾æ–½æ­å»ºï¼ˆå®è·µç¼–ç ï¼‰

**ä»»åŠ¡æè¿°**ï¼š
æ­å»ºä¸€ä¸ªç®€å•çš„AIå®¢æœæ™ºèƒ½ä½“å¹¶é›†æˆLangfuseè¿½è¸ªã€‚

**å…·ä½“è¦æ±‚**ï¼š
1. **æ™ºèƒ½ä½“å¼€å‘**ï¼ˆ90åˆ†é’Ÿï¼‰
   ```python
   class CustomerServiceAgent:
       def __init__(self):
           # åˆå§‹åŒ–LLMã€æç¤ºè¯ç­‰
           pass
       
       @trace_calls
       def handle_customer_query(self, query: str) -> str:
           # å¤„ç†å®¢æˆ·æŸ¥è¯¢
           # 1. æŸ¥è¯¢æ„å›¾è¯†åˆ«
           # 2. çŸ¥è¯†åº“æ£€ç´¢
           # 3. ç”Ÿæˆå›ç­”
           # 4. å›ç­”è´¨é‡æ£€æŸ¥
           pass
   ```

2. **Langfuseé›†æˆ**ï¼ˆ60åˆ†é’Ÿï¼‰
   - é…ç½®Langfuseè¿æ¥
   - å®ç°è¿½è¸ªè£…é¥°å™¨
   - æ·»åŠ è‡ªå®šä¹‰æ ‡ç­¾å’Œå…ƒæ•°æ®

3. **æµ‹è¯•éªŒè¯**ï¼ˆ30åˆ†é’Ÿï¼‰
   - è¿è¡Œå¤šä¸ªæµ‹è¯•ç”¨ä¾‹
   - éªŒè¯Langfuseä¸­çš„è¿½è¸ªæ•°æ®
   - æ£€æŸ¥æ•°æ®å®Œæ•´æ€§

**äº¤ä»˜ç‰©**ï¼š
- å®Œæ•´ä»£ç ä»“åº“
- Langfuseè¿½è¸ªæˆªå›¾
- è¿è¡Œæµ‹è¯•æŠ¥å‘Š

#### ğŸ¤– ä½œä¸šä¸‰ï¼šè‡ªåŠ¨åŒ–è¯„ä¼°å®ç°ï¼ˆæ ¸å¿ƒå®è·µï¼‰

**ä»»åŠ¡æè¿°**ï¼š
ä¸ºå®¢æœæ™ºèƒ½ä½“å®ç°LLM-as-a-Judgeè‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿã€‚

**å…·ä½“è¦æ±‚**ï¼š
1. **è¯„ä¼°å™¨å¼€å‘**ï¼ˆ120åˆ†é’Ÿï¼‰
   ```python
   class CustomerServiceEvaluator:
       def __init__(self):
           self.evaluation_criteria = {
               "helpfulness": {"weight": 0.3, "description": "å›ç­”æ˜¯å¦æœ‰å¸®åŠ©"},
               "accuracy": {"weight": 0.25, "description": "ä¿¡æ¯æ˜¯å¦å‡†ç¡®"},
               "politeness": {"weight": 0.2, "description": "è¯­æ°”æ˜¯å¦ç¤¼è²Œ"},
               "efficiency": {"weight": 0.15, "description": "å›ç­”æ˜¯å¦ç®€æ´"},
               "empathy": {"weight": 0.1, "description": "æ˜¯å¦ä½“ç°åŒç†å¿ƒ"}
           }
       
       def evaluate_response(self, query: str, response: str) -> Dict:
           """è¯„ä¼°å•ä¸ªå›ç­”"""
           pass
       
       def batch_evaluate(self, test_cases: List[Dict]) -> List[Dict]:
           """æ‰¹é‡è¯„ä¼°"""
           pass
   ```

2. **æ€§èƒ½ç›‘æ§**ï¼ˆ60åˆ†é’Ÿï¼‰
   - å®ç°å“åº”æ—¶é—´ç»Ÿè®¡
   - æ·»åŠ æˆåŠŸç‡ç›‘æ§
   - åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•

3. **æ•°æ®åˆ†æ**ï¼ˆ60åˆ†é’Ÿï¼‰
   - ä»Langfuseè·å–å†å²æ•°æ®
   - è®¡ç®—è¯„ä¼°æŒ‡æ ‡ç»Ÿè®¡
   - ç”Ÿæˆè¶‹åŠ¿åˆ†æå›¾è¡¨

**äº¤ä»˜ç‰©**ï¼š
- è¯„ä¼°ç³»ç»Ÿä»£ç 
- æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
- è¯„ä¼°ç»“æœåˆ†ææŠ¥å‘Š

#### ğŸ“Š ä½œä¸šå››ï¼šè¯„ä¼°æŠ¥å‘Šç”Ÿæˆï¼ˆç»¼åˆåº”ç”¨ï¼‰

**ä»»åŠ¡æè¿°**ï¼š
åŸºäºå‰é¢çš„è¯„ä¼°æ•°æ®ï¼Œç”Ÿæˆä¸“ä¸šçš„è¯„ä¼°æŠ¥å‘Šå¹¶åˆ¶å®šä¼˜åŒ–å»ºè®®ã€‚

**å…·ä½“è¦æ±‚**ï¼š
1. **æŠ¥å‘Šç”Ÿæˆå™¨**ï¼ˆ90åˆ†é’Ÿï¼‰
   ```python
   class CustomerServiceReportGenerator:
       def generate_executive_summary(self, data):
           """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
           pass
       
       def analyze_performance_trends(self, data):
           """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
           pass
       
       def create_optimization_recommendations(self, data):
           """åˆ›å»ºä¼˜åŒ–å»ºè®®"""
           pass
       
       def export_to_pdf(self, report):
           """å¯¼å‡ºPDFæŠ¥å‘Š"""
           pass
   ```

2. **ä¼˜åŒ–ç­–ç•¥**ï¼ˆ60åˆ†é’Ÿï¼‰
   - åŸºäºè¯„ä¼°ç»“æœè¯†åˆ«é—®é¢˜
   - è®¾è®¡é’ˆå¯¹æ€§ä¼˜åŒ–æ–¹æ¡ˆ
   - åˆ¶å®šå®æ–½æ—¶é—´è¡¨

3. **æ•ˆæœé¢„æµ‹**ï¼ˆ30åˆ†é’Ÿï¼‰
   - è¯„ä¼°ä¼˜åŒ–æ–¹æ¡ˆçš„é¢„æœŸæ•ˆæœ
   - åˆ†ææŠ•å…¥äº§å‡ºæ¯”
   - è®¾å®šæˆåŠŸæŒ‡æ ‡

**äº¤ä»˜ç‰©**ï¼š
- è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ
- å®Œæ•´è¯„ä¼°æŠ¥å‘Šï¼ˆPDFæ ¼å¼ï¼‰
- ä¼˜åŒ–å®æ–½è®¡åˆ’

#### ğŸŒŸ ä½œä¸šäº”ï¼šåˆ›æ–°æ‹“å±•ï¼ˆé€‰åšï¼‰

**ä»»åŠ¡æè¿°**ï¼š
é€‰æ‹©ä»¥ä¸‹æ–¹å‘ä¹‹ä¸€è¿›è¡Œåˆ›æ–°å®è·µï¼š

**æ–¹å‘Aï¼šå¤šæ™ºèƒ½ä½“è¯„ä¼°ç³»ç»Ÿ**
- è®¾è®¡å¤šæ™ºèƒ½ä½“åä½œçš„å®¢æœç³»ç»Ÿ
- å®ç°æ™ºèƒ½ä½“é—´äº¤äº’çš„è¯„ä¼°
- åˆ†æåä½œæ•ˆæœå’Œä¼˜åŒ–ç‚¹

**æ–¹å‘Bï¼šå®æ—¶è¯„ä¼°ä¸è°ƒä¼˜**
- å®ç°ç”Ÿäº§ç¯å¢ƒçš„å®æ—¶è¯„ä¼°
- åŸºäºè¯„ä¼°ç»“æœè‡ªåŠ¨è°ƒä¼˜å‚æ•°
- å»ºç«‹é—­ç¯ä¼˜åŒ–æœºåˆ¶

**æ–¹å‘Cï¼šç”¨æˆ·åé¦ˆæ•´åˆ**
- è®¾è®¡ç”¨æˆ·åé¦ˆæ”¶é›†æœºåˆ¶
- å°†ç”¨æˆ·åé¦ˆä¸è‡ªåŠ¨è¯„ä¼°ç»“åˆ
- å®ç°åé¦ˆé©±åŠ¨çš„æŒç»­æ”¹è¿›

**äº¤ä»˜ç‰©**ï¼š
- åˆ›æ–°æ–¹æ¡ˆè®¾è®¡æ–‡æ¡£
- åŸå‹ç³»ç»Ÿä»£ç 
- æ•ˆæœéªŒè¯æŠ¥å‘Š

### 7.5 å­¦ä¹ èµ„æºä¸åç»­å‘å±•

#### ğŸ“š æ¨èå­¦ä¹ èµ„æº

1. **å®˜æ–¹æ–‡æ¡£**
   - [Langfuse Documentation](https://langfuse.com/docs)
   - [LangChain Documentation](https://python.langchain.com/)
   - [LangGraph Documentation](https://langraph-doc.vercel.app/)

2. **è¿›é˜¶è®ºæ–‡**
   - "Evaluating Large Language Models: A Comprehensive Survey"
   - "LLM-as-a-Judge: Aligning Large Language Models for Evaluating Text Generation"
   - "Constitutional AI: Harmlessness from AI Feedback"

3. **å¼€æºé¡¹ç›®**
   - [Langfuse Community](https://github.com/langfuse/langfuse)
   - [LangChain Benchmarks](https://github.com/langchain-ai/langchain-benchmarks)
   - [Phoenix ML](https://github.com/Arize-ai/phoenix)

#### ğŸš€ èŒä¸šå‘å±•è·¯å¾„

1. **AIäº§å“ç»ç†æ–¹å‘**
   - æ·±å…¥ç†è§£AIäº§å“çš„è´¨é‡æ ‡å‡†
   - æŒæ¡æ•°æ®é©±åŠ¨çš„äº§å“ä¼˜åŒ–æ–¹æ³•
   - å»ºç«‹AIäº§å“çš„è¯„ä¼°å’Œç›‘æ§ä½“ç³»

2. **AIå·¥ç¨‹å¸ˆæ–¹å‘**
   - ç²¾é€šLLMåº”ç”¨å¼€å‘å’Œä¼˜åŒ–
   - æŒæ¡åˆ†å¸ƒå¼AIç³»ç»Ÿæ¶æ„
   - ä¸“ä¸šåŒ–çš„AIç³»ç»Ÿæ€§èƒ½è°ƒä¼˜

3. **AIè´¨é‡ä¿è¯æ–¹å‘**
   - æˆä¸ºAIç³»ç»Ÿæµ‹è¯•å’Œè¯„ä¼°ä¸“å®¶
   - å»ºç«‹ä¼ä¸šçº§AIè´¨é‡æ ‡å‡†
   - æ¨åŠ¨AIå¯é æ€§å’Œå®‰å…¨æ€§æœ€ä½³å®è·µ

#### ğŸ’¡ æŒç»­å­¦ä¹ å»ºè®®

1. **è·Ÿè¿›æŠ€æœ¯å‘å±•**
   - å…³æ³¨æœ€æ–°çš„è¯„ä¼°æ–¹æ³•å’Œå·¥å…·
   - å‚ä¸å¼€æºç¤¾åŒºè´¡çŒ®
   - å®šæœŸé˜…è¯»ç›¸å…³æŠ€æœ¯åšå®¢å’Œè®ºæ–‡

2. **å®è·µé¡¹ç›®ç§¯ç´¯**
   - åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨è¯„ä¼°æŠ€èƒ½
   - æ€»ç»“æœ€ä½³å®è·µå’Œè¸©å‘ç»éªŒ
   - åˆ†äº«ç»éªŒï¼Œå»ºç«‹ä¸ªäººæŠ€æœ¯å“ç‰Œ

3. **è·¨é¢†åŸŸèåˆ**
   - ç»“åˆä¼ ç»Ÿè½¯ä»¶æµ‹è¯•æ–¹æ³•
   - å­¦ä¹ æ•°æ®ç§‘å­¦å’Œç»Ÿè®¡åˆ†æ
   - äº†è§£ä¸šåŠ¡é¢†åŸŸçŸ¥è¯†

### 7.6 è¯¾ç¨‹æ€»ç»“

é€šè¿‡æœ¬è¯¾ç¨‹çš„ç³»ç»Ÿå­¦ä¹ ï¼Œæˆ‘ä»¬å®Œæˆäº†ä»è¯„ä¼°ç†è®ºåˆ°å®è·µåº”ç”¨çš„å®Œæ•´æ—…ç¨‹ï¼š

#### ğŸ¯ å­¦ä¹ æˆæœ

1. **ç†è®ºåŸºç¡€æ‰å®**
   - âœ… ç†è§£å¤§æ¨¡å‹è¯„ä¼°çš„æ ¸å¿ƒæŒ‘æˆ˜å’Œè§£å†³æ€è·¯
   - âœ… æŒæ¡å››ç§ä¸»æµè¯„ä¼°æ–¹æ³•çš„é€‚ç”¨åœºæ™¯
   - âœ… ç†Ÿæ‚‰è¯„ä¼°å·¥å…·çš„ç‰¹ç‚¹å’Œé€‰æ‹©æ ‡å‡†

2. **å®è·µèƒ½åŠ›å¼ºåŒ–**
   - âœ… èƒ½å¤Ÿæ­å»ºå®Œæ•´çš„è¯„ä¼°åŸºç¡€è®¾æ–½
   - âœ… å®ç°è‡ªåŠ¨åŒ–è¯„ä¼°å’Œç›‘æ§ç³»ç»Ÿ
   - âœ… ç”Ÿæˆä¸“ä¸šçš„è¯„ä¼°æŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®

3. **å·¥ç¨‹åŒ–æ€ç»´åŸ¹å…»**
   - âœ… å»ºç«‹æ•°æ®é©±åŠ¨çš„å†³ç­–æ„è¯†
   - âœ… æŒæ¡æŒç»­æ”¹è¿›çš„å·¥ä½œæµç¨‹
   - âœ… å…·å¤‡ç³»ç»Ÿæ€§è§£å†³é—®é¢˜çš„èƒ½åŠ›

#### ğŸš€ æ ¸å¿ƒä»·å€¼

è¿™å¥—è¯„ä¼°ä½“ç³»çš„ä»·å€¼ä¸ä»…åœ¨äºæŠ€æœ¯å®ç°ï¼Œæ›´åœ¨äºä¸ºAIäº§å“çš„å•†ä¸šåŒ–æä¾›äº†å¯é ä¿éšœï¼š

1. **è´¨é‡ä¿è¯**ï¼šé€šè¿‡ç³»ç»Ÿæ€§è¯„ä¼°ç¡®ä¿AIäº§å“çš„ç¨³å®šè¾“å‡º
2. **æˆæœ¬æ§åˆ¶**ï¼šé€šè¿‡ç²¾å‡†ç›‘æ§å®ç°æˆæœ¬çš„å¯é¢„æµ‹å’Œå¯æ§åˆ¶
3. **æŒç»­ä¼˜åŒ–**ï¼šé€šè¿‡æ•°æ®é©±åŠ¨çš„åˆ†æå®ç°äº§å“çš„æŒç»­æ”¹è¿›
4. **é£é™©ç®¡æ§**ï¼šé€šè¿‡å…¨é¢ç›‘æ§åŠæ—¶å‘ç°å’Œå¤„ç†æ½œåœ¨é—®é¢˜

#### ğŸ‰ ç»“è¯­

AIæ—¶ä»£çš„äº§å“å¼€å‘ä¸å†æ˜¯"é»‘ç›’"æ“ä½œï¼Œè€Œæ˜¯éœ€è¦å»ºç«‹ç§‘å­¦ã€ç³»ç»Ÿçš„è¯„ä¼°å’Œä¼˜åŒ–ä½“ç³»ã€‚æŒæ¡è¿™å¥—æ–¹æ³•è®ºï¼Œå°±æŒæ¡äº†AIäº§å“æˆåŠŸçš„å…³é”®ã€‚

è®©æˆ‘ä»¬å¸¦ç€è¿™äº›çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œåœ¨AIçš„æµªæ½®ä¸­åˆ›é€ å‡ºæ›´å¤šæœ‰ä»·å€¼çš„äº§å“å’ŒæœåŠ¡ï¼

---

**ğŸ“ æ­å–œå®Œæˆã€ŠAIæ—…è¡ŒAgentï¼šå¤§æ¨¡å‹è¯„ä¼°ä½“ç³»ä¸å®æˆ˜ã€‹è¯¾ç¨‹ï¼**

*æ„¿ä½ åœ¨AIçš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼Œåˆ›é€ å‡ºæ”¹å˜ä¸–ç•Œçš„äº§å“ï¼* ğŸŒŸ

